WARNING: Logging before InitGoogleLogging() is written to STDERR
I0803 10:30:41.684872 13998 solver.cpp:44] Initializing solver from parameters: 
test_iter: 20
test_interval: 250
base_lr: 0.0001
display: 20
max_iter: 5000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1500
snapshot: 5000
snapshot_prefix: "caffenet_train"
solver_mode: GPU
net: "caffenet/train_val.prototxt"
I0803 10:30:41.685781 13998 solver.cpp:87] Creating training net from net file: caffenet/train_val.prototxt
I0803 10:30:41.687111 13998 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0803 10:30:41.687142 13998 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0803 10:30:41.687343 13998 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "train.LMDB"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 10:30:41.687443 13998 layer_factory.hpp:77] Creating layer data
I0803 10:30:41.689337 13998 db_lmdb.cpp:35] Opened lmdb train.LMDB
I0803 10:30:41.689491 13998 net.cpp:86] Creating Layer data
I0803 10:30:41.689507 13998 net.cpp:382] data -> data
I0803 10:30:41.689527 13998 net.cpp:382] data -> label
I0803 10:30:41.690944 13998 data_layer.cpp:45] output data size: 256,3,227,227
I0803 10:30:42.110599 13998 net.cpp:124] Setting up data
I0803 10:30:42.110642 13998 net.cpp:131] Top shape: 256 3 227 227 (39574272)
I0803 10:30:42.110651 13998 net.cpp:131] Top shape: 256 (256)
I0803 10:30:42.110657 13998 net.cpp:139] Memory required for data: 158298112
I0803 10:30:42.110667 13998 layer_factory.hpp:77] Creating layer conv1
I0803 10:30:42.110694 13998 net.cpp:86] Creating Layer conv1
I0803 10:30:42.110702 13998 net.cpp:408] conv1 <- data
I0803 10:30:42.110714 13998 net.cpp:382] conv1 -> conv1
I0803 10:30:42.448899 13998 net.cpp:124] Setting up conv1
I0803 10:30:42.448954 13998 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 10:30:42.448962 13998 net.cpp:139] Memory required for data: 455667712
I0803 10:30:42.448989 13998 layer_factory.hpp:77] Creating layer relu1
I0803 10:30:42.449026 13998 net.cpp:86] Creating Layer relu1
I0803 10:30:42.449049 13998 net.cpp:408] relu1 <- conv1
I0803 10:30:42.449059 13998 net.cpp:369] relu1 -> conv1 (in-place)
I0803 10:30:42.449908 13998 net.cpp:124] Setting up relu1
I0803 10:30:42.449928 13998 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 10:30:42.449934 13998 net.cpp:139] Memory required for data: 753037312
I0803 10:30:42.449939 13998 layer_factory.hpp:77] Creating layer pool1
I0803 10:30:42.449956 13998 net.cpp:86] Creating Layer pool1
I0803 10:30:42.449962 13998 net.cpp:408] pool1 <- conv1
I0803 10:30:42.449971 13998 net.cpp:382] pool1 -> pool1
I0803 10:30:42.450049 13998 net.cpp:124] Setting up pool1
I0803 10:30:42.450062 13998 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 10:30:42.450068 13998 net.cpp:139] Memory required for data: 824700928
I0803 10:30:42.450073 13998 layer_factory.hpp:77] Creating layer norm1
I0803 10:30:42.450091 13998 net.cpp:86] Creating Layer norm1
I0803 10:30:42.450098 13998 net.cpp:408] norm1 <- pool1
I0803 10:30:42.450106 13998 net.cpp:382] norm1 -> norm1
I0803 10:30:42.450335 13998 net.cpp:124] Setting up norm1
I0803 10:30:42.450351 13998 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 10:30:42.450357 13998 net.cpp:139] Memory required for data: 896364544
I0803 10:30:42.450366 13998 layer_factory.hpp:77] Creating layer conv2
I0803 10:30:42.450407 13998 net.cpp:86] Creating Layer conv2
I0803 10:30:42.450417 13998 net.cpp:408] conv2 <- norm1
I0803 10:30:42.450426 13998 net.cpp:382] conv2 -> conv2
I0803 10:30:42.467962 13998 net.cpp:124] Setting up conv2
I0803 10:30:42.467983 13998 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 10:30:42.467991 13998 net.cpp:139] Memory required for data: 1087467520
I0803 10:30:42.468004 13998 layer_factory.hpp:77] Creating layer relu2
I0803 10:30:42.468017 13998 net.cpp:86] Creating Layer relu2
I0803 10:30:42.468024 13998 net.cpp:408] relu2 <- conv2
I0803 10:30:42.468031 13998 net.cpp:369] relu2 -> conv2 (in-place)
I0803 10:30:42.468261 13998 net.cpp:124] Setting up relu2
I0803 10:30:42.468276 13998 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 10:30:42.468283 13998 net.cpp:139] Memory required for data: 1278570496
I0803 10:30:42.468288 13998 layer_factory.hpp:77] Creating layer pool2
I0803 10:30:42.468298 13998 net.cpp:86] Creating Layer pool2
I0803 10:30:42.468304 13998 net.cpp:408] pool2 <- conv2
I0803 10:30:42.468314 13998 net.cpp:382] pool2 -> pool2
I0803 10:30:42.468369 13998 net.cpp:124] Setting up pool2
I0803 10:30:42.468379 13998 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 10:30:42.468384 13998 net.cpp:139] Memory required for data: 1322872832
I0803 10:30:42.468390 13998 layer_factory.hpp:77] Creating layer norm2
I0803 10:30:42.468405 13998 net.cpp:86] Creating Layer norm2
I0803 10:30:42.468411 13998 net.cpp:408] norm2 <- pool2
I0803 10:30:42.468418 13998 net.cpp:382] norm2 -> norm2
I0803 10:30:42.469221 13998 net.cpp:124] Setting up norm2
I0803 10:30:42.469238 13998 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 10:30:42.469244 13998 net.cpp:139] Memory required for data: 1367175168
I0803 10:30:42.469250 13998 layer_factory.hpp:77] Creating layer conv3
I0803 10:30:42.469269 13998 net.cpp:86] Creating Layer conv3
I0803 10:30:42.469277 13998 net.cpp:408] conv3 <- norm2
I0803 10:30:42.469290 13998 net.cpp:382] conv3 -> conv3
I0803 10:30:42.505844 13998 net.cpp:124] Setting up conv3
I0803 10:30:42.505867 13998 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 10:30:42.505875 13998 net.cpp:139] Memory required for data: 1433628672
I0803 10:30:42.505890 13998 layer_factory.hpp:77] Creating layer relu3
I0803 10:30:42.505903 13998 net.cpp:86] Creating Layer relu3
I0803 10:30:42.505909 13998 net.cpp:408] relu3 <- conv3
I0803 10:30:42.505918 13998 net.cpp:369] relu3 -> conv3 (in-place)
I0803 10:30:42.506140 13998 net.cpp:124] Setting up relu3
I0803 10:30:42.506155 13998 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 10:30:42.506162 13998 net.cpp:139] Memory required for data: 1500082176
I0803 10:30:42.506167 13998 layer_factory.hpp:77] Creating layer conv4
I0803 10:30:42.506183 13998 net.cpp:86] Creating Layer conv4
I0803 10:30:42.506191 13998 net.cpp:408] conv4 <- conv3
I0803 10:30:42.506202 13998 net.cpp:382] conv4 -> conv4
I0803 10:30:42.535795 13998 net.cpp:124] Setting up conv4
I0803 10:30:42.535816 13998 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 10:30:42.535823 13998 net.cpp:139] Memory required for data: 1566535680
I0803 10:30:42.535835 13998 layer_factory.hpp:77] Creating layer relu4
I0803 10:30:42.535848 13998 net.cpp:86] Creating Layer relu4
I0803 10:30:42.535856 13998 net.cpp:408] relu4 <- conv4
I0803 10:30:42.535866 13998 net.cpp:369] relu4 -> conv4 (in-place)
I0803 10:30:42.536084 13998 net.cpp:124] Setting up relu4
I0803 10:30:42.536098 13998 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 10:30:42.536105 13998 net.cpp:139] Memory required for data: 1632989184
I0803 10:30:42.536111 13998 layer_factory.hpp:77] Creating layer conv5
I0803 10:30:42.536129 13998 net.cpp:86] Creating Layer conv5
I0803 10:30:42.536136 13998 net.cpp:408] conv5 <- conv4
I0803 10:30:42.536145 13998 net.cpp:382] conv5 -> conv5
I0803 10:30:42.557849 13998 net.cpp:124] Setting up conv5
I0803 10:30:42.557869 13998 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 10:30:42.557876 13998 net.cpp:139] Memory required for data: 1677291520
I0803 10:30:42.557891 13998 layer_factory.hpp:77] Creating layer relu5
I0803 10:30:42.557905 13998 net.cpp:86] Creating Layer relu5
I0803 10:30:42.557912 13998 net.cpp:408] relu5 <- conv5
I0803 10:30:42.557919 13998 net.cpp:369] relu5 -> conv5 (in-place)
I0803 10:30:42.558135 13998 net.cpp:124] Setting up relu5
I0803 10:30:42.558149 13998 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 10:30:42.558156 13998 net.cpp:139] Memory required for data: 1721593856
I0803 10:30:42.558161 13998 layer_factory.hpp:77] Creating layer pool5
I0803 10:30:42.558176 13998 net.cpp:86] Creating Layer pool5
I0803 10:30:42.558182 13998 net.cpp:408] pool5 <- conv5
I0803 10:30:42.558192 13998 net.cpp:382] pool5 -> pool5
I0803 10:30:42.558254 13998 net.cpp:124] Setting up pool5
I0803 10:30:42.558265 13998 net.cpp:131] Top shape: 256 256 6 6 (2359296)
I0803 10:30:42.558271 13998 net.cpp:139] Memory required for data: 1731031040
I0803 10:30:42.558276 13998 layer_factory.hpp:77] Creating layer fc6
I0803 10:30:42.558292 13998 net.cpp:86] Creating Layer fc6
I0803 10:30:42.558300 13998 net.cpp:408] fc6 <- pool5
I0803 10:30:42.558310 13998 net.cpp:382] fc6 -> fc6
I0803 10:30:43.971807 13998 net.cpp:124] Setting up fc6
I0803 10:30:43.971851 13998 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 10:30:43.971858 13998 net.cpp:139] Memory required for data: 1735225344
I0803 10:30:43.971873 13998 layer_factory.hpp:77] Creating layer relu6
I0803 10:30:43.971887 13998 net.cpp:86] Creating Layer relu6
I0803 10:30:43.971894 13998 net.cpp:408] relu6 <- fc6
I0803 10:30:43.971904 13998 net.cpp:369] relu6 -> fc6 (in-place)
I0803 10:30:43.972868 13998 net.cpp:124] Setting up relu6
I0803 10:30:43.972898 13998 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 10:30:43.972904 13998 net.cpp:139] Memory required for data: 1739419648
I0803 10:30:43.972909 13998 layer_factory.hpp:77] Creating layer drop6
I0803 10:30:43.972930 13998 net.cpp:86] Creating Layer drop6
I0803 10:30:43.972937 13998 net.cpp:408] drop6 <- fc6
I0803 10:30:43.972946 13998 net.cpp:369] drop6 -> fc6 (in-place)
I0803 10:30:43.972980 13998 net.cpp:124] Setting up drop6
I0803 10:30:43.972993 13998 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 10:30:43.973001 13998 net.cpp:139] Memory required for data: 1743613952
I0803 10:30:43.973006 13998 layer_factory.hpp:77] Creating layer fc7
I0803 10:30:43.973018 13998 net.cpp:86] Creating Layer fc7
I0803 10:30:43.973024 13998 net.cpp:408] fc7 <- fc6
I0803 10:30:43.973034 13998 net.cpp:382] fc7 -> fc7
I0803 10:30:44.621685 13998 net.cpp:124] Setting up fc7
I0803 10:30:44.621742 13998 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 10:30:44.621747 13998 net.cpp:139] Memory required for data: 1747808256
I0803 10:30:44.621762 13998 layer_factory.hpp:77] Creating layer relu7
I0803 10:30:44.621776 13998 net.cpp:86] Creating Layer relu7
I0803 10:30:44.621783 13998 net.cpp:408] relu7 <- fc7
I0803 10:30:44.621796 13998 net.cpp:369] relu7 -> fc7 (in-place)
I0803 10:30:44.622778 13998 net.cpp:124] Setting up relu7
I0803 10:30:44.622797 13998 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 10:30:44.622802 13998 net.cpp:139] Memory required for data: 1752002560
I0803 10:30:44.622807 13998 layer_factory.hpp:77] Creating layer drop7
I0803 10:30:44.622817 13998 net.cpp:86] Creating Layer drop7
I0803 10:30:44.622822 13998 net.cpp:408] drop7 <- fc7
I0803 10:30:44.622828 13998 net.cpp:369] drop7 -> fc7 (in-place)
I0803 10:30:44.622866 13998 net.cpp:124] Setting up drop7
I0803 10:30:44.622877 13998 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 10:30:44.622884 13998 net.cpp:139] Memory required for data: 1756196864
I0803 10:30:44.622889 13998 layer_factory.hpp:77] Creating layer fc8
I0803 10:30:44.622902 13998 net.cpp:86] Creating Layer fc8
I0803 10:30:44.622908 13998 net.cpp:408] fc8 <- fc7
I0803 10:30:44.622917 13998 net.cpp:382] fc8 -> fc8
I0803 10:30:44.624008 13998 net.cpp:124] Setting up fc8
I0803 10:30:44.624023 13998 net.cpp:131] Top shape: 256 2 (512)
I0803 10:30:44.624028 13998 net.cpp:139] Memory required for data: 1756198912
I0803 10:30:44.624049 13998 layer_factory.hpp:77] Creating layer loss
I0803 10:30:44.624061 13998 net.cpp:86] Creating Layer loss
I0803 10:30:44.624066 13998 net.cpp:408] loss <- fc8
I0803 10:30:44.624073 13998 net.cpp:408] loss <- label
I0803 10:30:44.624078 13998 net.cpp:382] loss -> loss
I0803 10:30:44.624101 13998 layer_factory.hpp:77] Creating layer loss
I0803 10:30:44.624438 13998 net.cpp:124] Setting up loss
I0803 10:30:44.624464 13998 net.cpp:131] Top shape: (1)
I0803 10:30:44.624471 13998 net.cpp:134]     with loss weight 1
I0803 10:30:44.624495 13998 net.cpp:139] Memory required for data: 1756198916
I0803 10:30:44.624501 13998 net.cpp:200] loss needs backward computation.
I0803 10:30:44.624507 13998 net.cpp:200] fc8 needs backward computation.
I0803 10:30:44.624526 13998 net.cpp:200] drop7 needs backward computation.
I0803 10:30:44.624532 13998 net.cpp:200] relu7 needs backward computation.
I0803 10:30:44.624536 13998 net.cpp:200] fc7 needs backward computation.
I0803 10:30:44.624541 13998 net.cpp:200] drop6 needs backward computation.
I0803 10:30:44.624547 13998 net.cpp:200] relu6 needs backward computation.
I0803 10:30:44.624550 13998 net.cpp:200] fc6 needs backward computation.
I0803 10:30:44.624555 13998 net.cpp:200] pool5 needs backward computation.
I0803 10:30:44.624562 13998 net.cpp:200] relu5 needs backward computation.
I0803 10:30:44.624567 13998 net.cpp:200] conv5 needs backward computation.
I0803 10:30:44.624572 13998 net.cpp:200] relu4 needs backward computation.
I0803 10:30:44.624577 13998 net.cpp:200] conv4 needs backward computation.
I0803 10:30:44.624583 13998 net.cpp:200] relu3 needs backward computation.
I0803 10:30:44.624586 13998 net.cpp:200] conv3 needs backward computation.
I0803 10:30:44.624593 13998 net.cpp:200] norm2 needs backward computation.
I0803 10:30:44.624600 13998 net.cpp:200] pool2 needs backward computation.
I0803 10:30:44.624605 13998 net.cpp:200] relu2 needs backward computation.
I0803 10:30:44.624611 13998 net.cpp:200] conv2 needs backward computation.
I0803 10:30:44.624615 13998 net.cpp:200] norm1 needs backward computation.
I0803 10:30:44.624620 13998 net.cpp:200] pool1 needs backward computation.
I0803 10:30:44.624626 13998 net.cpp:200] relu1 needs backward computation.
I0803 10:30:44.624630 13998 net.cpp:200] conv1 needs backward computation.
I0803 10:30:44.624636 13998 net.cpp:202] data does not need backward computation.
I0803 10:30:44.624641 13998 net.cpp:244] This network produces output loss
I0803 10:30:44.624660 13998 net.cpp:257] Network initialization done.
I0803 10:30:44.626150 13998 solver.cpp:173] Creating test net (#0) specified by net file: caffenet/train_val.prototxt
I0803 10:30:44.626211 13998 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0803 10:30:44.626452 13998 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "val.LMDB"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 10:30:44.626606 13998 layer_factory.hpp:77] Creating layer data
I0803 10:30:44.628324 13998 db_lmdb.cpp:35] Opened lmdb val.LMDB
I0803 10:30:44.628470 13998 net.cpp:86] Creating Layer data
I0803 10:30:44.628485 13998 net.cpp:382] data -> data
I0803 10:30:44.628500 13998 net.cpp:382] data -> label
I0803 10:30:44.628865 13998 data_layer.cpp:45] output data size: 50,3,227,227
I0803 10:30:44.714426 13998 net.cpp:124] Setting up data
I0803 10:30:44.714491 13998 net.cpp:131] Top shape: 50 3 227 227 (7729350)
I0803 10:30:44.714503 13998 net.cpp:131] Top shape: 50 (50)
I0803 10:30:44.714509 13998 net.cpp:139] Memory required for data: 30917600
I0803 10:30:44.714519 13998 layer_factory.hpp:77] Creating layer label_data_1_split
I0803 10:30:44.714536 13998 net.cpp:86] Creating Layer label_data_1_split
I0803 10:30:44.714543 13998 net.cpp:408] label_data_1_split <- label
I0803 10:30:44.714555 13998 net.cpp:382] label_data_1_split -> label_data_1_split_0
I0803 10:30:44.714573 13998 net.cpp:382] label_data_1_split -> label_data_1_split_1
I0803 10:30:44.714650 13998 net.cpp:124] Setting up label_data_1_split
I0803 10:30:44.714663 13998 net.cpp:131] Top shape: 50 (50)
I0803 10:30:44.714669 13998 net.cpp:131] Top shape: 50 (50)
I0803 10:30:44.714674 13998 net.cpp:139] Memory required for data: 30918000
I0803 10:30:44.714680 13998 layer_factory.hpp:77] Creating layer conv1
I0803 10:30:44.714701 13998 net.cpp:86] Creating Layer conv1
I0803 10:30:44.714709 13998 net.cpp:408] conv1 <- data
I0803 10:30:44.714717 13998 net.cpp:382] conv1 -> conv1
I0803 10:30:44.724597 13998 net.cpp:124] Setting up conv1
I0803 10:30:44.724668 13998 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 10:30:44.724684 13998 net.cpp:139] Memory required for data: 88998000
I0803 10:30:44.724723 13998 layer_factory.hpp:77] Creating layer relu1
I0803 10:30:44.724755 13998 net.cpp:86] Creating Layer relu1
I0803 10:30:44.724771 13998 net.cpp:408] relu1 <- conv1
I0803 10:30:44.724787 13998 net.cpp:369] relu1 -> conv1 (in-place)
I0803 10:30:44.725204 13998 net.cpp:124] Setting up relu1
I0803 10:30:44.725234 13998 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 10:30:44.725244 13998 net.cpp:139] Memory required for data: 147078000
I0803 10:30:44.725258 13998 layer_factory.hpp:77] Creating layer pool1
I0803 10:30:44.725282 13998 net.cpp:86] Creating Layer pool1
I0803 10:30:44.725296 13998 net.cpp:408] pool1 <- conv1
I0803 10:30:44.725311 13998 net.cpp:382] pool1 -> pool1
I0803 10:30:44.725430 13998 net.cpp:124] Setting up pool1
I0803 10:30:44.725455 13998 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 10:30:44.725466 13998 net.cpp:139] Memory required for data: 161074800
I0803 10:30:44.725476 13998 layer_factory.hpp:77] Creating layer norm1
I0803 10:30:44.725499 13998 net.cpp:86] Creating Layer norm1
I0803 10:30:44.725512 13998 net.cpp:408] norm1 <- pool1
I0803 10:30:44.725528 13998 net.cpp:382] norm1 -> norm1
I0803 10:30:44.727089 13998 net.cpp:124] Setting up norm1
I0803 10:30:44.727123 13998 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 10:30:44.727134 13998 net.cpp:139] Memory required for data: 175071600
I0803 10:30:44.727145 13998 layer_factory.hpp:77] Creating layer conv2
I0803 10:30:44.727177 13998 net.cpp:86] Creating Layer conv2
I0803 10:30:44.727190 13998 net.cpp:408] conv2 <- norm1
I0803 10:30:44.727208 13998 net.cpp:382] conv2 -> conv2
I0803 10:30:44.754590 13998 net.cpp:124] Setting up conv2
I0803 10:30:44.754652 13998 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 10:30:44.754662 13998 net.cpp:139] Memory required for data: 212396400
I0803 10:30:44.754693 13998 layer_factory.hpp:77] Creating layer relu2
I0803 10:30:44.754719 13998 net.cpp:86] Creating Layer relu2
I0803 10:30:44.754732 13998 net.cpp:408] relu2 <- conv2
I0803 10:30:44.754746 13998 net.cpp:369] relu2 -> conv2 (in-place)
I0803 10:30:44.755126 13998 net.cpp:124] Setting up relu2
I0803 10:30:44.755149 13998 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 10:30:44.755158 13998 net.cpp:139] Memory required for data: 249721200
I0803 10:30:44.755167 13998 layer_factory.hpp:77] Creating layer pool2
I0803 10:30:44.755192 13998 net.cpp:86] Creating Layer pool2
I0803 10:30:44.755200 13998 net.cpp:408] pool2 <- conv2
I0803 10:30:44.755218 13998 net.cpp:382] pool2 -> pool2
I0803 10:30:44.755323 13998 net.cpp:124] Setting up pool2
I0803 10:30:44.755344 13998 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 10:30:44.755354 13998 net.cpp:139] Memory required for data: 258374000
I0803 10:30:44.755362 13998 layer_factory.hpp:77] Creating layer norm2
I0803 10:30:44.755383 13998 net.cpp:86] Creating Layer norm2
I0803 10:30:44.755394 13998 net.cpp:408] norm2 <- pool2
I0803 10:30:44.755406 13998 net.cpp:382] norm2 -> norm2
I0803 10:30:44.756712 13998 net.cpp:124] Setting up norm2
I0803 10:30:44.756739 13998 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 10:30:44.756750 13998 net.cpp:139] Memory required for data: 267026800
I0803 10:30:44.756762 13998 layer_factory.hpp:77] Creating layer conv3
I0803 10:30:44.756791 13998 net.cpp:86] Creating Layer conv3
I0803 10:30:44.756803 13998 net.cpp:408] conv3 <- norm2
I0803 10:30:44.756821 13998 net.cpp:382] conv3 -> conv3
I0803 10:30:44.806884 13998 net.cpp:124] Setting up conv3
I0803 10:30:44.806936 13998 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 10:30:44.806943 13998 net.cpp:139] Memory required for data: 280006000
I0803 10:30:44.806973 13998 layer_factory.hpp:77] Creating layer relu3
I0803 10:30:44.806995 13998 net.cpp:86] Creating Layer relu3
I0803 10:30:44.807005 13998 net.cpp:408] relu3 <- conv3
I0803 10:30:44.807018 13998 net.cpp:369] relu3 -> conv3 (in-place)
I0803 10:30:44.807930 13998 net.cpp:124] Setting up relu3
I0803 10:30:44.807951 13998 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 10:30:44.807960 13998 net.cpp:139] Memory required for data: 292985200
I0803 10:30:44.807966 13998 layer_factory.hpp:77] Creating layer conv4
I0803 10:30:44.807991 13998 net.cpp:86] Creating Layer conv4
I0803 10:30:44.808001 13998 net.cpp:408] conv4 <- conv3
I0803 10:30:44.808027 13998 net.cpp:382] conv4 -> conv4
I0803 10:30:44.847743 13998 net.cpp:124] Setting up conv4
I0803 10:30:44.847800 13998 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 10:30:44.847807 13998 net.cpp:139] Memory required for data: 305964400
I0803 10:30:44.847826 13998 layer_factory.hpp:77] Creating layer relu4
I0803 10:30:44.847843 13998 net.cpp:86] Creating Layer relu4
I0803 10:30:44.847851 13998 net.cpp:408] relu4 <- conv4
I0803 10:30:44.847867 13998 net.cpp:369] relu4 -> conv4 (in-place)
I0803 10:30:44.848132 13998 net.cpp:124] Setting up relu4
I0803 10:30:44.848147 13998 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 10:30:44.848155 13998 net.cpp:139] Memory required for data: 318943600
I0803 10:30:44.848160 13998 layer_factory.hpp:77] Creating layer conv5
I0803 10:30:44.848186 13998 net.cpp:86] Creating Layer conv5
I0803 10:30:44.848194 13998 net.cpp:408] conv5 <- conv4
I0803 10:30:44.848204 13998 net.cpp:382] conv5 -> conv5
I0803 10:30:44.870498 13998 net.cpp:124] Setting up conv5
I0803 10:30:44.870551 13998 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 10:30:44.870558 13998 net.cpp:139] Memory required for data: 327596400
I0803 10:30:44.870584 13998 layer_factory.hpp:77] Creating layer relu5
I0803 10:30:44.870601 13998 net.cpp:86] Creating Layer relu5
I0803 10:30:44.870609 13998 net.cpp:408] relu5 <- conv5
I0803 10:30:44.870625 13998 net.cpp:369] relu5 -> conv5 (in-place)
I0803 10:30:44.870858 13998 net.cpp:124] Setting up relu5
I0803 10:30:44.870873 13998 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 10:30:44.870880 13998 net.cpp:139] Memory required for data: 336249200
I0803 10:30:44.870885 13998 layer_factory.hpp:77] Creating layer pool5
I0803 10:30:44.870905 13998 net.cpp:86] Creating Layer pool5
I0803 10:30:44.870913 13998 net.cpp:408] pool5 <- conv5
I0803 10:30:44.870921 13998 net.cpp:382] pool5 -> pool5
I0803 10:30:44.870997 13998 net.cpp:124] Setting up pool5
I0803 10:30:44.871009 13998 net.cpp:131] Top shape: 50 256 6 6 (460800)
I0803 10:30:44.871016 13998 net.cpp:139] Memory required for data: 338092400
I0803 10:30:44.871021 13998 layer_factory.hpp:77] Creating layer fc6
I0803 10:30:44.871034 13998 net.cpp:86] Creating Layer fc6
I0803 10:30:44.871040 13998 net.cpp:408] fc6 <- pool5
I0803 10:30:44.871050 13998 net.cpp:382] fc6 -> fc6
I0803 10:30:46.297701 13998 net.cpp:124] Setting up fc6
I0803 10:30:46.297770 13998 net.cpp:131] Top shape: 50 4096 (204800)
I0803 10:30:46.297776 13998 net.cpp:139] Memory required for data: 338911600
I0803 10:30:46.297794 13998 layer_factory.hpp:77] Creating layer relu6
I0803 10:30:46.297825 13998 net.cpp:86] Creating Layer relu6
I0803 10:30:46.297833 13998 net.cpp:408] relu6 <- fc6
I0803 10:30:46.297845 13998 net.cpp:369] relu6 -> fc6 (in-place)
I0803 10:30:46.298938 13998 net.cpp:124] Setting up relu6
I0803 10:30:46.298967 13998 net.cpp:131] Top shape: 50 4096 (204800)
I0803 10:30:46.298974 13998 net.cpp:139] Memory required for data: 339730800
I0803 10:30:46.298979 13998 layer_factory.hpp:77] Creating layer drop6
I0803 10:30:46.298998 13998 net.cpp:86] Creating Layer drop6
I0803 10:30:46.299005 13998 net.cpp:408] drop6 <- fc6
I0803 10:30:46.299018 13998 net.cpp:369] drop6 -> fc6 (in-place)
I0803 10:30:46.299065 13998 net.cpp:124] Setting up drop6
I0803 10:30:46.299077 13998 net.cpp:131] Top shape: 50 4096 (204800)
I0803 10:30:46.299082 13998 net.cpp:139] Memory required for data: 340550000
I0803 10:30:46.299088 13998 layer_factory.hpp:77] Creating layer fc7
I0803 10:30:46.299104 13998 net.cpp:86] Creating Layer fc7
I0803 10:30:46.299111 13998 net.cpp:408] fc7 <- fc6
I0803 10:30:46.299121 13998 net.cpp:382] fc7 -> fc7
I0803 10:30:46.972926 13998 net.cpp:124] Setting up fc7
I0803 10:30:46.972987 13998 net.cpp:131] Top shape: 50 4096 (204800)
I0803 10:30:46.972992 13998 net.cpp:139] Memory required for data: 341369200
I0803 10:30:46.973021 13998 layer_factory.hpp:77] Creating layer relu7
I0803 10:30:46.973047 13998 net.cpp:86] Creating Layer relu7
I0803 10:30:46.973054 13998 net.cpp:408] relu7 <- fc7
I0803 10:30:46.973063 13998 net.cpp:369] relu7 -> fc7 (in-place)
I0803 10:30:46.973887 13998 net.cpp:124] Setting up relu7
I0803 10:30:46.973903 13998 net.cpp:131] Top shape: 50 4096 (204800)
I0803 10:30:46.973908 13998 net.cpp:139] Memory required for data: 342188400
I0803 10:30:46.973914 13998 layer_factory.hpp:77] Creating layer drop7
I0803 10:30:46.973927 13998 net.cpp:86] Creating Layer drop7
I0803 10:30:46.973932 13998 net.cpp:408] drop7 <- fc7
I0803 10:30:46.973937 13998 net.cpp:369] drop7 -> fc7 (in-place)
I0803 10:30:46.973981 13998 net.cpp:124] Setting up drop7
I0803 10:30:46.973992 13998 net.cpp:131] Top shape: 50 4096 (204800)
I0803 10:30:46.973997 13998 net.cpp:139] Memory required for data: 343007600
I0803 10:30:46.974002 13998 layer_factory.hpp:77] Creating layer fc8
I0803 10:30:46.974017 13998 net.cpp:86] Creating Layer fc8
I0803 10:30:46.974022 13998 net.cpp:408] fc8 <- fc7
I0803 10:30:46.974030 13998 net.cpp:382] fc8 -> fc8
I0803 10:30:46.974647 13998 net.cpp:124] Setting up fc8
I0803 10:30:46.974663 13998 net.cpp:131] Top shape: 50 2 (100)
I0803 10:30:46.974666 13998 net.cpp:139] Memory required for data: 343008000
I0803 10:30:46.974675 13998 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0803 10:30:46.974684 13998 net.cpp:86] Creating Layer fc8_fc8_0_split
I0803 10:30:46.974689 13998 net.cpp:408] fc8_fc8_0_split <- fc8
I0803 10:30:46.974704 13998 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0803 10:30:46.974714 13998 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0803 10:30:46.974764 13998 net.cpp:124] Setting up fc8_fc8_0_split
I0803 10:30:46.974776 13998 net.cpp:131] Top shape: 50 2 (100)
I0803 10:30:46.974782 13998 net.cpp:131] Top shape: 50 2 (100)
I0803 10:30:46.974786 13998 net.cpp:139] Memory required for data: 343008800
I0803 10:30:46.974792 13998 layer_factory.hpp:77] Creating layer accuracy
I0803 10:30:46.974808 13998 net.cpp:86] Creating Layer accuracy
I0803 10:30:46.974815 13998 net.cpp:408] accuracy <- fc8_fc8_0_split_0
I0803 10:30:46.974822 13998 net.cpp:408] accuracy <- label_data_1_split_0
I0803 10:30:46.974831 13998 net.cpp:382] accuracy -> accuracy
I0803 10:30:46.974843 13998 net.cpp:124] Setting up accuracy
I0803 10:30:46.974853 13998 net.cpp:131] Top shape: (1)
I0803 10:30:46.974858 13998 net.cpp:139] Memory required for data: 343008804
I0803 10:30:46.974862 13998 layer_factory.hpp:77] Creating layer loss
I0803 10:30:46.974872 13998 net.cpp:86] Creating Layer loss
I0803 10:30:46.974877 13998 net.cpp:408] loss <- fc8_fc8_0_split_1
I0803 10:30:46.974882 13998 net.cpp:408] loss <- label_data_1_split_1
I0803 10:30:46.974889 13998 net.cpp:382] loss -> loss
I0803 10:30:46.974900 13998 layer_factory.hpp:77] Creating layer loss
I0803 10:30:46.975966 13998 net.cpp:124] Setting up loss
I0803 10:30:46.975982 13998 net.cpp:131] Top shape: (1)
I0803 10:30:46.975987 13998 net.cpp:134]     with loss weight 1
I0803 10:30:46.976009 13998 net.cpp:139] Memory required for data: 343008808
I0803 10:30:46.976014 13998 net.cpp:200] loss needs backward computation.
I0803 10:30:46.976019 13998 net.cpp:202] accuracy does not need backward computation.
I0803 10:30:46.976027 13998 net.cpp:200] fc8_fc8_0_split needs backward computation.
I0803 10:30:46.976032 13998 net.cpp:200] fc8 needs backward computation.
I0803 10:30:46.976035 13998 net.cpp:200] drop7 needs backward computation.
I0803 10:30:46.976039 13998 net.cpp:200] relu7 needs backward computation.
I0803 10:30:46.976042 13998 net.cpp:200] fc7 needs backward computation.
I0803 10:30:46.976047 13998 net.cpp:200] drop6 needs backward computation.
I0803 10:30:46.976050 13998 net.cpp:200] relu6 needs backward computation.
I0803 10:30:46.976054 13998 net.cpp:200] fc6 needs backward computation.
I0803 10:30:46.976064 13998 net.cpp:200] pool5 needs backward computation.
I0803 10:30:46.976074 13998 net.cpp:200] relu5 needs backward computation.
I0803 10:30:46.976080 13998 net.cpp:200] conv5 needs backward computation.
I0803 10:30:46.976094 13998 net.cpp:200] relu4 needs backward computation.
I0803 10:30:46.976132 13998 net.cpp:200] conv4 needs backward computation.
I0803 10:30:46.976138 13998 net.cpp:200] relu3 needs backward computation.
I0803 10:30:46.976147 13998 net.cpp:200] conv3 needs backward computation.
I0803 10:30:46.976161 13998 net.cpp:200] norm2 needs backward computation.
I0803 10:30:46.976166 13998 net.cpp:200] pool2 needs backward computation.
I0803 10:30:46.976176 13998 net.cpp:200] relu2 needs backward computation.
I0803 10:30:46.976196 13998 net.cpp:200] conv2 needs backward computation.
I0803 10:30:46.976200 13998 net.cpp:200] norm1 needs backward computation.
I0803 10:30:46.976220 13998 net.cpp:200] pool1 needs backward computation.
I0803 10:30:46.976227 13998 net.cpp:200] relu1 needs backward computation.
I0803 10:30:46.976233 13998 net.cpp:200] conv1 needs backward computation.
I0803 10:30:46.976238 13998 net.cpp:202] label_data_1_split does not need backward computation.
I0803 10:30:46.976248 13998 net.cpp:202] data does not need backward computation.
I0803 10:30:46.976253 13998 net.cpp:244] This network produces output accuracy
I0803 10:30:46.976258 13998 net.cpp:244] This network produces output loss
I0803 10:30:46.976279 13998 net.cpp:257] Network initialization done.
I0803 10:30:46.976418 13998 solver.cpp:56] Solver scaffolding done.
I0803 10:30:46.984180 13998 solver.cpp:331] Iteration 0, Testing net (#0)
I0803 10:30:47.377207 13998 blocking_queue.cpp:49] Waiting for data
I0803 10:30:48.355305 13998 solver.cpp:398]     Test net output #0: accuracy = 0.628
I0803 10:30:48.355368 13998 solver.cpp:398]     Test net output #1: loss = 0.688083 (* 1 = 0.688083 loss)
I0803 10:30:49.262187 13998 solver.cpp:219] Iteration 0 (0 iter/s, 2.28288s/20 iters), loss = 0.889152
I0803 10:30:49.265271 13998 solver.cpp:238]     Train net output #0: loss = 0.889152 (* 1 = 0.889152 loss)
I0803 10:30:49.265300 13998 sgd_solver.cpp:105] Iteration 0, lr = 0.0001
I0803 10:31:07.578639 13998 solver.cpp:219] Iteration 20 (1.09212 iter/s, 18.3131s/20 iters), loss = 0.79983
I0803 10:31:07.578780 13998 solver.cpp:238]     Train net output #0: loss = 0.79983 (* 1 = 0.79983 loss)
I0803 10:31:07.578797 13998 sgd_solver.cpp:105] Iteration 20, lr = 0.0001
I0803 10:31:25.860754 13998 solver.cpp:219] Iteration 40 (1.09399 iter/s, 18.2818s/20 iters), loss = 0.795323
I0803 10:31:25.860849 13998 solver.cpp:238]     Train net output #0: loss = 0.795323 (* 1 = 0.795323 loss)
I0803 10:31:25.860867 13998 sgd_solver.cpp:105] Iteration 40, lr = 0.0001
I0803 10:31:44.158176 13998 solver.cpp:219] Iteration 60 (1.09307 iter/s, 18.2971s/20 iters), loss = 0.756882
I0803 10:31:44.158305 13998 solver.cpp:238]     Train net output #0: loss = 0.756882 (* 1 = 0.756882 loss)
I0803 10:31:44.158337 13998 sgd_solver.cpp:105] Iteration 60, lr = 0.0001
I0803 10:32:02.453166 13998 solver.cpp:219] Iteration 80 (1.09322 iter/s, 18.2945s/20 iters), loss = 0.719983
I0803 10:32:02.453260 13998 solver.cpp:238]     Train net output #0: loss = 0.719983 (* 1 = 0.719983 loss)
I0803 10:32:02.453276 13998 sgd_solver.cpp:105] Iteration 80, lr = 0.0001
I0803 10:32:20.748042 13998 solver.cpp:219] Iteration 100 (1.09322 iter/s, 18.2945s/20 iters), loss = 0.783311
I0803 10:32:20.748142 13998 solver.cpp:238]     Train net output #0: loss = 0.783311 (* 1 = 0.783311 loss)
I0803 10:32:20.748159 13998 sgd_solver.cpp:105] Iteration 100, lr = 0.0001
I0803 10:32:25.670042 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:32:39.053400 13998 solver.cpp:219] Iteration 120 (1.0926 iter/s, 18.3049s/20 iters), loss = 0.768026
I0803 10:32:39.053572 13998 solver.cpp:238]     Train net output #0: loss = 0.768026 (* 1 = 0.768026 loss)
I0803 10:32:39.053603 13998 sgd_solver.cpp:105] Iteration 120, lr = 0.0001
I0803 10:32:57.353255 13998 solver.cpp:219] Iteration 140 (1.09293 iter/s, 18.2994s/20 iters), loss = 0.691258
I0803 10:32:57.353358 13998 solver.cpp:238]     Train net output #0: loss = 0.691258 (* 1 = 0.691258 loss)
I0803 10:32:57.353374 13998 sgd_solver.cpp:105] Iteration 140, lr = 0.0001
I0803 10:33:15.659076 13998 solver.cpp:219] Iteration 160 (1.09257 iter/s, 18.3054s/20 iters), loss = 0.720649
I0803 10:33:15.659191 13998 solver.cpp:238]     Train net output #0: loss = 0.720649 (* 1 = 0.720649 loss)
I0803 10:33:15.659224 13998 sgd_solver.cpp:105] Iteration 160, lr = 0.0001
I0803 10:33:33.959257 13998 solver.cpp:219] Iteration 180 (1.09291 iter/s, 18.2998s/20 iters), loss = 0.676753
I0803 10:33:33.959375 13998 solver.cpp:238]     Train net output #0: loss = 0.676753 (* 1 = 0.676753 loss)
I0803 10:33:33.959395 13998 sgd_solver.cpp:105] Iteration 180, lr = 0.0001
I0803 10:33:52.262249 13998 solver.cpp:219] Iteration 200 (1.09275 iter/s, 18.3025s/20 iters), loss = 0.711242
I0803 10:33:52.262332 13998 solver.cpp:238]     Train net output #0: loss = 0.711242 (* 1 = 0.711242 loss)
I0803 10:33:52.262359 13998 sgd_solver.cpp:105] Iteration 200, lr = 0.0001
I0803 10:34:05.548763 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:34:10.560518 13998 solver.cpp:219] Iteration 220 (1.09302 iter/s, 18.2979s/20 iters), loss = 0.739045
I0803 10:34:10.560634 13998 solver.cpp:238]     Train net output #0: loss = 0.739045 (* 1 = 0.739045 loss)
I0803 10:34:10.560649 13998 sgd_solver.cpp:105] Iteration 220, lr = 0.0001
I0803 10:34:28.860894 13998 solver.cpp:219] Iteration 240 (1.0929 iter/s, 18.3s/20 iters), loss = 0.717839
I0803 10:34:28.861003 13998 solver.cpp:238]     Train net output #0: loss = 0.717839 (* 1 = 0.717839 loss)
I0803 10:34:28.861035 13998 sgd_solver.cpp:105] Iteration 240, lr = 0.0001
I0803 10:34:36.491360 13998 solver.cpp:331] Iteration 250, Testing net (#0)
I0803 10:34:38.406888 13998 solver.cpp:398]     Test net output #0: accuracy = 0.623
I0803 10:34:38.406996 13998 solver.cpp:398]     Test net output #1: loss = 0.658122 (* 1 = 0.658122 loss)
I0803 10:34:48.426923 13998 solver.cpp:219] Iteration 260 (1.0222 iter/s, 19.5656s/20 iters), loss = 0.739336
I0803 10:34:48.442358 13998 solver.cpp:238]     Train net output #0: loss = 0.739336 (* 1 = 0.739336 loss)
I0803 10:34:48.442400 13998 sgd_solver.cpp:105] Iteration 260, lr = 0.0001
I0803 10:35:06.748205 13998 solver.cpp:219] Iteration 280 (1.09257 iter/s, 18.3055s/20 iters), loss = 0.680169
I0803 10:35:06.748327 13998 solver.cpp:238]     Train net output #0: loss = 0.680169 (* 1 = 0.680169 loss)
I0803 10:35:06.748361 13998 sgd_solver.cpp:105] Iteration 280, lr = 0.0001
I0803 10:35:25.050730 13998 solver.cpp:219] Iteration 300 (1.09277 iter/s, 18.3021s/20 iters), loss = 0.745273
I0803 10:35:25.050823 13998 solver.cpp:238]     Train net output #0: loss = 0.745273 (* 1 = 0.745273 loss)
I0803 10:35:25.050838 13998 sgd_solver.cpp:105] Iteration 300, lr = 0.0001
I0803 10:35:43.344327 13998 solver.cpp:219] Iteration 320 (1.0933 iter/s, 18.2932s/20 iters), loss = 0.699094
I0803 10:35:43.344431 13998 solver.cpp:238]     Train net output #0: loss = 0.699094 (* 1 = 0.699094 loss)
I0803 10:35:43.344460 13998 sgd_solver.cpp:105] Iteration 320, lr = 0.0001
I0803 10:35:46.624707 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:36:01.648705 13998 solver.cpp:219] Iteration 340 (1.09266 iter/s, 18.304s/20 iters), loss = 0.681889
I0803 10:36:01.648802 13998 solver.cpp:238]     Train net output #0: loss = 0.681889 (* 1 = 0.681889 loss)
I0803 10:36:01.648816 13998 sgd_solver.cpp:105] Iteration 340, lr = 0.0001
I0803 10:36:19.955338 13998 solver.cpp:219] Iteration 360 (1.09253 iter/s, 18.3061s/20 iters), loss = 0.726069
I0803 10:36:19.955435 13998 solver.cpp:238]     Train net output #0: loss = 0.726069 (* 1 = 0.726069 loss)
I0803 10:36:19.955451 13998 sgd_solver.cpp:105] Iteration 360, lr = 0.0001
I0803 10:36:38.257447 13998 solver.cpp:219] Iteration 380 (1.09279 iter/s, 18.3017s/20 iters), loss = 0.736664
I0803 10:36:38.257534 13998 solver.cpp:238]     Train net output #0: loss = 0.736664 (* 1 = 0.736664 loss)
I0803 10:36:38.257546 13998 sgd_solver.cpp:105] Iteration 380, lr = 0.0001
I0803 10:36:56.549403 13998 solver.cpp:219] Iteration 400 (1.0934 iter/s, 18.2915s/20 iters), loss = 0.769033
I0803 10:36:56.549522 13998 solver.cpp:238]     Train net output #0: loss = 0.769033 (* 1 = 0.769033 loss)
I0803 10:36:56.549538 13998 sgd_solver.cpp:105] Iteration 400, lr = 0.0001
I0803 10:37:14.844969 13998 solver.cpp:219] Iteration 420 (1.09319 iter/s, 18.2952s/20 iters), loss = 0.711155
I0803 10:37:14.845063 13998 solver.cpp:238]     Train net output #0: loss = 0.711155 (* 1 = 0.711155 loss)
I0803 10:37:14.845080 13998 sgd_solver.cpp:105] Iteration 420, lr = 0.0001
I0803 10:37:27.152283 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:37:33.150818 13998 solver.cpp:219] Iteration 440 (1.09258 iter/s, 18.3053s/20 iters), loss = 0.694817
I0803 10:37:33.150988 13998 solver.cpp:238]     Train net output #0: loss = 0.694817 (* 1 = 0.694817 loss)
I0803 10:37:33.151021 13998 sgd_solver.cpp:105] Iteration 440, lr = 0.0001
I0803 10:37:51.463268 13998 solver.cpp:219] Iteration 460 (1.09219 iter/s, 18.3119s/20 iters), loss = 0.759043
I0803 10:37:51.463392 13998 solver.cpp:238]     Train net output #0: loss = 0.759043 (* 1 = 0.759043 loss)
I0803 10:37:51.463413 13998 sgd_solver.cpp:105] Iteration 460, lr = 0.0001
I0803 10:38:09.759883 13998 solver.cpp:219] Iteration 480 (1.09313 iter/s, 18.296s/20 iters), loss = 0.700004
I0803 10:38:09.760061 13998 solver.cpp:238]     Train net output #0: loss = 0.700004 (* 1 = 0.700004 loss)
I0803 10:38:09.760080 13998 sgd_solver.cpp:105] Iteration 480, lr = 0.0001
I0803 10:38:26.615718 13998 solver.cpp:331] Iteration 500, Testing net (#0)
I0803 10:38:28.457705 13998 solver.cpp:398]     Test net output #0: accuracy = 0.621
I0803 10:38:28.457792 13998 solver.cpp:398]     Test net output #1: loss = 0.654086 (* 1 = 0.654086 loss)
I0803 10:38:29.356982 13998 solver.cpp:219] Iteration 500 (1.02059 iter/s, 19.5966s/20 iters), loss = 0.737968
I0803 10:38:29.360095 13998 solver.cpp:238]     Train net output #0: loss = 0.737968 (* 1 = 0.737968 loss)
I0803 10:38:29.360139 13998 sgd_solver.cpp:105] Iteration 500, lr = 0.0001
I0803 10:38:47.677438 13998 solver.cpp:219] Iteration 520 (1.09188 iter/s, 18.317s/20 iters), loss = 0.698136
I0803 10:38:47.677579 13998 solver.cpp:238]     Train net output #0: loss = 0.698136 (* 1 = 0.698136 loss)
I0803 10:38:47.677610 13998 sgd_solver.cpp:105] Iteration 520, lr = 0.0001
I0803 10:39:05.992987 13998 solver.cpp:219] Iteration 540 (1.09199 iter/s, 18.3151s/20 iters), loss = 0.6853
I0803 10:39:05.993105 13998 solver.cpp:238]     Train net output #0: loss = 0.6853 (* 1 = 0.6853 loss)
I0803 10:39:05.993126 13998 sgd_solver.cpp:105] Iteration 540, lr = 0.0001
I0803 10:39:08.212474 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:39:24.307333 13998 solver.cpp:219] Iteration 560 (1.09206 iter/s, 18.3139s/20 iters), loss = 0.664141
I0803 10:39:24.307438 13998 solver.cpp:238]     Train net output #0: loss = 0.664141 (* 1 = 0.664141 loss)
I0803 10:39:24.307456 13998 sgd_solver.cpp:105] Iteration 560, lr = 0.0001
I0803 10:39:42.622824 13998 solver.cpp:219] Iteration 580 (1.092 iter/s, 18.3151s/20 iters), loss = 0.691935
I0803 10:39:42.622907 13998 solver.cpp:238]     Train net output #0: loss = 0.691935 (* 1 = 0.691935 loss)
I0803 10:39:42.622925 13998 sgd_solver.cpp:105] Iteration 580, lr = 0.0001
I0803 10:40:00.936808 13998 solver.cpp:219] Iteration 600 (1.09208 iter/s, 18.3136s/20 iters), loss = 0.672618
I0803 10:40:00.936900 13998 solver.cpp:238]     Train net output #0: loss = 0.672618 (* 1 = 0.672618 loss)
I0803 10:40:00.936918 13998 sgd_solver.cpp:105] Iteration 600, lr = 0.0001
I0803 10:40:19.247684 13998 solver.cpp:219] Iteration 620 (1.09228 iter/s, 18.3104s/20 iters), loss = 0.640616
I0803 10:40:19.247843 13998 solver.cpp:238]     Train net output #0: loss = 0.640616 (* 1 = 0.640616 loss)
I0803 10:40:19.247859 13998 sgd_solver.cpp:105] Iteration 620, lr = 0.0001
I0803 10:40:37.561976 13998 solver.cpp:219] Iteration 640 (1.09207 iter/s, 18.3138s/20 iters), loss = 0.728383
I0803 10:40:37.562075 13998 solver.cpp:238]     Train net output #0: loss = 0.728383 (* 1 = 0.728383 loss)
I0803 10:40:37.562093 13998 sgd_solver.cpp:105] Iteration 640, lr = 0.0001
I0803 10:40:48.141891 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:40:55.863596 13998 solver.cpp:219] Iteration 660 (1.09283 iter/s, 18.3011s/20 iters), loss = 0.687729
I0803 10:40:55.863726 13998 solver.cpp:238]     Train net output #0: loss = 0.687729 (* 1 = 0.687729 loss)
I0803 10:40:55.863755 13998 sgd_solver.cpp:105] Iteration 660, lr = 0.0001
I0803 10:41:14.174123 13998 solver.cpp:219] Iteration 680 (1.09229 iter/s, 18.3101s/20 iters), loss = 0.713643
I0803 10:41:14.174221 13998 solver.cpp:238]     Train net output #0: loss = 0.713643 (* 1 = 0.713643 loss)
I0803 10:41:14.174240 13998 sgd_solver.cpp:105] Iteration 680, lr = 0.0001
I0803 10:41:32.482564 13998 solver.cpp:219] Iteration 700 (1.09242 iter/s, 18.3079s/20 iters), loss = 0.665099
I0803 10:41:32.482707 13998 solver.cpp:238]     Train net output #0: loss = 0.665099 (* 1 = 0.665099 loss)
I0803 10:41:32.482734 13998 sgd_solver.cpp:105] Iteration 700, lr = 0.0001
I0803 10:41:50.801805 13998 solver.cpp:219] Iteration 720 (1.09178 iter/s, 18.3188s/20 iters), loss = 0.634739
I0803 10:41:50.801925 13998 solver.cpp:238]     Train net output #0: loss = 0.634739 (* 1 = 0.634739 loss)
I0803 10:41:50.801956 13998 sgd_solver.cpp:105] Iteration 720, lr = 0.0001
I0803 10:42:09.099097 13998 solver.cpp:219] Iteration 740 (1.09308 iter/s, 18.2969s/20 iters), loss = 0.666602
I0803 10:42:09.099195 13998 solver.cpp:238]     Train net output #0: loss = 0.666602 (* 1 = 0.666602 loss)
I0803 10:42:09.099215 13998 sgd_solver.cpp:105] Iteration 740, lr = 0.0001
I0803 10:42:16.795590 13998 solver.cpp:331] Iteration 750, Testing net (#0)
I0803 10:42:18.643062 13998 solver.cpp:398]     Test net output #0: accuracy = 0.652
I0803 10:42:18.643155 13998 solver.cpp:398]     Test net output #1: loss = 0.629718 (* 1 = 0.629718 loss)
I0803 10:42:28.698993 13998 solver.cpp:219] Iteration 760 (1.02044 iter/s, 19.5994s/20 iters), loss = 0.687717
I0803 10:42:28.699097 13998 solver.cpp:238]     Train net output #0: loss = 0.687717 (* 1 = 0.687717 loss)
I0803 10:42:28.699115 13998 sgd_solver.cpp:105] Iteration 760, lr = 0.0001
I0803 10:42:29.102318 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:42:46.998452 13998 solver.cpp:219] Iteration 780 (1.09295 iter/s, 18.2991s/20 iters), loss = 0.681157
I0803 10:42:46.998564 13998 solver.cpp:238]     Train net output #0: loss = 0.681157 (* 1 = 0.681157 loss)
I0803 10:42:46.998584 13998 sgd_solver.cpp:105] Iteration 780, lr = 0.0001
I0803 10:43:05.313019 13998 solver.cpp:219] Iteration 800 (1.09206 iter/s, 18.3141s/20 iters), loss = 0.711627
I0803 10:43:05.313103 13998 solver.cpp:238]     Train net output #0: loss = 0.711627 (* 1 = 0.711627 loss)
I0803 10:43:05.313117 13998 sgd_solver.cpp:105] Iteration 800, lr = 0.0001
I0803 10:43:23.626452 13998 solver.cpp:219] Iteration 820 (1.09212 iter/s, 18.313s/20 iters), loss = 0.690191
I0803 10:43:23.626574 13998 solver.cpp:238]     Train net output #0: loss = 0.690191 (* 1 = 0.690191 loss)
I0803 10:43:23.626598 13998 sgd_solver.cpp:105] Iteration 820, lr = 0.0001
I0803 10:43:41.927626 13998 solver.cpp:219] Iteration 840 (1.09286 iter/s, 18.3007s/20 iters), loss = 0.650028
I0803 10:43:41.927774 13998 solver.cpp:238]     Train net output #0: loss = 0.650028 (* 1 = 0.650028 loss)
I0803 10:43:41.927814 13998 sgd_solver.cpp:105] Iteration 840, lr = 0.0001
I0803 10:44:00.227191 13998 solver.cpp:219] Iteration 860 (1.09295 iter/s, 18.2991s/20 iters), loss = 0.68584
I0803 10:44:00.227290 13998 solver.cpp:238]     Train net output #0: loss = 0.68584 (* 1 = 0.68584 loss)
I0803 10:44:00.227308 13998 sgd_solver.cpp:105] Iteration 860, lr = 0.0001
I0803 10:44:09.732954 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:44:18.537319 13998 solver.cpp:219] Iteration 880 (1.09232 iter/s, 18.3097s/20 iters), loss = 0.657546
I0803 10:44:18.537416 13998 solver.cpp:238]     Train net output #0: loss = 0.657546 (* 1 = 0.657546 loss)
I0803 10:44:18.537433 13998 sgd_solver.cpp:105] Iteration 880, lr = 0.0001
I0803 10:44:36.855911 13998 solver.cpp:219] Iteration 900 (1.09181 iter/s, 18.3182s/20 iters), loss = 0.635763
I0803 10:44:36.856040 13998 solver.cpp:238]     Train net output #0: loss = 0.635763 (* 1 = 0.635763 loss)
I0803 10:44:36.856060 13998 sgd_solver.cpp:105] Iteration 900, lr = 0.0001
I0803 10:44:55.154044 13998 solver.cpp:219] Iteration 920 (1.09303 iter/s, 18.2977s/20 iters), loss = 0.658678
I0803 10:44:55.154142 13998 solver.cpp:238]     Train net output #0: loss = 0.658678 (* 1 = 0.658678 loss)
I0803 10:44:55.154160 13998 sgd_solver.cpp:105] Iteration 920, lr = 0.0001
I0803 10:45:13.465005 13998 solver.cpp:219] Iteration 940 (1.09227 iter/s, 18.3104s/20 iters), loss = 0.660676
I0803 10:45:13.465154 13998 solver.cpp:238]     Train net output #0: loss = 0.660676 (* 1 = 0.660676 loss)
I0803 10:45:13.465183 13998 sgd_solver.cpp:105] Iteration 940, lr = 0.0001
I0803 10:45:31.769348 13998 solver.cpp:219] Iteration 960 (1.09267 iter/s, 18.3038s/20 iters), loss = 0.644311
I0803 10:45:31.769506 13998 solver.cpp:238]     Train net output #0: loss = 0.644311 (* 1 = 0.644311 loss)
I0803 10:45:31.769525 13998 sgd_solver.cpp:105] Iteration 960, lr = 0.0001
I0803 10:45:49.624877 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:45:50.076648 13998 solver.cpp:219] Iteration 980 (1.09249 iter/s, 18.3069s/20 iters), loss = 0.69317
I0803 10:45:50.076743 13998 solver.cpp:238]     Train net output #0: loss = 0.69317 (* 1 = 0.69317 loss)
I0803 10:45:50.076758 13998 sgd_solver.cpp:105] Iteration 980, lr = 0.0001
I0803 10:46:06.915514 13998 solver.cpp:331] Iteration 1000, Testing net (#0)
I0803 10:46:08.833290 13998 solver.cpp:398]     Test net output #0: accuracy = 0.606
I0803 10:46:08.833413 13998 solver.cpp:398]     Test net output #1: loss = 0.642441 (* 1 = 0.642441 loss)
I0803 10:46:09.733009 13998 solver.cpp:219] Iteration 1000 (1.0175 iter/s, 19.656s/20 iters), loss = 0.636966
I0803 10:46:09.733073 13998 solver.cpp:238]     Train net output #0: loss = 0.636966 (* 1 = 0.636966 loss)
I0803 10:46:09.733096 13998 sgd_solver.cpp:105] Iteration 1000, lr = 0.0001
I0803 10:46:28.034559 13998 solver.cpp:219] Iteration 1020 (1.09283 iter/s, 18.3011s/20 iters), loss = 0.709666
I0803 10:46:28.034710 13998 solver.cpp:238]     Train net output #0: loss = 0.709666 (* 1 = 0.709666 loss)
I0803 10:46:28.034732 13998 sgd_solver.cpp:105] Iteration 1020, lr = 0.0001
I0803 10:46:46.316311 13998 solver.cpp:219] Iteration 1040 (1.09401 iter/s, 18.2813s/20 iters), loss = 0.662536
I0803 10:46:46.331781 13998 solver.cpp:238]     Train net output #0: loss = 0.662536 (* 1 = 0.662536 loss)
I0803 10:46:46.331812 13998 sgd_solver.cpp:105] Iteration 1040, lr = 0.0001
I0803 10:47:04.613489 13998 solver.cpp:219] Iteration 1060 (1.09401 iter/s, 18.2814s/20 iters), loss = 0.638164
I0803 10:47:04.628674 13998 solver.cpp:238]     Train net output #0: loss = 0.638164 (* 1 = 0.638164 loss)
I0803 10:47:04.628708 13998 sgd_solver.cpp:105] Iteration 1060, lr = 0.0001
I0803 10:47:22.952502 13998 solver.cpp:219] Iteration 1080 (1.0915 iter/s, 18.3235s/20 iters), loss = 0.663385
I0803 10:47:22.952677 13998 solver.cpp:238]     Train net output #0: loss = 0.663385 (* 1 = 0.663385 loss)
I0803 10:47:22.952713 13998 sgd_solver.cpp:105] Iteration 1080, lr = 0.0001
I0803 10:47:30.750582 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:47:41.270649 13998 solver.cpp:219] Iteration 1100 (1.09184 iter/s, 18.3176s/20 iters), loss = 0.636141
I0803 10:47:41.270772 13998 solver.cpp:238]     Train net output #0: loss = 0.636141 (* 1 = 0.636141 loss)
I0803 10:47:41.270809 13998 sgd_solver.cpp:105] Iteration 1100, lr = 0.0001
I0803 10:47:59.592788 13998 solver.cpp:219] Iteration 1120 (1.0916 iter/s, 18.3216s/20 iters), loss = 0.660499
I0803 10:47:59.592914 13998 solver.cpp:238]     Train net output #0: loss = 0.660499 (* 1 = 0.660499 loss)
I0803 10:47:59.592934 13998 sgd_solver.cpp:105] Iteration 1120, lr = 0.0001
I0803 10:48:17.898504 13998 solver.cpp:219] Iteration 1140 (1.09259 iter/s, 18.3052s/20 iters), loss = 0.674948
I0803 10:48:17.898682 13998 solver.cpp:238]     Train net output #0: loss = 0.674948 (* 1 = 0.674948 loss)
I0803 10:48:17.898721 13998 sgd_solver.cpp:105] Iteration 1140, lr = 0.0001
I0803 10:48:36.213485 13998 solver.cpp:219] Iteration 1160 (1.09203 iter/s, 18.3144s/20 iters), loss = 0.676087
I0803 10:48:36.213598 13998 solver.cpp:238]     Train net output #0: loss = 0.676087 (* 1 = 0.676087 loss)
I0803 10:48:36.213620 13998 sgd_solver.cpp:105] Iteration 1160, lr = 0.0001
I0803 10:48:54.592798 13998 solver.cpp:219] Iteration 1180 (1.08821 iter/s, 18.3788s/20 iters), loss = 0.651767
I0803 10:48:54.593045 13998 solver.cpp:238]     Train net output #0: loss = 0.651767 (* 1 = 0.651767 loss)
I0803 10:48:54.593089 13998 sgd_solver.cpp:105] Iteration 1180, lr = 0.0001
I0803 10:49:11.491976 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:49:12.943958 13998 solver.cpp:219] Iteration 1200 (1.08988 iter/s, 18.3507s/20 iters), loss = 0.597542
I0803 10:49:12.944077 13998 solver.cpp:238]     Train net output #0: loss = 0.597542 (* 1 = 0.597542 loss)
I0803 10:49:12.944113 13998 sgd_solver.cpp:105] Iteration 1200, lr = 0.0001
I0803 10:49:31.259003 13998 solver.cpp:219] Iteration 1220 (1.09203 iter/s, 18.3146s/20 iters), loss = 0.621686
I0803 10:49:31.259120 13998 solver.cpp:238]     Train net output #0: loss = 0.621686 (* 1 = 0.621686 loss)
I0803 10:49:31.259142 13998 sgd_solver.cpp:105] Iteration 1220, lr = 0.0001
I0803 10:49:52.699975 13998 solver.cpp:219] Iteration 1240 (0.932813 iter/s, 21.4405s/20 iters), loss = 0.656616
I0803 10:49:52.700084 13998 solver.cpp:238]     Train net output #0: loss = 0.656616 (* 1 = 0.656616 loss)
I0803 10:49:52.700103 13998 sgd_solver.cpp:105] Iteration 1240, lr = 0.0001
I0803 10:50:03.369349 13998 solver.cpp:331] Iteration 1250, Testing net (#0)
I0803 10:50:05.046046 14067 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:50:05.945932 13998 solver.cpp:398]     Test net output #0: accuracy = 0.609
I0803 10:50:05.946015 13998 solver.cpp:398]     Test net output #1: loss = 0.622489 (* 1 = 0.622489 loss)
I0803 10:50:19.956446 13998 solver.cpp:219] Iteration 1260 (0.733785 iter/s, 27.256s/20 iters), loss = 0.647889
I0803 10:50:19.956555 13998 solver.cpp:238]     Train net output #0: loss = 0.647889 (* 1 = 0.647889 loss)
I0803 10:50:19.956578 13998 sgd_solver.cpp:105] Iteration 1260, lr = 0.0001
I0803 10:50:41.071198 13998 solver.cpp:219] Iteration 1280 (0.94727 iter/s, 21.1133s/20 iters), loss = 0.605893
I0803 10:50:41.071300 13998 solver.cpp:238]     Train net output #0: loss = 0.605893 (* 1 = 0.605893 loss)
I0803 10:50:41.071319 13998 sgd_solver.cpp:105] Iteration 1280, lr = 0.0001
I0803 10:50:59.391022 13998 solver.cpp:219] Iteration 1300 (1.09174 iter/s, 18.3194s/20 iters), loss = 0.632573
I0803 10:50:59.391135 13998 solver.cpp:238]     Train net output #0: loss = 0.632573 (* 1 = 0.632573 loss)
I0803 10:50:59.391155 13998 sgd_solver.cpp:105] Iteration 1300, lr = 0.0001
I0803 10:51:06.177412 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:51:17.714666 13998 solver.cpp:219] Iteration 1320 (1.09151 iter/s, 18.3232s/20 iters), loss = 0.604793
I0803 10:51:17.714790 13998 solver.cpp:238]     Train net output #0: loss = 0.604793 (* 1 = 0.604793 loss)
I0803 10:51:17.714824 13998 sgd_solver.cpp:105] Iteration 1320, lr = 0.0001
I0803 10:51:36.030699 13998 solver.cpp:219] Iteration 1340 (1.09197 iter/s, 18.3156s/20 iters), loss = 0.646252
I0803 10:51:36.030814 13998 solver.cpp:238]     Train net output #0: loss = 0.646252 (* 1 = 0.646252 loss)
I0803 10:51:36.030846 13998 sgd_solver.cpp:105] Iteration 1340, lr = 0.0001
I0803 10:51:54.346336 13998 solver.cpp:219] Iteration 1360 (1.09199 iter/s, 18.3152s/20 iters), loss = 0.644503
I0803 10:51:54.346462 13998 solver.cpp:238]     Train net output #0: loss = 0.644503 (* 1 = 0.644503 loss)
I0803 10:51:54.346487 13998 sgd_solver.cpp:105] Iteration 1360, lr = 0.0001
I0803 10:52:12.663087 13998 solver.cpp:219] Iteration 1380 (1.09192 iter/s, 18.3164s/20 iters), loss = 0.644859
I0803 10:52:12.663210 13998 solver.cpp:238]     Train net output #0: loss = 0.644859 (* 1 = 0.644859 loss)
I0803 10:52:12.663228 13998 sgd_solver.cpp:105] Iteration 1380, lr = 0.0001
I0803 10:52:31.048981 13998 solver.cpp:219] Iteration 1400 (1.08782 iter/s, 18.3854s/20 iters), loss = 0.645963
I0803 10:52:31.049193 13998 solver.cpp:238]     Train net output #0: loss = 0.645963 (* 1 = 0.645963 loss)
I0803 10:52:31.049235 13998 sgd_solver.cpp:105] Iteration 1400, lr = 0.0001
I0803 10:52:50.646159 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:52:55.265470 13998 solver.cpp:219] Iteration 1420 (0.825903 iter/s, 24.2159s/20 iters), loss = 0.640564
I0803 10:52:55.265588 13998 solver.cpp:238]     Train net output #0: loss = 0.640564 (* 1 = 0.640564 loss)
I0803 10:52:55.265619 13998 sgd_solver.cpp:105] Iteration 1420, lr = 0.0001
I0803 10:53:20.699445 13998 solver.cpp:219] Iteration 1440 (0.786366 iter/s, 25.4335s/20 iters), loss = 0.608701
I0803 10:53:20.699558 13998 solver.cpp:238]     Train net output #0: loss = 0.608701 (* 1 = 0.608701 loss)
I0803 10:53:20.699576 13998 sgd_solver.cpp:105] Iteration 1440, lr = 0.0001
I0803 10:53:46.324242 13998 solver.cpp:219] Iteration 1460 (0.780509 iter/s, 25.6243s/20 iters), loss = 0.637691
I0803 10:53:46.324348 13998 solver.cpp:238]     Train net output #0: loss = 0.637691 (* 1 = 0.637691 loss)
I0803 10:53:46.324368 13998 sgd_solver.cpp:105] Iteration 1460, lr = 0.0001
I0803 10:54:11.930136 13998 solver.cpp:219] Iteration 1480 (0.78112 iter/s, 25.6042s/20 iters), loss = 0.62828
I0803 10:54:11.930246 13998 solver.cpp:238]     Train net output #0: loss = 0.62828 (* 1 = 0.62828 loss)
I0803 10:54:11.930264 13998 sgd_solver.cpp:105] Iteration 1480, lr = 0.0001
I0803 10:54:35.432395 13998 solver.cpp:331] Iteration 1500, Testing net (#0)
I0803 10:54:37.322201 13998 solver.cpp:398]     Test net output #0: accuracy = 0.617
I0803 10:54:37.322290 13998 solver.cpp:398]     Test net output #1: loss = 0.612852 (* 1 = 0.612852 loss)
I0803 10:54:38.220656 13998 solver.cpp:219] Iteration 1500 (0.760745 iter/s, 26.29s/20 iters), loss = 0.590864
I0803 10:54:38.223810 13998 solver.cpp:238]     Train net output #0: loss = 0.590864 (* 1 = 0.590864 loss)
I0803 10:54:38.223839 13998 sgd_solver.cpp:105] Iteration 1500, lr = 1e-05
I0803 10:54:56.545516 13998 solver.cpp:219] Iteration 1520 (1.09162 iter/s, 18.3213s/20 iters), loss = 0.632602
I0803 10:54:56.545639 13998 solver.cpp:238]     Train net output #0: loss = 0.632602 (* 1 = 0.632602 loss)
I0803 10:54:56.545667 13998 sgd_solver.cpp:105] Iteration 1520, lr = 1e-05
I0803 10:55:01.631731 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:55:14.872025 13998 solver.cpp:219] Iteration 1540 (1.09134 iter/s, 18.326s/20 iters), loss = 0.606434
I0803 10:55:14.872203 13998 solver.cpp:238]     Train net output #0: loss = 0.606434 (* 1 = 0.606434 loss)
I0803 10:55:14.872223 13998 sgd_solver.cpp:105] Iteration 1540, lr = 1e-05
I0803 10:55:33.176642 13998 solver.cpp:219] Iteration 1560 (1.09265 iter/s, 18.3042s/20 iters), loss = 0.635113
I0803 10:55:33.176745 13998 solver.cpp:238]     Train net output #0: loss = 0.635113 (* 1 = 0.635113 loss)
I0803 10:55:33.176762 13998 sgd_solver.cpp:105] Iteration 1560, lr = 1e-05
I0803 10:55:51.494307 13998 solver.cpp:219] Iteration 1580 (1.09187 iter/s, 18.3173s/20 iters), loss = 0.612195
I0803 10:55:51.494410 13998 solver.cpp:238]     Train net output #0: loss = 0.612195 (* 1 = 0.612195 loss)
I0803 10:55:51.494424 13998 sgd_solver.cpp:105] Iteration 1580, lr = 1e-05
I0803 10:56:09.807009 13998 solver.cpp:219] Iteration 1600 (1.09217 iter/s, 18.3122s/20 iters), loss = 0.593468
I0803 10:56:09.807191 13998 solver.cpp:238]     Train net output #0: loss = 0.593468 (* 1 = 0.593468 loss)
I0803 10:56:09.807227 13998 sgd_solver.cpp:105] Iteration 1600, lr = 1e-05
I0803 10:56:28.128768 13998 solver.cpp:219] Iteration 1620 (1.09163 iter/s, 18.3212s/20 iters), loss = 0.660874
I0803 10:56:28.128954 13998 solver.cpp:238]     Train net output #0: loss = 0.660874 (* 1 = 0.660874 loss)
I0803 10:56:28.128993 13998 sgd_solver.cpp:105] Iteration 1620, lr = 1e-05
I0803 10:56:42.297180 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:56:46.439477 13998 solver.cpp:219] Iteration 1640 (1.09229 iter/s, 18.3102s/20 iters), loss = 0.665339
I0803 10:56:46.439602 13998 solver.cpp:238]     Train net output #0: loss = 0.665339 (* 1 = 0.665339 loss)
I0803 10:56:46.439623 13998 sgd_solver.cpp:105] Iteration 1640, lr = 1e-05
I0803 10:57:04.762800 13998 solver.cpp:219] Iteration 1660 (1.09153 iter/s, 18.3228s/20 iters), loss = 0.574054
I0803 10:57:04.762980 13998 solver.cpp:238]     Train net output #0: loss = 0.574054 (* 1 = 0.574054 loss)
I0803 10:57:04.763015 13998 sgd_solver.cpp:105] Iteration 1660, lr = 1e-05
I0803 10:57:23.074332 13998 solver.cpp:219] Iteration 1680 (1.09224 iter/s, 18.311s/20 iters), loss = 0.659516
I0803 10:57:23.074452 13998 solver.cpp:238]     Train net output #0: loss = 0.659516 (* 1 = 0.659516 loss)
I0803 10:57:23.074504 13998 sgd_solver.cpp:105] Iteration 1680, lr = 1e-05
I0803 10:57:41.389787 13998 solver.cpp:219] Iteration 1700 (1.092 iter/s, 18.315s/20 iters), loss = 0.646234
I0803 10:57:41.389919 13998 solver.cpp:238]     Train net output #0: loss = 0.646234 (* 1 = 0.646234 loss)
I0803 10:57:41.389956 13998 sgd_solver.cpp:105] Iteration 1700, lr = 1e-05
I0803 10:57:59.707659 13998 solver.cpp:219] Iteration 1720 (1.09185 iter/s, 18.3175s/20 iters), loss = 0.630804
I0803 10:57:59.707747 13998 solver.cpp:238]     Train net output #0: loss = 0.630804 (* 1 = 0.630804 loss)
I0803 10:57:59.707762 13998 sgd_solver.cpp:105] Iteration 1720, lr = 1e-05
I0803 10:58:18.027880 13998 solver.cpp:219] Iteration 1740 (1.09171 iter/s, 18.3198s/20 iters), loss = 0.603592
I0803 10:58:18.028002 13998 solver.cpp:238]     Train net output #0: loss = 0.603592 (* 1 = 0.603592 loss)
I0803 10:58:18.028023 13998 sgd_solver.cpp:105] Iteration 1740, lr = 1e-05
I0803 10:58:22.117202 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 10:58:25.646812 13998 solver.cpp:331] Iteration 1750, Testing net (#0)
I0803 10:58:27.677680 13998 solver.cpp:398]     Test net output #0: accuracy = 0.634
I0803 10:58:27.677764 13998 solver.cpp:398]     Test net output #1: loss = 0.610842 (* 1 = 0.610842 loss)
I0803 10:58:37.744457 13998 solver.cpp:219] Iteration 1760 (1.0144 iter/s, 19.7161s/20 iters), loss = 0.610152
I0803 10:58:37.744637 13998 solver.cpp:238]     Train net output #0: loss = 0.610152 (* 1 = 0.610152 loss)
I0803 10:58:37.744673 13998 sgd_solver.cpp:105] Iteration 1760, lr = 1e-05
I0803 10:58:56.055374 13998 solver.cpp:219] Iteration 1780 (1.09228 iter/s, 18.3104s/20 iters), loss = 0.623142
I0803 10:58:56.055536 13998 solver.cpp:238]     Train net output #0: loss = 0.623142 (* 1 = 0.623142 loss)
I0803 10:58:56.055578 13998 sgd_solver.cpp:105] Iteration 1780, lr = 1e-05
I0803 10:59:14.375684 13998 solver.cpp:219] Iteration 1800 (1.09171 iter/s, 18.3198s/20 iters), loss = 0.632575
I0803 10:59:14.375795 13998 solver.cpp:238]     Train net output #0: loss = 0.632575 (* 1 = 0.632575 loss)
I0803 10:59:14.375815 13998 sgd_solver.cpp:105] Iteration 1800, lr = 1e-05
I0803 10:59:32.693403 13998 solver.cpp:219] Iteration 1820 (1.09187 iter/s, 18.3172s/20 iters), loss = 0.626205
I0803 10:59:32.693609 13998 solver.cpp:238]     Train net output #0: loss = 0.626205 (* 1 = 0.626205 loss)
I0803 10:59:32.693655 13998 sgd_solver.cpp:105] Iteration 1820, lr = 1e-05
I0803 10:59:51.021385 13998 solver.cpp:219] Iteration 1840 (1.09126 iter/s, 18.3275s/20 iters), loss = 0.593682
I0803 10:59:51.021526 13998 solver.cpp:238]     Train net output #0: loss = 0.593682 (* 1 = 0.593682 loss)
I0803 10:59:51.021548 13998 sgd_solver.cpp:105] Iteration 1840, lr = 1e-05
I0803 11:00:03.445350 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:00:09.337257 13998 solver.cpp:219] Iteration 1860 (1.09198 iter/s, 18.3154s/20 iters), loss = 0.61359
I0803 11:00:09.337383 13998 solver.cpp:238]     Train net output #0: loss = 0.61359 (* 1 = 0.61359 loss)
I0803 11:00:09.337402 13998 sgd_solver.cpp:105] Iteration 1860, lr = 1e-05
I0803 11:00:27.656280 13998 solver.cpp:219] Iteration 1880 (1.09179 iter/s, 18.3185s/20 iters), loss = 0.626093
I0803 11:00:27.656455 13998 solver.cpp:238]     Train net output #0: loss = 0.626093 (* 1 = 0.626093 loss)
I0803 11:00:27.656491 13998 sgd_solver.cpp:105] Iteration 1880, lr = 1e-05
I0803 11:00:45.971478 13998 solver.cpp:219] Iteration 1900 (1.09202 iter/s, 18.3147s/20 iters), loss = 0.586329
I0803 11:00:45.971606 13998 solver.cpp:238]     Train net output #0: loss = 0.586329 (* 1 = 0.586329 loss)
I0803 11:00:45.971632 13998 sgd_solver.cpp:105] Iteration 1900, lr = 1e-05
I0803 11:01:04.290205 13998 solver.cpp:219] Iteration 1920 (1.0918 iter/s, 18.3183s/20 iters), loss = 0.58554
I0803 11:01:04.290325 13998 solver.cpp:238]     Train net output #0: loss = 0.58554 (* 1 = 0.58554 loss)
I0803 11:01:04.290344 13998 sgd_solver.cpp:105] Iteration 1920, lr = 1e-05
I0803 11:01:22.610618 13998 solver.cpp:219] Iteration 1940 (1.09171 iter/s, 18.3199s/20 iters), loss = 0.582476
I0803 11:01:22.610738 13998 solver.cpp:238]     Train net output #0: loss = 0.582476 (* 1 = 0.582476 loss)
I0803 11:01:22.610769 13998 sgd_solver.cpp:105] Iteration 1940, lr = 1e-05
I0803 11:01:40.925012 13998 solver.cpp:219] Iteration 1960 (1.09207 iter/s, 18.3139s/20 iters), loss = 0.62448
I0803 11:01:40.925194 13998 solver.cpp:238]     Train net output #0: loss = 0.62448 (* 1 = 0.62448 loss)
I0803 11:01:40.925226 13998 sgd_solver.cpp:105] Iteration 1960, lr = 1e-05
I0803 11:01:44.068755 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:01:59.241264 13998 solver.cpp:219] Iteration 1980 (1.09196 iter/s, 18.3157s/20 iters), loss = 0.616386
I0803 11:01:59.241451 13998 solver.cpp:238]     Train net output #0: loss = 0.616386 (* 1 = 0.616386 loss)
I0803 11:01:59.241487 13998 sgd_solver.cpp:105] Iteration 1980, lr = 1e-05
I0803 11:02:16.030473 13998 solver.cpp:331] Iteration 2000, Testing net (#0)
I0803 11:02:17.961748 13998 solver.cpp:398]     Test net output #0: accuracy = 0.639
I0803 11:02:17.961823 13998 solver.cpp:398]     Test net output #1: loss = 0.604041 (* 1 = 0.604041 loss)
I0803 11:02:18.857774 13998 solver.cpp:219] Iteration 2000 (1.01957 iter/s, 19.616s/20 iters), loss = 0.624727
I0803 11:02:18.860911 13998 solver.cpp:238]     Train net output #0: loss = 0.624727 (* 1 = 0.624727 loss)
I0803 11:02:18.860949 13998 sgd_solver.cpp:105] Iteration 2000, lr = 1e-05
I0803 11:02:37.176415 13998 solver.cpp:219] Iteration 2020 (1.09199 iter/s, 18.3152s/20 iters), loss = 0.588761
I0803 11:02:37.176568 13998 solver.cpp:238]     Train net output #0: loss = 0.588761 (* 1 = 0.588761 loss)
I0803 11:02:37.176589 13998 sgd_solver.cpp:105] Iteration 2020, lr = 1e-05
I0803 11:02:55.483223 13998 solver.cpp:219] Iteration 2040 (1.09252 iter/s, 18.3063s/20 iters), loss = 0.586599
I0803 11:02:55.483348 13998 solver.cpp:238]     Train net output #0: loss = 0.586599 (* 1 = 0.586599 loss)
I0803 11:02:55.483378 13998 sgd_solver.cpp:105] Iteration 2040, lr = 1e-05
I0803 11:03:13.803297 13998 solver.cpp:219] Iteration 2060 (1.09173 iter/s, 18.3195s/20 iters), loss = 0.679181
I0803 11:03:13.803515 13998 solver.cpp:238]     Train net output #0: loss = 0.679181 (* 1 = 0.679181 loss)
I0803 11:03:13.803571 13998 sgd_solver.cpp:105] Iteration 2060, lr = 1e-05
I0803 11:03:25.173893 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:03:32.122592 13998 solver.cpp:219] Iteration 2080 (1.09177 iter/s, 18.3188s/20 iters), loss = 0.672011
I0803 11:03:32.122715 13998 solver.cpp:238]     Train net output #0: loss = 0.672011 (* 1 = 0.672011 loss)
I0803 11:03:32.122737 13998 sgd_solver.cpp:105] Iteration 2080, lr = 1e-05
I0803 11:03:50.455452 13998 solver.cpp:219] Iteration 2100 (1.09096 iter/s, 18.3324s/20 iters), loss = 0.605071
I0803 11:03:50.455575 13998 solver.cpp:238]     Train net output #0: loss = 0.605071 (* 1 = 0.605071 loss)
I0803 11:03:50.455595 13998 sgd_solver.cpp:105] Iteration 2100, lr = 1e-05
I0803 11:04:08.778501 13998 solver.cpp:219] Iteration 2120 (1.09155 iter/s, 18.3225s/20 iters), loss = 0.618404
I0803 11:04:08.778681 13998 solver.cpp:238]     Train net output #0: loss = 0.618404 (* 1 = 0.618404 loss)
I0803 11:04:08.778717 13998 sgd_solver.cpp:105] Iteration 2120, lr = 1e-05
I0803 11:04:27.099345 13998 solver.cpp:219] Iteration 2140 (1.09168 iter/s, 18.3203s/20 iters), loss = 0.597798
I0803 11:04:27.099467 13998 solver.cpp:238]     Train net output #0: loss = 0.597798 (* 1 = 0.597798 loss)
I0803 11:04:27.099496 13998 sgd_solver.cpp:105] Iteration 2140, lr = 1e-05
I0803 11:04:45.419911 13998 solver.cpp:219] Iteration 2160 (1.0917 iter/s, 18.3201s/20 iters), loss = 0.596293
I0803 11:04:45.420035 13998 solver.cpp:238]     Train net output #0: loss = 0.596293 (* 1 = 0.596293 loss)
I0803 11:04:45.420055 13998 sgd_solver.cpp:105] Iteration 2160, lr = 1e-05
I0803 11:05:03.731717 13998 solver.cpp:219] Iteration 2180 (1.09222 iter/s, 18.3114s/20 iters), loss = 0.608126
I0803 11:05:03.731818 13998 solver.cpp:238]     Train net output #0: loss = 0.608126 (* 1 = 0.608126 loss)
I0803 11:05:03.731837 13998 sgd_solver.cpp:105] Iteration 2180, lr = 1e-05
I0803 11:05:05.185696 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:05:22.052199 13998 solver.cpp:219] Iteration 2200 (1.0917 iter/s, 18.3201s/20 iters), loss = 0.612231
I0803 11:05:22.052332 13998 solver.cpp:238]     Train net output #0: loss = 0.612231 (* 1 = 0.612231 loss)
I0803 11:05:22.052364 13998 sgd_solver.cpp:105] Iteration 2200, lr = 1e-05
I0803 11:05:40.349047 13998 solver.cpp:219] Iteration 2220 (1.09311 iter/s, 18.2964s/20 iters), loss = 0.61759
I0803 11:05:40.349150 13998 solver.cpp:238]     Train net output #0: loss = 0.61759 (* 1 = 0.61759 loss)
I0803 11:05:40.349169 13998 sgd_solver.cpp:105] Iteration 2220, lr = 1e-05
I0803 11:05:58.658608 13998 solver.cpp:219] Iteration 2240 (1.09235 iter/s, 18.3092s/20 iters), loss = 0.633635
I0803 11:05:58.658733 13998 solver.cpp:238]     Train net output #0: loss = 0.633635 (* 1 = 0.633635 loss)
I0803 11:05:58.658753 13998 sgd_solver.cpp:105] Iteration 2240, lr = 1e-05
I0803 11:06:06.296535 13998 solver.cpp:331] Iteration 2250, Testing net (#0)
I0803 11:06:08.305722 13998 solver.cpp:398]     Test net output #0: accuracy = 0.648
I0803 11:06:08.305843 13998 solver.cpp:398]     Test net output #1: loss = 0.602993 (* 1 = 0.602993 loss)
I0803 11:06:18.377071 13998 solver.cpp:219] Iteration 2260 (1.0143 iter/s, 19.718s/20 iters), loss = 0.661877
I0803 11:06:18.377229 13998 solver.cpp:238]     Train net output #0: loss = 0.661877 (* 1 = 0.661877 loss)
I0803 11:06:18.377254 13998 sgd_solver.cpp:105] Iteration 2260, lr = 1e-05
I0803 11:06:36.697741 13998 solver.cpp:219] Iteration 2280 (1.09169 iter/s, 18.3202s/20 iters), loss = 0.611205
I0803 11:06:36.697904 13998 solver.cpp:238]     Train net output #0: loss = 0.611205 (* 1 = 0.611205 loss)
I0803 11:06:36.697943 13998 sgd_solver.cpp:105] Iteration 2280, lr = 1e-05
I0803 11:06:46.458140 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:06:55.021791 13998 solver.cpp:219] Iteration 2300 (1.09149 iter/s, 18.3235s/20 iters), loss = 0.631137
I0803 11:06:55.021981 13998 solver.cpp:238]     Train net output #0: loss = 0.631137 (* 1 = 0.631137 loss)
I0803 11:06:55.022030 13998 sgd_solver.cpp:105] Iteration 2300, lr = 1e-05
I0803 11:07:13.342965 13998 solver.cpp:219] Iteration 2320 (1.09166 iter/s, 18.3207s/20 iters), loss = 0.625835
I0803 11:07:13.343091 13998 solver.cpp:238]     Train net output #0: loss = 0.625835 (* 1 = 0.625835 loss)
I0803 11:07:13.343111 13998 sgd_solver.cpp:105] Iteration 2320, lr = 1e-05
I0803 11:07:31.652732 13998 solver.cpp:219] Iteration 2340 (1.09234 iter/s, 18.3094s/20 iters), loss = 0.637076
I0803 11:07:31.652850 13998 solver.cpp:238]     Train net output #0: loss = 0.637076 (* 1 = 0.637076 loss)
I0803 11:07:31.652868 13998 sgd_solver.cpp:105] Iteration 2340, lr = 1e-05
I0803 11:07:49.977401 13998 solver.cpp:219] Iteration 2360 (1.09145 iter/s, 18.3242s/20 iters), loss = 0.584446
I0803 11:07:49.977504 13998 solver.cpp:238]     Train net output #0: loss = 0.584446 (* 1 = 0.584446 loss)
I0803 11:07:49.977524 13998 sgd_solver.cpp:105] Iteration 2360, lr = 1e-05
I0803 11:08:08.291594 13998 solver.cpp:219] Iteration 2380 (1.09207 iter/s, 18.3138s/20 iters), loss = 0.618721
I0803 11:08:08.291735 13998 solver.cpp:238]     Train net output #0: loss = 0.618721 (* 1 = 0.618721 loss)
I0803 11:08:08.291772 13998 sgd_solver.cpp:105] Iteration 2380, lr = 1e-05
I0803 11:08:26.611054 13998 solver.cpp:219] Iteration 2400 (1.09177 iter/s, 18.3189s/20 iters), loss = 0.615656
I0803 11:08:26.611233 13998 solver.cpp:238]     Train net output #0: loss = 0.615656 (* 1 = 0.615656 loss)
I0803 11:08:26.611268 13998 sgd_solver.cpp:105] Iteration 2400, lr = 1e-05
I0803 11:08:26.878865 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:08:44.929157 13998 solver.cpp:219] Iteration 2420 (1.09184 iter/s, 18.3176s/20 iters), loss = 0.596125
I0803 11:08:44.929285 13998 solver.cpp:238]     Train net output #0: loss = 0.596125 (* 1 = 0.596125 loss)
I0803 11:08:44.929316 13998 sgd_solver.cpp:105] Iteration 2420, lr = 1e-05
I0803 11:09:03.234432 13998 solver.cpp:219] Iteration 2440 (1.09261 iter/s, 18.3048s/20 iters), loss = 0.64061
I0803 11:09:03.234541 13998 solver.cpp:238]     Train net output #0: loss = 0.64061 (* 1 = 0.64061 loss)
I0803 11:09:03.234562 13998 sgd_solver.cpp:105] Iteration 2440, lr = 1e-05
I0803 11:09:21.550822 13998 solver.cpp:219] Iteration 2460 (1.09195 iter/s, 18.3159s/20 iters), loss = 0.585885
I0803 11:09:21.550997 13998 solver.cpp:238]     Train net output #0: loss = 0.585885 (* 1 = 0.585885 loss)
I0803 11:09:21.551025 13998 sgd_solver.cpp:105] Iteration 2460, lr = 1e-05
I0803 11:09:39.858048 13998 solver.cpp:219] Iteration 2480 (1.09249 iter/s, 18.3068s/20 iters), loss = 0.589129
I0803 11:09:39.858172 13998 solver.cpp:238]     Train net output #0: loss = 0.589129 (* 1 = 0.589129 loss)
I0803 11:09:39.858192 13998 sgd_solver.cpp:105] Iteration 2480, lr = 1e-05
I0803 11:09:56.634876 13998 solver.cpp:331] Iteration 2500, Testing net (#0)
I0803 11:09:58.660451 13998 solver.cpp:398]     Test net output #0: accuracy = 0.633
I0803 11:09:58.660559 13998 solver.cpp:398]     Test net output #1: loss = 0.602955 (* 1 = 0.602955 loss)
I0803 11:09:59.554744 13998 solver.cpp:219] Iteration 2500 (1.01542 iter/s, 19.6963s/20 iters), loss = 0.600227
I0803 11:09:59.557914 13998 solver.cpp:238]     Train net output #0: loss = 0.600227 (* 1 = 0.600227 loss)
I0803 11:09:59.557960 13998 sgd_solver.cpp:105] Iteration 2500, lr = 1e-05
I0803 11:09:59.573307 14067 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:10:08.284471 14066 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:10:17.879984 13998 solver.cpp:219] Iteration 2520 (1.0916 iter/s, 18.3217s/20 iters), loss = 0.587093
I0803 11:10:17.880162 13998 solver.cpp:238]     Train net output #0: loss = 0.587093 (* 1 = 0.587093 loss)
I0803 11:10:17.880203 13998 sgd_solver.cpp:105] Iteration 2520, lr = 1e-05
I0803 11:10:36.210759 13998 solver.cpp:219] Iteration 2540 (1.09109 iter/s, 18.3303s/20 iters), loss = 0.649875
I0803 11:10:36.210881 13998 solver.cpp:238]     Train net output #0: loss = 0.649875 (* 1 = 0.649875 loss)
I0803 11:10:36.210916 13998 sgd_solver.cpp:105] Iteration 2540, lr = 1e-05
