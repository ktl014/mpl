WARNING: Logging before InitGoogleLogging() is written to STDERR
I0803 11:16:27.448272 14409 solver.cpp:44] Initializing solver from parameters: 
test_iter: 20
test_interval: 250
base_lr: 0.001
display: 20
max_iter: 5000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1500
snapshot: 5000
snapshot_prefix: "caffenet_train"
solver_mode: GPU
net: "caffenet/train_val.prototxt"
I0803 11:16:27.449290 14409 solver.cpp:87] Creating training net from net file: caffenet/train_val.prototxt
I0803 11:16:27.450736 14409 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0803 11:16:27.450775 14409 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0803 11:16:27.450976 14409 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "train.LMDB"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 11:16:27.451084 14409 layer_factory.hpp:77] Creating layer data
I0803 11:16:27.453292 14409 db_lmdb.cpp:35] Opened lmdb train.LMDB
I0803 11:16:27.453474 14409 net.cpp:86] Creating Layer data
I0803 11:16:27.453490 14409 net.cpp:382] data -> data
I0803 11:16:27.453513 14409 net.cpp:382] data -> label
I0803 11:16:27.454948 14409 data_layer.cpp:45] output data size: 256,3,227,227
I0803 11:16:27.871841 14409 net.cpp:124] Setting up data
I0803 11:16:27.871902 14409 net.cpp:131] Top shape: 256 3 227 227 (39574272)
I0803 11:16:27.871912 14409 net.cpp:131] Top shape: 256 (256)
I0803 11:16:27.871917 14409 net.cpp:139] Memory required for data: 158298112
I0803 11:16:27.871927 14409 layer_factory.hpp:77] Creating layer conv1
I0803 11:16:27.871966 14409 net.cpp:86] Creating Layer conv1
I0803 11:16:27.871976 14409 net.cpp:408] conv1 <- data
I0803 11:16:27.871990 14409 net.cpp:382] conv1 -> conv1
I0803 11:16:28.177271 14409 net.cpp:124] Setting up conv1
I0803 11:16:28.177326 14409 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 11:16:28.177333 14409 net.cpp:139] Memory required for data: 455667712
I0803 11:16:28.177356 14409 layer_factory.hpp:77] Creating layer relu1
I0803 11:16:28.177372 14409 net.cpp:86] Creating Layer relu1
I0803 11:16:28.177392 14409 net.cpp:408] relu1 <- conv1
I0803 11:16:28.177402 14409 net.cpp:369] relu1 -> conv1 (in-place)
I0803 11:16:28.178167 14409 net.cpp:124] Setting up relu1
I0803 11:16:28.178184 14409 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 11:16:28.178189 14409 net.cpp:139] Memory required for data: 753037312
I0803 11:16:28.178207 14409 layer_factory.hpp:77] Creating layer pool1
I0803 11:16:28.178217 14409 net.cpp:86] Creating Layer pool1
I0803 11:16:28.178222 14409 net.cpp:408] pool1 <- conv1
I0803 11:16:28.178230 14409 net.cpp:382] pool1 -> pool1
I0803 11:16:28.178299 14409 net.cpp:124] Setting up pool1
I0803 11:16:28.178313 14409 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 11:16:28.178318 14409 net.cpp:139] Memory required for data: 824700928
I0803 11:16:28.178323 14409 layer_factory.hpp:77] Creating layer norm1
I0803 11:16:28.178336 14409 net.cpp:86] Creating Layer norm1
I0803 11:16:28.178344 14409 net.cpp:408] norm1 <- pool1
I0803 11:16:28.178350 14409 net.cpp:382] norm1 -> norm1
I0803 11:16:28.178589 14409 net.cpp:124] Setting up norm1
I0803 11:16:28.178606 14409 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 11:16:28.178612 14409 net.cpp:139] Memory required for data: 896364544
I0803 11:16:28.178618 14409 layer_factory.hpp:77] Creating layer conv2
I0803 11:16:28.178635 14409 net.cpp:86] Creating Layer conv2
I0803 11:16:28.178642 14409 net.cpp:408] conv2 <- norm1
I0803 11:16:28.178650 14409 net.cpp:382] conv2 -> conv2
I0803 11:16:28.194979 14409 net.cpp:124] Setting up conv2
I0803 11:16:28.194999 14409 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 11:16:28.195006 14409 net.cpp:139] Memory required for data: 1087467520
I0803 11:16:28.195020 14409 layer_factory.hpp:77] Creating layer relu2
I0803 11:16:28.195030 14409 net.cpp:86] Creating Layer relu2
I0803 11:16:28.195037 14409 net.cpp:408] relu2 <- conv2
I0803 11:16:28.195047 14409 net.cpp:369] relu2 -> conv2 (in-place)
I0803 11:16:28.195276 14409 net.cpp:124] Setting up relu2
I0803 11:16:28.195291 14409 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 11:16:28.195297 14409 net.cpp:139] Memory required for data: 1278570496
I0803 11:16:28.195303 14409 layer_factory.hpp:77] Creating layer pool2
I0803 11:16:28.195313 14409 net.cpp:86] Creating Layer pool2
I0803 11:16:28.195320 14409 net.cpp:408] pool2 <- conv2
I0803 11:16:28.195327 14409 net.cpp:382] pool2 -> pool2
I0803 11:16:28.195381 14409 net.cpp:124] Setting up pool2
I0803 11:16:28.195394 14409 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 11:16:28.195400 14409 net.cpp:139] Memory required for data: 1322872832
I0803 11:16:28.195405 14409 layer_factory.hpp:77] Creating layer norm2
I0803 11:16:28.195420 14409 net.cpp:86] Creating Layer norm2
I0803 11:16:28.195425 14409 net.cpp:408] norm2 <- pool2
I0803 11:16:28.195433 14409 net.cpp:382] norm2 -> norm2
I0803 11:16:28.196236 14409 net.cpp:124] Setting up norm2
I0803 11:16:28.196254 14409 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 11:16:28.196259 14409 net.cpp:139] Memory required for data: 1367175168
I0803 11:16:28.196265 14409 layer_factory.hpp:77] Creating layer conv3
I0803 11:16:28.196281 14409 net.cpp:86] Creating Layer conv3
I0803 11:16:28.196288 14409 net.cpp:408] conv3 <- norm2
I0803 11:16:28.196300 14409 net.cpp:382] conv3 -> conv3
I0803 11:16:28.236941 14409 net.cpp:124] Setting up conv3
I0803 11:16:28.236976 14409 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 11:16:28.236982 14409 net.cpp:139] Memory required for data: 1433628672
I0803 11:16:28.237000 14409 layer_factory.hpp:77] Creating layer relu3
I0803 11:16:28.237020 14409 net.cpp:86] Creating Layer relu3
I0803 11:16:28.237030 14409 net.cpp:408] relu3 <- conv3
I0803 11:16:28.237038 14409 net.cpp:369] relu3 -> conv3 (in-place)
I0803 11:16:28.237267 14409 net.cpp:124] Setting up relu3
I0803 11:16:28.237282 14409 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 11:16:28.237287 14409 net.cpp:139] Memory required for data: 1500082176
I0803 11:16:28.237294 14409 layer_factory.hpp:77] Creating layer conv4
I0803 11:16:28.237313 14409 net.cpp:86] Creating Layer conv4
I0803 11:16:28.237319 14409 net.cpp:408] conv4 <- conv3
I0803 11:16:28.237330 14409 net.cpp:382] conv4 -> conv4
I0803 11:16:28.266978 14409 net.cpp:124] Setting up conv4
I0803 11:16:28.266999 14409 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 11:16:28.267004 14409 net.cpp:139] Memory required for data: 1566535680
I0803 11:16:28.267014 14409 layer_factory.hpp:77] Creating layer relu4
I0803 11:16:28.267025 14409 net.cpp:86] Creating Layer relu4
I0803 11:16:28.267030 14409 net.cpp:408] relu4 <- conv4
I0803 11:16:28.267037 14409 net.cpp:369] relu4 -> conv4 (in-place)
I0803 11:16:28.267256 14409 net.cpp:124] Setting up relu4
I0803 11:16:28.267271 14409 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 11:16:28.267277 14409 net.cpp:139] Memory required for data: 1632989184
I0803 11:16:28.267282 14409 layer_factory.hpp:77] Creating layer conv5
I0803 11:16:28.267302 14409 net.cpp:86] Creating Layer conv5
I0803 11:16:28.267308 14409 net.cpp:408] conv5 <- conv4
I0803 11:16:28.267316 14409 net.cpp:382] conv5 -> conv5
I0803 11:16:28.288440 14409 net.cpp:124] Setting up conv5
I0803 11:16:28.288458 14409 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 11:16:28.288477 14409 net.cpp:139] Memory required for data: 1677291520
I0803 11:16:28.288493 14409 layer_factory.hpp:77] Creating layer relu5
I0803 11:16:28.288504 14409 net.cpp:86] Creating Layer relu5
I0803 11:16:28.288511 14409 net.cpp:408] relu5 <- conv5
I0803 11:16:28.288519 14409 net.cpp:369] relu5 -> conv5 (in-place)
I0803 11:16:28.288743 14409 net.cpp:124] Setting up relu5
I0803 11:16:28.288758 14409 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 11:16:28.288764 14409 net.cpp:139] Memory required for data: 1721593856
I0803 11:16:28.288770 14409 layer_factory.hpp:77] Creating layer pool5
I0803 11:16:28.288784 14409 net.cpp:86] Creating Layer pool5
I0803 11:16:28.288790 14409 net.cpp:408] pool5 <- conv5
I0803 11:16:28.288800 14409 net.cpp:382] pool5 -> pool5
I0803 11:16:28.288857 14409 net.cpp:124] Setting up pool5
I0803 11:16:28.288872 14409 net.cpp:131] Top shape: 256 256 6 6 (2359296)
I0803 11:16:28.288878 14409 net.cpp:139] Memory required for data: 1731031040
I0803 11:16:28.288883 14409 layer_factory.hpp:77] Creating layer fc6
I0803 11:16:28.288902 14409 net.cpp:86] Creating Layer fc6
I0803 11:16:28.288908 14409 net.cpp:408] fc6 <- pool5
I0803 11:16:28.288919 14409 net.cpp:382] fc6 -> fc6
I0803 11:16:29.710117 14409 net.cpp:124] Setting up fc6
I0803 11:16:29.710181 14409 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 11:16:29.710188 14409 net.cpp:139] Memory required for data: 1735225344
I0803 11:16:29.710203 14409 layer_factory.hpp:77] Creating layer relu6
I0803 11:16:29.710217 14409 net.cpp:86] Creating Layer relu6
I0803 11:16:29.710223 14409 net.cpp:408] relu6 <- fc6
I0803 11:16:29.710233 14409 net.cpp:369] relu6 -> fc6 (in-place)
I0803 11:16:29.711223 14409 net.cpp:124] Setting up relu6
I0803 11:16:29.711251 14409 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 11:16:29.711257 14409 net.cpp:139] Memory required for data: 1739419648
I0803 11:16:29.711262 14409 layer_factory.hpp:77] Creating layer drop6
I0803 11:16:29.711285 14409 net.cpp:86] Creating Layer drop6
I0803 11:16:29.711293 14409 net.cpp:408] drop6 <- fc6
I0803 11:16:29.711302 14409 net.cpp:369] drop6 -> fc6 (in-place)
I0803 11:16:29.711338 14409 net.cpp:124] Setting up drop6
I0803 11:16:29.711349 14409 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 11:16:29.711354 14409 net.cpp:139] Memory required for data: 1743613952
I0803 11:16:29.711359 14409 layer_factory.hpp:77] Creating layer fc7
I0803 11:16:29.711371 14409 net.cpp:86] Creating Layer fc7
I0803 11:16:29.711377 14409 net.cpp:408] fc7 <- fc6
I0803 11:16:29.711387 14409 net.cpp:382] fc7 -> fc7
I0803 11:16:30.344097 14409 net.cpp:124] Setting up fc7
I0803 11:16:30.344154 14409 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 11:16:30.344161 14409 net.cpp:139] Memory required for data: 1747808256
I0803 11:16:30.344174 14409 layer_factory.hpp:77] Creating layer relu7
I0803 11:16:30.344189 14409 net.cpp:86] Creating Layer relu7
I0803 11:16:30.344195 14409 net.cpp:408] relu7 <- fc7
I0803 11:16:30.344208 14409 net.cpp:369] relu7 -> fc7 (in-place)
I0803 11:16:30.345165 14409 net.cpp:124] Setting up relu7
I0803 11:16:30.345193 14409 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 11:16:30.345198 14409 net.cpp:139] Memory required for data: 1752002560
I0803 11:16:30.345204 14409 layer_factory.hpp:77] Creating layer drop7
I0803 11:16:30.345214 14409 net.cpp:86] Creating Layer drop7
I0803 11:16:30.345218 14409 net.cpp:408] drop7 <- fc7
I0803 11:16:30.345230 14409 net.cpp:369] drop7 -> fc7 (in-place)
I0803 11:16:30.345263 14409 net.cpp:124] Setting up drop7
I0803 11:16:30.345274 14409 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 11:16:30.345280 14409 net.cpp:139] Memory required for data: 1756196864
I0803 11:16:30.345285 14409 layer_factory.hpp:77] Creating layer fc8
I0803 11:16:30.345299 14409 net.cpp:86] Creating Layer fc8
I0803 11:16:30.345304 14409 net.cpp:408] fc8 <- fc7
I0803 11:16:30.345312 14409 net.cpp:382] fc8 -> fc8
I0803 11:16:30.346426 14409 net.cpp:124] Setting up fc8
I0803 11:16:30.346441 14409 net.cpp:131] Top shape: 256 2 (512)
I0803 11:16:30.346447 14409 net.cpp:139] Memory required for data: 1756198912
I0803 11:16:30.346457 14409 layer_factory.hpp:77] Creating layer loss
I0803 11:16:30.346467 14409 net.cpp:86] Creating Layer loss
I0803 11:16:30.346472 14409 net.cpp:408] loss <- fc8
I0803 11:16:30.346485 14409 net.cpp:408] loss <- label
I0803 11:16:30.346493 14409 net.cpp:382] loss -> loss
I0803 11:16:30.346509 14409 layer_factory.hpp:77] Creating layer loss
I0803 11:16:30.346837 14409 net.cpp:124] Setting up loss
I0803 11:16:30.346853 14409 net.cpp:131] Top shape: (1)
I0803 11:16:30.346858 14409 net.cpp:134]     with loss weight 1
I0803 11:16:30.346884 14409 net.cpp:139] Memory required for data: 1756198916
I0803 11:16:30.346890 14409 net.cpp:200] loss needs backward computation.
I0803 11:16:30.346896 14409 net.cpp:200] fc8 needs backward computation.
I0803 11:16:30.346901 14409 net.cpp:200] drop7 needs backward computation.
I0803 11:16:30.346906 14409 net.cpp:200] relu7 needs backward computation.
I0803 11:16:30.346910 14409 net.cpp:200] fc7 needs backward computation.
I0803 11:16:30.346915 14409 net.cpp:200] drop6 needs backward computation.
I0803 11:16:30.346920 14409 net.cpp:200] relu6 needs backward computation.
I0803 11:16:30.346925 14409 net.cpp:200] fc6 needs backward computation.
I0803 11:16:30.346930 14409 net.cpp:200] pool5 needs backward computation.
I0803 11:16:30.346936 14409 net.cpp:200] relu5 needs backward computation.
I0803 11:16:30.346940 14409 net.cpp:200] conv5 needs backward computation.
I0803 11:16:30.346947 14409 net.cpp:200] relu4 needs backward computation.
I0803 11:16:30.346951 14409 net.cpp:200] conv4 needs backward computation.
I0803 11:16:30.346958 14409 net.cpp:200] relu3 needs backward computation.
I0803 11:16:30.346962 14409 net.cpp:200] conv3 needs backward computation.
I0803 11:16:30.346969 14409 net.cpp:200] norm2 needs backward computation.
I0803 11:16:30.346973 14409 net.cpp:200] pool2 needs backward computation.
I0803 11:16:30.346979 14409 net.cpp:200] relu2 needs backward computation.
I0803 11:16:30.346983 14409 net.cpp:200] conv2 needs backward computation.
I0803 11:16:30.346990 14409 net.cpp:200] norm1 needs backward computation.
I0803 11:16:30.346997 14409 net.cpp:200] pool1 needs backward computation.
I0803 11:16:30.347002 14409 net.cpp:200] relu1 needs backward computation.
I0803 11:16:30.347007 14409 net.cpp:200] conv1 needs backward computation.
I0803 11:16:30.347012 14409 net.cpp:202] data does not need backward computation.
I0803 11:16:30.347018 14409 net.cpp:244] This network produces output loss
I0803 11:16:30.347038 14409 net.cpp:257] Network initialization done.
I0803 11:16:30.348430 14409 solver.cpp:173] Creating test net (#0) specified by net file: caffenet/train_val.prototxt
I0803 11:16:30.348490 14409 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0803 11:16:30.348759 14409 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "val.LMDB"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 11:16:30.348896 14409 layer_factory.hpp:77] Creating layer data
I0803 11:16:30.350726 14409 db_lmdb.cpp:35] Opened lmdb val.LMDB
I0803 11:16:30.350908 14409 net.cpp:86] Creating Layer data
I0803 11:16:30.350922 14409 net.cpp:382] data -> data
I0803 11:16:30.350936 14409 net.cpp:382] data -> label
I0803 11:16:30.351305 14409 data_layer.cpp:45] output data size: 50,3,227,227
I0803 11:16:30.438508 14409 net.cpp:124] Setting up data
I0803 11:16:30.438555 14409 net.cpp:131] Top shape: 50 3 227 227 (7729350)
I0803 11:16:30.438563 14409 net.cpp:131] Top shape: 50 (50)
I0803 11:16:30.438570 14409 net.cpp:139] Memory required for data: 30917600
I0803 11:16:30.438580 14409 layer_factory.hpp:77] Creating layer label_data_1_split
I0803 11:16:30.438598 14409 net.cpp:86] Creating Layer label_data_1_split
I0803 11:16:30.438606 14409 net.cpp:408] label_data_1_split <- label
I0803 11:16:30.438616 14409 net.cpp:382] label_data_1_split -> label_data_1_split_0
I0803 11:16:30.438630 14409 net.cpp:382] label_data_1_split -> label_data_1_split_1
I0803 11:16:30.438706 14409 net.cpp:124] Setting up label_data_1_split
I0803 11:16:30.438719 14409 net.cpp:131] Top shape: 50 (50)
I0803 11:16:30.438726 14409 net.cpp:131] Top shape: 50 (50)
I0803 11:16:30.438730 14409 net.cpp:139] Memory required for data: 30918000
I0803 11:16:30.438737 14409 layer_factory.hpp:77] Creating layer conv1
I0803 11:16:30.438758 14409 net.cpp:86] Creating Layer conv1
I0803 11:16:30.438765 14409 net.cpp:408] conv1 <- data
I0803 11:16:30.438774 14409 net.cpp:382] conv1 -> conv1
I0803 11:16:30.448282 14409 net.cpp:124] Setting up conv1
I0803 11:16:30.448350 14409 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 11:16:30.448365 14409 net.cpp:139] Memory required for data: 88998000
I0803 11:16:30.448402 14409 layer_factory.hpp:77] Creating layer relu1
I0803 11:16:30.448427 14409 net.cpp:86] Creating Layer relu1
I0803 11:16:30.448443 14409 net.cpp:408] relu1 <- conv1
I0803 11:16:30.448458 14409 net.cpp:369] relu1 -> conv1 (in-place)
I0803 11:16:30.448879 14409 net.cpp:124] Setting up relu1
I0803 11:16:30.448909 14409 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 11:16:30.448918 14409 net.cpp:139] Memory required for data: 147078000
I0803 11:16:30.448928 14409 layer_factory.hpp:77] Creating layer pool1
I0803 11:16:30.448951 14409 net.cpp:86] Creating Layer pool1
I0803 11:16:30.448961 14409 net.cpp:408] pool1 <- conv1
I0803 11:16:30.448976 14409 net.cpp:382] pool1 -> pool1
I0803 11:16:30.449097 14409 net.cpp:124] Setting up pool1
I0803 11:16:30.449121 14409 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 11:16:30.449133 14409 net.cpp:139] Memory required for data: 161074800
I0803 11:16:30.449142 14409 layer_factory.hpp:77] Creating layer norm1
I0803 11:16:30.449162 14409 net.cpp:86] Creating Layer norm1
I0803 11:16:30.449175 14409 net.cpp:408] norm1 <- pool1
I0803 11:16:30.449198 14409 net.cpp:382] norm1 -> norm1
I0803 11:16:30.450764 14409 net.cpp:124] Setting up norm1
I0803 11:16:30.450800 14409 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 11:16:30.450814 14409 net.cpp:139] Memory required for data: 175071600
I0803 11:16:30.450827 14409 layer_factory.hpp:77] Creating layer conv2
I0803 11:16:30.450858 14409 net.cpp:86] Creating Layer conv2
I0803 11:16:30.450872 14409 net.cpp:408] conv2 <- norm1
I0803 11:16:30.450891 14409 net.cpp:382] conv2 -> conv2
I0803 11:16:30.479563 14409 net.cpp:124] Setting up conv2
I0803 11:16:30.479620 14409 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 11:16:30.479629 14409 net.cpp:139] Memory required for data: 212396400
I0803 11:16:30.479661 14409 layer_factory.hpp:77] Creating layer relu2
I0803 11:16:30.479682 14409 net.cpp:86] Creating Layer relu2
I0803 11:16:30.479691 14409 net.cpp:408] relu2 <- conv2
I0803 11:16:30.479710 14409 net.cpp:369] relu2 -> conv2 (in-place)
I0803 11:16:30.480085 14409 net.cpp:124] Setting up relu2
I0803 11:16:30.480109 14409 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 11:16:30.480118 14409 net.cpp:139] Memory required for data: 249721200
I0803 11:16:30.480125 14409 layer_factory.hpp:77] Creating layer pool2
I0803 11:16:30.480149 14409 net.cpp:86] Creating Layer pool2
I0803 11:16:30.480159 14409 net.cpp:408] pool2 <- conv2
I0803 11:16:30.480171 14409 net.cpp:382] pool2 -> pool2
I0803 11:16:30.480276 14409 net.cpp:124] Setting up pool2
I0803 11:16:30.480298 14409 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 11:16:30.480307 14409 net.cpp:139] Memory required for data: 258374000
I0803 11:16:30.480316 14409 layer_factory.hpp:77] Creating layer norm2
I0803 11:16:30.480337 14409 net.cpp:86] Creating Layer norm2
I0803 11:16:30.480350 14409 net.cpp:408] norm2 <- pool2
I0803 11:16:30.480361 14409 net.cpp:382] norm2 -> norm2
I0803 11:16:30.481647 14409 net.cpp:124] Setting up norm2
I0803 11:16:30.481673 14409 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 11:16:30.481681 14409 net.cpp:139] Memory required for data: 267026800
I0803 11:16:30.481690 14409 layer_factory.hpp:77] Creating layer conv3
I0803 11:16:30.481719 14409 net.cpp:86] Creating Layer conv3
I0803 11:16:30.481727 14409 net.cpp:408] conv3 <- norm2
I0803 11:16:30.481750 14409 net.cpp:382] conv3 -> conv3
I0803 11:16:30.533288 14409 net.cpp:124] Setting up conv3
I0803 11:16:30.533342 14409 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 11:16:30.533350 14409 net.cpp:139] Memory required for data: 280006000
I0803 11:16:30.533380 14409 layer_factory.hpp:77] Creating layer relu3
I0803 11:16:30.533399 14409 net.cpp:86] Creating Layer relu3
I0803 11:16:30.533411 14409 net.cpp:408] relu3 <- conv3
I0803 11:16:30.533426 14409 net.cpp:369] relu3 -> conv3 (in-place)
I0803 11:16:30.534402 14409 net.cpp:124] Setting up relu3
I0803 11:16:30.534423 14409 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 11:16:30.534430 14409 net.cpp:139] Memory required for data: 292985200
I0803 11:16:30.534436 14409 layer_factory.hpp:77] Creating layer conv4
I0803 11:16:30.534462 14409 net.cpp:86] Creating Layer conv4
I0803 11:16:30.534471 14409 net.cpp:408] conv4 <- conv3
I0803 11:16:30.534494 14409 net.cpp:382] conv4 -> conv4
I0803 11:16:30.574965 14409 net.cpp:124] Setting up conv4
I0803 11:16:30.575017 14409 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 11:16:30.575024 14409 net.cpp:139] Memory required for data: 305964400
I0803 11:16:30.575042 14409 layer_factory.hpp:77] Creating layer relu4
I0803 11:16:30.575059 14409 net.cpp:86] Creating Layer relu4
I0803 11:16:30.575067 14409 net.cpp:408] relu4 <- conv4
I0803 11:16:30.575080 14409 net.cpp:369] relu4 -> conv4 (in-place)
I0803 11:16:30.575351 14409 net.cpp:124] Setting up relu4
I0803 11:16:30.575366 14409 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 11:16:30.575371 14409 net.cpp:139] Memory required for data: 318943600
I0803 11:16:30.575377 14409 layer_factory.hpp:77] Creating layer conv5
I0803 11:16:30.575397 14409 net.cpp:86] Creating Layer conv5
I0803 11:16:30.575403 14409 net.cpp:408] conv5 <- conv4
I0803 11:16:30.575415 14409 net.cpp:382] conv5 -> conv5
I0803 11:16:30.597615 14409 net.cpp:124] Setting up conv5
I0803 11:16:30.597662 14409 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 11:16:30.597669 14409 net.cpp:139] Memory required for data: 327596400
I0803 11:16:30.597695 14409 layer_factory.hpp:77] Creating layer relu5
I0803 11:16:30.597714 14409 net.cpp:86] Creating Layer relu5
I0803 11:16:30.597723 14409 net.cpp:408] relu5 <- conv5
I0803 11:16:30.597734 14409 net.cpp:369] relu5 -> conv5 (in-place)
I0803 11:16:30.597965 14409 net.cpp:124] Setting up relu5
I0803 11:16:30.597978 14409 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 11:16:30.597985 14409 net.cpp:139] Memory required for data: 336249200
I0803 11:16:30.597990 14409 layer_factory.hpp:77] Creating layer pool5
I0803 11:16:30.598009 14409 net.cpp:86] Creating Layer pool5
I0803 11:16:30.598016 14409 net.cpp:408] pool5 <- conv5
I0803 11:16:30.598024 14409 net.cpp:382] pool5 -> pool5
I0803 11:16:30.598099 14409 net.cpp:124] Setting up pool5
I0803 11:16:30.598112 14409 net.cpp:131] Top shape: 50 256 6 6 (460800)
I0803 11:16:30.598119 14409 net.cpp:139] Memory required for data: 338092400
I0803 11:16:30.598124 14409 layer_factory.hpp:77] Creating layer fc6
I0803 11:16:30.598140 14409 net.cpp:86] Creating Layer fc6
I0803 11:16:30.598147 14409 net.cpp:408] fc6 <- pool5
I0803 11:16:30.598155 14409 net.cpp:382] fc6 -> fc6
I0803 11:16:32.031217 14409 net.cpp:124] Setting up fc6
I0803 11:16:32.031291 14409 net.cpp:131] Top shape: 50 4096 (204800)
I0803 11:16:32.031298 14409 net.cpp:139] Memory required for data: 338911600
I0803 11:16:32.031317 14409 layer_factory.hpp:77] Creating layer relu6
I0803 11:16:32.031348 14409 net.cpp:86] Creating Layer relu6
I0803 11:16:32.031358 14409 net.cpp:408] relu6 <- fc6
I0803 11:16:32.031373 14409 net.cpp:369] relu6 -> fc6 (in-place)
I0803 11:16:32.032410 14409 net.cpp:124] Setting up relu6
I0803 11:16:32.032425 14409 net.cpp:131] Top shape: 50 4096 (204800)
I0803 11:16:32.032430 14409 net.cpp:139] Memory required for data: 339730800
I0803 11:16:32.032449 14409 layer_factory.hpp:77] Creating layer drop6
I0803 11:16:32.032460 14409 net.cpp:86] Creating Layer drop6
I0803 11:16:32.032465 14409 net.cpp:408] drop6 <- fc6
I0803 11:16:32.032475 14409 net.cpp:369] drop6 -> fc6 (in-place)
I0803 11:16:32.032519 14409 net.cpp:124] Setting up drop6
I0803 11:16:32.032531 14409 net.cpp:131] Top shape: 50 4096 (204800)
I0803 11:16:32.032537 14409 net.cpp:139] Memory required for data: 340550000
I0803 11:16:32.032542 14409 layer_factory.hpp:77] Creating layer fc7
I0803 11:16:32.032564 14409 net.cpp:86] Creating Layer fc7
I0803 11:16:32.032570 14409 net.cpp:408] fc7 <- fc6
I0803 11:16:32.032582 14409 net.cpp:382] fc7 -> fc7
I0803 11:16:32.661793 14409 net.cpp:124] Setting up fc7
I0803 11:16:32.661869 14409 net.cpp:131] Top shape: 50 4096 (204800)
I0803 11:16:32.661875 14409 net.cpp:139] Memory required for data: 341369200
I0803 11:16:32.661895 14409 layer_factory.hpp:77] Creating layer relu7
I0803 11:16:32.661914 14409 net.cpp:86] Creating Layer relu7
I0803 11:16:32.661924 14409 net.cpp:408] relu7 <- fc7
I0803 11:16:32.661938 14409 net.cpp:369] relu7 -> fc7 (in-place)
I0803 11:16:32.662313 14409 net.cpp:124] Setting up relu7
I0803 11:16:32.662328 14409 net.cpp:131] Top shape: 50 4096 (204800)
I0803 11:16:32.662333 14409 net.cpp:139] Memory required for data: 342188400
I0803 11:16:32.662338 14409 layer_factory.hpp:77] Creating layer drop7
I0803 11:16:32.662348 14409 net.cpp:86] Creating Layer drop7
I0803 11:16:32.662353 14409 net.cpp:408] drop7 <- fc7
I0803 11:16:32.662372 14409 net.cpp:369] drop7 -> fc7 (in-place)
I0803 11:16:32.662432 14409 net.cpp:124] Setting up drop7
I0803 11:16:32.662446 14409 net.cpp:131] Top shape: 50 4096 (204800)
I0803 11:16:32.662451 14409 net.cpp:139] Memory required for data: 343007600
I0803 11:16:32.662456 14409 layer_factory.hpp:77] Creating layer fc8
I0803 11:16:32.662472 14409 net.cpp:86] Creating Layer fc8
I0803 11:16:32.662484 14409 net.cpp:408] fc8 <- fc7
I0803 11:16:32.662493 14409 net.cpp:382] fc8 -> fc8
I0803 11:16:32.662997 14409 net.cpp:124] Setting up fc8
I0803 11:16:32.663025 14409 net.cpp:131] Top shape: 50 2 (100)
I0803 11:16:32.663031 14409 net.cpp:139] Memory required for data: 343008000
I0803 11:16:32.663040 14409 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0803 11:16:32.663050 14409 net.cpp:86] Creating Layer fc8_fc8_0_split
I0803 11:16:32.663055 14409 net.cpp:408] fc8_fc8_0_split <- fc8
I0803 11:16:32.663081 14409 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0803 11:16:32.663092 14409 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0803 11:16:32.663148 14409 net.cpp:124] Setting up fc8_fc8_0_split
I0803 11:16:32.663161 14409 net.cpp:131] Top shape: 50 2 (100)
I0803 11:16:32.663167 14409 net.cpp:131] Top shape: 50 2 (100)
I0803 11:16:32.663172 14409 net.cpp:139] Memory required for data: 343008800
I0803 11:16:32.663178 14409 layer_factory.hpp:77] Creating layer accuracy
I0803 11:16:32.663205 14409 net.cpp:86] Creating Layer accuracy
I0803 11:16:32.663213 14409 net.cpp:408] accuracy <- fc8_fc8_0_split_0
I0803 11:16:32.663219 14409 net.cpp:408] accuracy <- label_data_1_split_0
I0803 11:16:32.663229 14409 net.cpp:382] accuracy -> accuracy
I0803 11:16:32.663245 14409 net.cpp:124] Setting up accuracy
I0803 11:16:32.663254 14409 net.cpp:131] Top shape: (1)
I0803 11:16:32.663260 14409 net.cpp:139] Memory required for data: 343008804
I0803 11:16:32.663265 14409 layer_factory.hpp:77] Creating layer loss
I0803 11:16:32.663275 14409 net.cpp:86] Creating Layer loss
I0803 11:16:32.663281 14409 net.cpp:408] loss <- fc8_fc8_0_split_1
I0803 11:16:32.663287 14409 net.cpp:408] loss <- label_data_1_split_1
I0803 11:16:32.663295 14409 net.cpp:382] loss -> loss
I0803 11:16:32.663306 14409 layer_factory.hpp:77] Creating layer loss
I0803 11:16:32.664361 14409 net.cpp:124] Setting up loss
I0803 11:16:32.664377 14409 net.cpp:131] Top shape: (1)
I0803 11:16:32.664384 14409 net.cpp:134]     with loss weight 1
I0803 11:16:32.664407 14409 net.cpp:139] Memory required for data: 343008808
I0803 11:16:32.664413 14409 net.cpp:200] loss needs backward computation.
I0803 11:16:32.664420 14409 net.cpp:202] accuracy does not need backward computation.
I0803 11:16:32.664427 14409 net.cpp:200] fc8_fc8_0_split needs backward computation.
I0803 11:16:32.664433 14409 net.cpp:200] fc8 needs backward computation.
I0803 11:16:32.664438 14409 net.cpp:200] drop7 needs backward computation.
I0803 11:16:32.664443 14409 net.cpp:200] relu7 needs backward computation.
I0803 11:16:32.664449 14409 net.cpp:200] fc7 needs backward computation.
I0803 11:16:32.664453 14409 net.cpp:200] drop6 needs backward computation.
I0803 11:16:32.664458 14409 net.cpp:200] relu6 needs backward computation.
I0803 11:16:32.664464 14409 net.cpp:200] fc6 needs backward computation.
I0803 11:16:32.664469 14409 net.cpp:200] pool5 needs backward computation.
I0803 11:16:32.664476 14409 net.cpp:200] relu5 needs backward computation.
I0803 11:16:32.664480 14409 net.cpp:200] conv5 needs backward computation.
I0803 11:16:32.664487 14409 net.cpp:200] relu4 needs backward computation.
I0803 11:16:32.664491 14409 net.cpp:200] conv4 needs backward computation.
I0803 11:16:32.664496 14409 net.cpp:200] relu3 needs backward computation.
I0803 11:16:32.664502 14409 net.cpp:200] conv3 needs backward computation.
I0803 11:16:32.664507 14409 net.cpp:200] norm2 needs backward computation.
I0803 11:16:32.664516 14409 net.cpp:200] pool2 needs backward computation.
I0803 11:16:32.664523 14409 net.cpp:200] relu2 needs backward computation.
I0803 11:16:32.664527 14409 net.cpp:200] conv2 needs backward computation.
I0803 11:16:32.664535 14409 net.cpp:200] norm1 needs backward computation.
I0803 11:16:32.664539 14409 net.cpp:200] pool1 needs backward computation.
I0803 11:16:32.664546 14409 net.cpp:200] relu1 needs backward computation.
I0803 11:16:32.664551 14409 net.cpp:200] conv1 needs backward computation.
I0803 11:16:32.664556 14409 net.cpp:202] label_data_1_split does not need backward computation.
I0803 11:16:32.664563 14409 net.cpp:202] data does not need backward computation.
I0803 11:16:32.664567 14409 net.cpp:244] This network produces output accuracy
I0803 11:16:32.664572 14409 net.cpp:244] This network produces output loss
I0803 11:16:32.664599 14409 net.cpp:257] Network initialization done.
I0803 11:16:32.664744 14409 solver.cpp:56] Solver scaffolding done.
I0803 11:16:32.672063 14409 solver.cpp:331] Iteration 0, Testing net (#0)
I0803 11:16:33.062450 14409 blocking_queue.cpp:49] Waiting for data
I0803 11:16:34.105350 14409 solver.cpp:398]     Test net output #0: accuracy = 0.372
I0803 11:16:34.105422 14409 solver.cpp:398]     Test net output #1: loss = 1.16795 (* 1 = 1.16795 loss)
I0803 11:16:35.009078 14409 solver.cpp:219] Iteration 0 (0 iter/s, 2.34232s/20 iters), loss = 1.31366
I0803 11:16:35.012188 14409 solver.cpp:238]     Train net output #0: loss = 1.31366 (* 1 = 1.31366 loss)
I0803 11:16:35.012243 14409 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0803 11:16:53.332967 14409 solver.cpp:219] Iteration 20 (1.09167 iter/s, 18.3206s/20 iters), loss = 0.957814
I0803 11:16:53.333075 14409 solver.cpp:238]     Train net output #0: loss = 0.957814 (* 1 = 0.957814 loss)
I0803 11:16:53.333092 14409 sgd_solver.cpp:105] Iteration 20, lr = 0.001
I0803 11:17:11.667260 14409 solver.cpp:219] Iteration 40 (1.09088 iter/s, 18.3338s/20 iters), loss = 0.683718
I0803 11:17:11.667407 14409 solver.cpp:238]     Train net output #0: loss = 0.683718 (* 1 = 0.683718 loss)
I0803 11:17:11.667439 14409 sgd_solver.cpp:105] Iteration 40, lr = 0.001
I0803 11:17:29.989966 14409 solver.cpp:219] Iteration 60 (1.09156 iter/s, 18.3223s/20 iters), loss = 0.709684
I0803 11:17:29.990067 14409 solver.cpp:238]     Train net output #0: loss = 0.709684 (* 1 = 0.709684 loss)
I0803 11:17:29.990085 14409 sgd_solver.cpp:105] Iteration 60, lr = 0.001
I0803 11:17:48.315729 14409 solver.cpp:219] Iteration 80 (1.09139 iter/s, 18.3253s/20 iters), loss = 0.650195
I0803 11:17:48.315884 14409 solver.cpp:238]     Train net output #0: loss = 0.650195 (* 1 = 0.650195 loss)
I0803 11:17:48.315898 14409 sgd_solver.cpp:105] Iteration 80, lr = 0.001
I0803 11:18:06.639264 14409 solver.cpp:219] Iteration 100 (1.09152 iter/s, 18.323s/20 iters), loss = 0.671251
I0803 11:18:06.639420 14409 solver.cpp:238]     Train net output #0: loss = 0.671251 (* 1 = 0.671251 loss)
I0803 11:18:06.639438 14409 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0803 11:18:11.561125 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:18:24.952483 14409 solver.cpp:219] Iteration 120 (1.09214 iter/s, 18.3128s/20 iters), loss = 0.666828
I0803 11:18:24.952627 14409 solver.cpp:238]     Train net output #0: loss = 0.666828 (* 1 = 0.666828 loss)
I0803 11:18:24.952653 14409 sgd_solver.cpp:105] Iteration 120, lr = 0.001
I0803 11:18:43.264858 14409 solver.cpp:219] Iteration 140 (1.09219 iter/s, 18.3119s/20 iters), loss = 0.731283
I0803 11:18:43.264961 14409 solver.cpp:238]     Train net output #0: loss = 0.731283 (* 1 = 0.731283 loss)
I0803 11:18:43.264976 14409 sgd_solver.cpp:105] Iteration 140, lr = 0.001
I0803 11:19:01.582046 14409 solver.cpp:219] Iteration 160 (1.0919 iter/s, 18.3167s/20 iters), loss = 0.695905
I0803 11:19:01.582186 14409 solver.cpp:238]     Train net output #0: loss = 0.695905 (* 1 = 0.695905 loss)
I0803 11:19:01.582206 14409 sgd_solver.cpp:105] Iteration 160, lr = 0.001
I0803 11:19:19.903419 14409 solver.cpp:219] Iteration 180 (1.09165 iter/s, 18.321s/20 iters), loss = 0.643285
I0803 11:19:19.903508 14409 solver.cpp:238]     Train net output #0: loss = 0.643285 (* 1 = 0.643285 loss)
I0803 11:19:19.903523 14409 sgd_solver.cpp:105] Iteration 180, lr = 0.001
I0803 11:19:38.226955 14409 solver.cpp:219] Iteration 200 (1.09151 iter/s, 18.3232s/20 iters), loss = 0.613171
I0803 11:19:38.227073 14409 solver.cpp:238]     Train net output #0: loss = 0.613171 (* 1 = 0.613171 loss)
I0803 11:19:38.227105 14409 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0803 11:19:51.613492 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:19:56.555474 14409 solver.cpp:219] Iteration 220 (1.09122 iter/s, 18.3281s/20 iters), loss = 0.598375
I0803 11:19:56.555586 14409 solver.cpp:238]     Train net output #0: loss = 0.598375 (* 1 = 0.598375 loss)
I0803 11:19:56.555601 14409 sgd_solver.cpp:105] Iteration 220, lr = 0.001
I0803 11:20:14.873152 14409 solver.cpp:219] Iteration 240 (1.09187 iter/s, 18.3173s/20 iters), loss = 0.590376
I0803 11:20:14.873273 14409 solver.cpp:238]     Train net output #0: loss = 0.590376 (* 1 = 0.590376 loss)
I0803 11:20:14.873306 14409 sgd_solver.cpp:105] Iteration 240, lr = 0.001
I0803 11:20:22.494874 14409 solver.cpp:331] Iteration 250, Testing net (#0)
I0803 11:20:24.536661 14409 solver.cpp:398]     Test net output #0: accuracy = 0.623
I0803 11:20:24.536737 14409 solver.cpp:398]     Test net output #1: loss = 0.662291 (* 1 = 0.662291 loss)
I0803 11:20:34.607236 14409 solver.cpp:219] Iteration 260 (1.0135 iter/s, 19.7336s/20 iters), loss = 0.600232
I0803 11:20:34.607342 14409 solver.cpp:238]     Train net output #0: loss = 0.600232 (* 1 = 0.600232 loss)
I0803 11:20:34.607355 14409 sgd_solver.cpp:105] Iteration 260, lr = 0.001
I0803 11:20:52.929595 14409 solver.cpp:219] Iteration 280 (1.09159 iter/s, 18.322s/20 iters), loss = 0.611992
I0803 11:20:52.929690 14409 solver.cpp:238]     Train net output #0: loss = 0.611992 (* 1 = 0.611992 loss)
I0803 11:20:52.929703 14409 sgd_solver.cpp:105] Iteration 280, lr = 0.001
I0803 11:21:11.256228 14409 solver.cpp:219] Iteration 300 (1.09133 iter/s, 18.3262s/20 iters), loss = 0.577162
I0803 11:21:11.256350 14409 solver.cpp:238]     Train net output #0: loss = 0.577162 (* 1 = 0.577162 loss)
I0803 11:21:11.256371 14409 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0803 11:21:29.581254 14409 solver.cpp:219] Iteration 320 (1.09143 iter/s, 18.3246s/20 iters), loss = 0.583635
I0803 11:21:29.581360 14409 solver.cpp:238]     Train net output #0: loss = 0.583635 (* 1 = 0.583635 loss)
I0803 11:21:29.581375 14409 sgd_solver.cpp:105] Iteration 320, lr = 0.001
I0803 11:21:32.787282 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:21:47.914069 14409 solver.cpp:219] Iteration 340 (1.09097 iter/s, 18.3323s/20 iters), loss = 0.560271
I0803 11:21:47.914216 14409 solver.cpp:238]     Train net output #0: loss = 0.560271 (* 1 = 0.560271 loss)
I0803 11:21:47.914247 14409 sgd_solver.cpp:105] Iteration 340, lr = 0.001
I0803 11:22:06.233904 14409 solver.cpp:219] Iteration 360 (1.09174 iter/s, 18.3194s/20 iters), loss = 0.704208
I0803 11:22:06.234020 14409 solver.cpp:238]     Train net output #0: loss = 0.704208 (* 1 = 0.704208 loss)
I0803 11:22:06.234052 14409 sgd_solver.cpp:105] Iteration 360, lr = 0.001
I0803 11:22:24.556753 14409 solver.cpp:219] Iteration 380 (1.09156 iter/s, 18.3224s/20 iters), loss = 0.531723
I0803 11:22:24.556864 14409 solver.cpp:238]     Train net output #0: loss = 0.531723 (* 1 = 0.531723 loss)
I0803 11:22:24.556885 14409 sgd_solver.cpp:105] Iteration 380, lr = 0.001
I0803 11:22:42.881306 14409 solver.cpp:219] Iteration 400 (1.09146 iter/s, 18.3241s/20 iters), loss = 0.577642
I0803 11:22:42.881458 14409 solver.cpp:238]     Train net output #0: loss = 0.577642 (* 1 = 0.577642 loss)
I0803 11:22:42.881474 14409 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0803 11:23:01.205924 14409 solver.cpp:219] Iteration 420 (1.09146 iter/s, 18.3241s/20 iters), loss = 0.596067
I0803 11:23:01.206075 14409 solver.cpp:238]     Train net output #0: loss = 0.596067 (* 1 = 0.596067 loss)
I0803 11:23:01.206096 14409 sgd_solver.cpp:105] Iteration 420, lr = 0.001
I0803 11:23:13.438359 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:23:19.524309 14409 solver.cpp:219] Iteration 440 (1.09183 iter/s, 18.3179s/20 iters), loss = 0.568911
I0803 11:23:19.524410 14409 solver.cpp:238]     Train net output #0: loss = 0.568911 (* 1 = 0.568911 loss)
I0803 11:23:19.524428 14409 sgd_solver.cpp:105] Iteration 440, lr = 0.001
I0803 11:23:37.860708 14409 solver.cpp:219] Iteration 460 (1.09075 iter/s, 18.336s/20 iters), loss = 0.586824
I0803 11:23:37.860821 14409 solver.cpp:238]     Train net output #0: loss = 0.586824 (* 1 = 0.586824 loss)
I0803 11:23:37.860839 14409 sgd_solver.cpp:105] Iteration 460, lr = 0.001
I0803 11:23:56.190757 14409 solver.cpp:219] Iteration 480 (1.09113 iter/s, 18.3296s/20 iters), loss = 0.526126
I0803 11:23:56.190855 14409 solver.cpp:238]     Train net output #0: loss = 0.526126 (* 1 = 0.526126 loss)
I0803 11:23:56.190870 14409 sgd_solver.cpp:105] Iteration 480, lr = 0.001
I0803 11:24:12.996423 14409 solver.cpp:331] Iteration 500, Testing net (#0)
I0803 11:24:14.969840 14409 solver.cpp:398]     Test net output #0: accuracy = 0.703
I0803 11:24:14.969923 14409 solver.cpp:398]     Test net output #1: loss = 0.552046 (* 1 = 0.552046 loss)
I0803 11:24:15.867786 14409 solver.cpp:219] Iteration 500 (1.01644 iter/s, 19.6765s/20 iters), loss = 0.543146
I0803 11:24:15.870928 14409 solver.cpp:238]     Train net output #0: loss = 0.543146 (* 1 = 0.543146 loss)
I0803 11:24:15.870962 14409 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0803 11:24:34.205440 14409 solver.cpp:219] Iteration 520 (1.09086 iter/s, 18.3342s/20 iters), loss = 0.55828
I0803 11:24:34.205543 14409 solver.cpp:238]     Train net output #0: loss = 0.55828 (* 1 = 0.55828 loss)
I0803 11:24:34.205564 14409 sgd_solver.cpp:105] Iteration 520, lr = 0.001
I0803 11:24:52.533463 14409 solver.cpp:219] Iteration 540 (1.09125 iter/s, 18.3276s/20 iters), loss = 0.572308
I0803 11:24:52.533555 14409 solver.cpp:238]     Train net output #0: loss = 0.572308 (* 1 = 0.572308 loss)
I0803 11:24:52.533568 14409 sgd_solver.cpp:105] Iteration 540, lr = 0.001
I0803 11:24:54.741467 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:25:10.857656 14409 solver.cpp:219] Iteration 560 (1.09148 iter/s, 18.3238s/20 iters), loss = 0.568029
I0803 11:25:10.857774 14409 solver.cpp:238]     Train net output #0: loss = 0.568029 (* 1 = 0.568029 loss)
I0803 11:25:10.857805 14409 sgd_solver.cpp:105] Iteration 560, lr = 0.001
I0803 11:25:29.191444 14409 solver.cpp:219] Iteration 580 (1.09091 iter/s, 18.3334s/20 iters), loss = 0.64868
I0803 11:25:29.191555 14409 solver.cpp:238]     Train net output #0: loss = 0.64868 (* 1 = 0.64868 loss)
I0803 11:25:29.191571 14409 sgd_solver.cpp:105] Iteration 580, lr = 0.001
I0803 11:25:47.508137 14409 solver.cpp:219] Iteration 600 (1.09192 iter/s, 18.3163s/20 iters), loss = 0.56573
I0803 11:25:47.508239 14409 solver.cpp:238]     Train net output #0: loss = 0.56573 (* 1 = 0.56573 loss)
I0803 11:25:47.508257 14409 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0803 11:26:05.836899 14409 solver.cpp:219] Iteration 620 (1.09121 iter/s, 18.3283s/20 iters), loss = 0.54459
I0803 11:26:05.837033 14409 solver.cpp:238]     Train net output #0: loss = 0.54459 (* 1 = 0.54459 loss)
I0803 11:26:05.837062 14409 sgd_solver.cpp:105] Iteration 620, lr = 0.001
I0803 11:26:24.166805 14409 solver.cpp:219] Iteration 640 (1.09114 iter/s, 18.3295s/20 iters), loss = 0.574163
I0803 11:26:24.166913 14409 solver.cpp:238]     Train net output #0: loss = 0.574163 (* 1 = 0.574163 loss)
I0803 11:26:24.166929 14409 sgd_solver.cpp:105] Iteration 640, lr = 0.001
I0803 11:26:34.731017 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:26:42.491562 14409 solver.cpp:219] Iteration 660 (1.09145 iter/s, 18.3243s/20 iters), loss = 0.609718
I0803 11:26:42.491736 14409 solver.cpp:238]     Train net output #0: loss = 0.609718 (* 1 = 0.609718 loss)
I0803 11:26:42.491755 14409 sgd_solver.cpp:105] Iteration 660, lr = 0.001
I0803 11:27:00.826979 14409 solver.cpp:219] Iteration 680 (1.09082 iter/s, 18.3349s/20 iters), loss = 0.568739
I0803 11:27:00.827131 14409 solver.cpp:238]     Train net output #0: loss = 0.568739 (* 1 = 0.568739 loss)
I0803 11:27:00.827169 14409 sgd_solver.cpp:105] Iteration 680, lr = 0.001
I0803 11:27:19.158962 14409 solver.cpp:219] Iteration 700 (1.09102 iter/s, 18.3314s/20 iters), loss = 0.549582
I0803 11:27:19.159128 14409 solver.cpp:238]     Train net output #0: loss = 0.549582 (* 1 = 0.549582 loss)
I0803 11:27:19.159162 14409 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0803 11:27:37.489334 14409 solver.cpp:219] Iteration 720 (1.09111 iter/s, 18.3299s/20 iters), loss = 0.496511
I0803 11:27:37.489434 14409 solver.cpp:238]     Train net output #0: loss = 0.496511 (* 1 = 0.496511 loss)
I0803 11:27:37.489452 14409 sgd_solver.cpp:105] Iteration 720, lr = 0.001
I0803 11:27:55.817261 14409 solver.cpp:219] Iteration 740 (1.09126 iter/s, 18.3275s/20 iters), loss = 0.531771
I0803 11:27:55.817400 14409 solver.cpp:238]     Train net output #0: loss = 0.531771 (* 1 = 0.531771 loss)
I0803 11:27:55.817435 14409 sgd_solver.cpp:105] Iteration 740, lr = 0.001
I0803 11:28:03.532279 14409 solver.cpp:331] Iteration 750, Testing net (#0)
I0803 11:28:05.393656 14409 solver.cpp:398]     Test net output #0: accuracy = 0.727
I0803 11:28:05.393784 14409 solver.cpp:398]     Test net output #1: loss = 0.493396 (* 1 = 0.493396 loss)
I0803 11:28:15.463485 14409 solver.cpp:219] Iteration 760 (1.01803 iter/s, 19.6457s/20 iters), loss = 0.556137
I0803 11:28:15.463572 14409 solver.cpp:238]     Train net output #0: loss = 0.556137 (* 1 = 0.556137 loss)
I0803 11:28:15.463598 14409 sgd_solver.cpp:105] Iteration 760, lr = 0.001
I0803 11:28:15.871953 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:28:33.788686 14409 solver.cpp:219] Iteration 780 (1.09143 iter/s, 18.3247s/20 iters), loss = 0.571412
I0803 11:28:33.788913 14409 solver.cpp:238]     Train net output #0: loss = 0.571412 (* 1 = 0.571412 loss)
I0803 11:28:33.788946 14409 sgd_solver.cpp:105] Iteration 780, lr = 0.001
I0803 11:28:52.113845 14409 solver.cpp:219] Iteration 800 (1.09143 iter/s, 18.3246s/20 iters), loss = 0.533376
I0803 11:28:52.113997 14409 solver.cpp:238]     Train net output #0: loss = 0.533376 (* 1 = 0.533376 loss)
I0803 11:28:52.114023 14409 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0803 11:29:10.447190 14409 solver.cpp:219] Iteration 820 (1.09094 iter/s, 18.3328s/20 iters), loss = 0.512477
I0803 11:29:10.447341 14409 solver.cpp:238]     Train net output #0: loss = 0.512477 (* 1 = 0.512477 loss)
I0803 11:29:10.447368 14409 sgd_solver.cpp:105] Iteration 820, lr = 0.001
I0803 11:29:28.768230 14409 solver.cpp:219] Iteration 840 (1.09167 iter/s, 18.3205s/20 iters), loss = 0.54565
I0803 11:29:28.768401 14409 solver.cpp:238]     Train net output #0: loss = 0.54565 (* 1 = 0.54565 loss)
I0803 11:29:28.768427 14409 sgd_solver.cpp:105] Iteration 840, lr = 0.001
I0803 11:29:47.081542 14409 solver.cpp:219] Iteration 860 (1.09213 iter/s, 18.3129s/20 iters), loss = 0.540839
I0803 11:29:47.081640 14409 solver.cpp:238]     Train net output #0: loss = 0.540839 (* 1 = 0.540839 loss)
I0803 11:29:47.081656 14409 sgd_solver.cpp:105] Iteration 860, lr = 0.001
I0803 11:29:56.672940 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:30:05.407451 14409 solver.cpp:219] Iteration 880 (1.09137 iter/s, 18.3255s/20 iters), loss = 0.486213
I0803 11:30:05.407548 14409 solver.cpp:238]     Train net output #0: loss = 0.486213 (* 1 = 0.486213 loss)
I0803 11:30:05.407563 14409 sgd_solver.cpp:105] Iteration 880, lr = 0.001
I0803 11:30:23.745105 14409 solver.cpp:219] Iteration 900 (1.09068 iter/s, 18.3371s/20 iters), loss = 0.507901
I0803 11:30:23.745259 14409 solver.cpp:238]     Train net output #0: loss = 0.507901 (* 1 = 0.507901 loss)
I0803 11:30:23.745290 14409 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0803 11:30:42.069674 14409 solver.cpp:219] Iteration 920 (1.09146 iter/s, 18.3241s/20 iters), loss = 0.554304
I0803 11:30:42.069773 14409 solver.cpp:238]     Train net output #0: loss = 0.554304 (* 1 = 0.554304 loss)
I0803 11:30:42.069790 14409 sgd_solver.cpp:105] Iteration 920, lr = 0.001
I0803 11:31:00.396720 14409 solver.cpp:219] Iteration 940 (1.09131 iter/s, 18.3266s/20 iters), loss = 0.584955
I0803 11:31:00.396817 14409 solver.cpp:238]     Train net output #0: loss = 0.584955 (* 1 = 0.584955 loss)
I0803 11:31:00.396834 14409 sgd_solver.cpp:105] Iteration 940, lr = 0.001
I0803 11:31:18.711679 14409 solver.cpp:219] Iteration 960 (1.09203 iter/s, 18.3145s/20 iters), loss = 0.472341
I0803 11:31:18.711835 14409 solver.cpp:238]     Train net output #0: loss = 0.472341 (* 1 = 0.472341 loss)
I0803 11:31:18.711866 14409 sgd_solver.cpp:105] Iteration 960, lr = 0.001
I0803 11:31:36.526294 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:31:37.033020 14409 solver.cpp:219] Iteration 980 (1.09165 iter/s, 18.3208s/20 iters), loss = 0.493916
I0803 11:31:37.033154 14409 solver.cpp:238]     Train net output #0: loss = 0.493916 (* 1 = 0.493916 loss)
I0803 11:31:37.033190 14409 sgd_solver.cpp:105] Iteration 980, lr = 0.001
I0803 11:31:53.889622 14409 solver.cpp:331] Iteration 1000, Testing net (#0)
I0803 11:31:55.714519 14409 solver.cpp:398]     Test net output #0: accuracy = 0.735
I0803 11:31:55.714628 14409 solver.cpp:398]     Test net output #1: loss = 0.500671 (* 1 = 0.500671 loss)
I0803 11:31:56.612524 14409 solver.cpp:219] Iteration 1000 (1.0215 iter/s, 19.5791s/20 iters), loss = 0.484293
I0803 11:31:56.615707 14409 solver.cpp:238]     Train net output #0: loss = 0.484293 (* 1 = 0.484293 loss)
I0803 11:31:56.615759 14409 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0803 11:32:14.941588 14409 solver.cpp:219] Iteration 1020 (1.09137 iter/s, 18.3256s/20 iters), loss = 0.496543
I0803 11:32:14.941714 14409 solver.cpp:238]     Train net output #0: loss = 0.496543 (* 1 = 0.496543 loss)
I0803 11:32:14.941742 14409 sgd_solver.cpp:105] Iteration 1020, lr = 0.001
I0803 11:32:33.270851 14409 solver.cpp:219] Iteration 1040 (1.09118 iter/s, 18.3289s/20 iters), loss = 0.534153
I0803 11:32:33.270946 14409 solver.cpp:238]     Train net output #0: loss = 0.534153 (* 1 = 0.534153 loss)
I0803 11:32:33.270962 14409 sgd_solver.cpp:105] Iteration 1040, lr = 0.001
I0803 11:32:51.592360 14409 solver.cpp:219] Iteration 1060 (1.09164 iter/s, 18.321s/20 iters), loss = 0.499281
I0803 11:32:51.592557 14409 solver.cpp:238]     Train net output #0: loss = 0.499281 (* 1 = 0.499281 loss)
I0803 11:32:51.592571 14409 sgd_solver.cpp:105] Iteration 1060, lr = 0.001
I0803 11:33:09.930374 14409 solver.cpp:219] Iteration 1080 (1.09066 iter/s, 18.3375s/20 iters), loss = 0.433476
I0803 11:33:09.930564 14409 solver.cpp:238]     Train net output #0: loss = 0.433476 (* 1 = 0.433476 loss)
I0803 11:33:09.930594 14409 sgd_solver.cpp:105] Iteration 1080, lr = 0.001
I0803 11:33:17.718231 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:33:28.251775 14409 solver.cpp:219] Iteration 1100 (1.09165 iter/s, 18.3209s/20 iters), loss = 0.419986
I0803 11:33:28.251910 14409 solver.cpp:238]     Train net output #0: loss = 0.419986 (* 1 = 0.419986 loss)
I0803 11:33:28.251924 14409 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0803 11:33:46.587694 14409 solver.cpp:219] Iteration 1120 (1.09079 iter/s, 18.3354s/20 iters), loss = 0.533897
I0803 11:33:46.587795 14409 solver.cpp:238]     Train net output #0: loss = 0.533897 (* 1 = 0.533897 loss)
I0803 11:33:46.587811 14409 sgd_solver.cpp:105] Iteration 1120, lr = 0.001
I0803 11:34:04.904454 14409 solver.cpp:219] Iteration 1140 (1.09192 iter/s, 18.3164s/20 iters), loss = 0.448745
I0803 11:34:04.904572 14409 solver.cpp:238]     Train net output #0: loss = 0.448745 (* 1 = 0.448745 loss)
I0803 11:34:04.904603 14409 sgd_solver.cpp:105] Iteration 1140, lr = 0.001
I0803 11:34:23.230674 14409 solver.cpp:219] Iteration 1160 (1.09136 iter/s, 18.3257s/20 iters), loss = 0.453148
I0803 11:34:23.230844 14409 solver.cpp:238]     Train net output #0: loss = 0.453148 (* 1 = 0.453148 loss)
I0803 11:34:23.230876 14409 sgd_solver.cpp:105] Iteration 1160, lr = 0.001
I0803 11:34:41.547053 14409 solver.cpp:219] Iteration 1180 (1.09195 iter/s, 18.3158s/20 iters), loss = 0.441254
I0803 11:34:41.547195 14409 solver.cpp:238]     Train net output #0: loss = 0.441254 (* 1 = 0.441254 loss)
I0803 11:34:41.547230 14409 sgd_solver.cpp:105] Iteration 1180, lr = 0.001
I0803 11:34:58.370625 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:34:59.875368 14409 solver.cpp:219] Iteration 1200 (1.09123 iter/s, 18.3279s/20 iters), loss = 0.475043
I0803 11:34:59.875485 14409 solver.cpp:238]     Train net output #0: loss = 0.475043 (* 1 = 0.475043 loss)
I0803 11:34:59.875504 14409 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0803 11:35:18.195924 14409 solver.cpp:219] Iteration 1220 (1.09169 iter/s, 18.3201s/20 iters), loss = 0.461841
I0803 11:35:18.196038 14409 solver.cpp:238]     Train net output #0: loss = 0.461841 (* 1 = 0.461841 loss)
I0803 11:35:18.196064 14409 sgd_solver.cpp:105] Iteration 1220, lr = 0.001
I0803 11:35:36.517767 14409 solver.cpp:219] Iteration 1240 (1.09162 iter/s, 18.3214s/20 iters), loss = 0.361088
I0803 11:35:36.517864 14409 solver.cpp:238]     Train net output #0: loss = 0.361088 (* 1 = 0.361088 loss)
I0803 11:35:36.517879 14409 sgd_solver.cpp:105] Iteration 1240, lr = 0.001
I0803 11:35:44.226721 14409 solver.cpp:331] Iteration 1250, Testing net (#0)
I0803 11:35:45.490592 14480 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:35:46.200582 14409 solver.cpp:398]     Test net output #0: accuracy = 0.78
I0803 11:35:46.200664 14409 solver.cpp:398]     Test net output #1: loss = 0.434525 (* 1 = 0.434525 loss)
I0803 11:35:56.270828 14409 solver.cpp:219] Iteration 1260 (1.01253 iter/s, 19.7525s/20 iters), loss = 0.437588
I0803 11:35:56.271029 14409 solver.cpp:238]     Train net output #0: loss = 0.437588 (* 1 = 0.437588 loss)
I0803 11:35:56.271057 14409 sgd_solver.cpp:105] Iteration 1260, lr = 0.001
I0803 11:36:14.594252 14409 solver.cpp:219] Iteration 1280 (1.09153 iter/s, 18.323s/20 iters), loss = 0.422673
I0803 11:36:14.594352 14409 solver.cpp:238]     Train net output #0: loss = 0.422673 (* 1 = 0.422673 loss)
I0803 11:36:14.594369 14409 sgd_solver.cpp:105] Iteration 1280, lr = 0.001
I0803 11:36:32.871323 14409 solver.cpp:219] Iteration 1300 (1.09429 iter/s, 18.2767s/20 iters), loss = 0.431225
I0803 11:36:32.887079 14409 solver.cpp:238]     Train net output #0: loss = 0.431225 (* 1 = 0.431225 loss)
I0803 11:36:32.887114 14409 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0803 11:36:40.567347 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:36:51.183293 14409 solver.cpp:219] Iteration 1320 (1.09314 iter/s, 18.2959s/20 iters), loss = 0.466466
I0803 11:36:51.198747 14409 solver.cpp:238]     Train net output #0: loss = 0.466466 (* 1 = 0.466466 loss)
I0803 11:36:51.198787 14409 sgd_solver.cpp:105] Iteration 1320, lr = 0.001
I0803 11:37:09.510004 14409 solver.cpp:219] Iteration 1340 (1.09224 iter/s, 18.3109s/20 iters), loss = 0.462499
I0803 11:37:09.525621 14409 solver.cpp:238]     Train net output #0: loss = 0.462499 (* 1 = 0.462499 loss)
I0803 11:37:09.525640 14409 sgd_solver.cpp:105] Iteration 1340, lr = 0.001
I0803 11:37:27.845872 14409 solver.cpp:219] Iteration 1360 (1.09171 iter/s, 18.3199s/20 iters), loss = 0.391677
I0803 11:37:27.861608 14409 solver.cpp:238]     Train net output #0: loss = 0.391677 (* 1 = 0.391677 loss)
I0803 11:37:27.861635 14409 sgd_solver.cpp:105] Iteration 1360, lr = 0.001
I0803 11:37:46.174623 14409 solver.cpp:219] Iteration 1380 (1.09214 iter/s, 18.3127s/20 iters), loss = 0.475346
I0803 11:37:46.190331 14409 solver.cpp:238]     Train net output #0: loss = 0.475346 (* 1 = 0.475346 loss)
I0803 11:37:46.190358 14409 sgd_solver.cpp:105] Iteration 1380, lr = 0.001
I0803 11:38:04.536870 14409 solver.cpp:219] Iteration 1400 (1.09014 iter/s, 18.3463s/20 iters), loss = 0.435672
I0803 11:38:04.536970 14409 solver.cpp:238]     Train net output #0: loss = 0.435672 (* 1 = 0.435672 loss)
I0803 11:38:04.536988 14409 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0803 11:38:19.669817 14478 data_layer.cpp:73] Restarting data prefetching from start.
I0803 11:38:22.862882 14409 solver.cpp:219] Iteration 1420 (1.09137 iter/s, 18.3256s/20 iters), loss = 0.374153
I0803 11:38:22.862998 14409 solver.cpp:238]     Train net output #0: loss = 0.374153 (* 1 = 0.374153 loss)
I0803 11:38:22.863013 14409 sgd_solver.cpp:105] Iteration 1420, lr = 0.001
I0803 11:38:41.198002 14409 solver.cpp:219] Iteration 1440 (1.09083 iter/s, 18.3347s/20 iters), loss = 0.354799
I0803 11:38:41.198115 14409 solver.cpp:238]     Train net output #0: loss = 0.354799 (* 1 = 0.354799 loss)
I0803 11:38:41.198132 14409 sgd_solver.cpp:105] Iteration 1440, lr = 0.001
I0803 11:38:59.535965 14409 solver.cpp:219] Iteration 1460 (1.09066 iter/s, 18.3375s/20 iters), loss = 0.503571
I0803 11:38:59.536059 14409 solver.cpp:238]     Train net output #0: loss = 0.503571 (* 1 = 0.503571 loss)
I0803 11:38:59.536075 14409 sgd_solver.cpp:105] Iteration 1460, lr = 0.001
I0803 11:39:17.862648 14409 solver.cpp:219] Iteration 1480 (1.09134 iter/s, 18.3261s/20 iters), loss = 0.412729
I0803 11:39:17.862859 14409 solver.cpp:238]     Train net output #0: loss = 0.412729 (* 1 = 0.412729 loss)
I0803 11:39:17.862890 14409 sgd_solver.cpp:105] Iteration 1480, lr = 0.001
I0803 11:39:34.741499 14409 solver.cpp:331] Iteration 1500, Testing net (#0)
I0803 11:39:36.686722 14409 solver.cpp:398]     Test net output #0: accuracy = 0.854
I0803 11:39:36.686839 14409 solver.cpp:398]     Test net output #1: loss = 0.34441 (* 1 = 0.34441 loss)
I0803 11:39:37.584373 14409 solver.cpp:219] Iteration 1500 (1.01413 iter/s, 19.7213s/20 iters), loss = 0.400865
I0803 11:39:37.587558 14409 solver.cpp:238]     Train net output #0: loss = 0.400865 (* 1 = 0.400865 loss)
I0803 11:39:37.587600 14409 sgd_solver.cpp:105] Iteration 1500, lr = 0.0001
