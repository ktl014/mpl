WARNING: Logging before InitGoogleLogging() is written to STDERR
I0803 16:20:31.293488 17673 solver.cpp:44] Initializing solver from parameters: 
test_iter: 20
test_interval: 250
base_lr: 0.001
display: 20
max_iter: 5000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1500
snapshot: 5000
snapshot_prefix: "caffenet_train"
solver_mode: GPU
net: "caffenet/train_val_exp6.prototxt"
I0803 16:20:31.294419 17673 solver.cpp:87] Creating training net from net file: caffenet/train_val_exp6.prototxt
I0803 16:20:31.295730 17673 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0803 16:20:31.295760 17673 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0803 16:20:31.295977 17673 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "train.LMDB"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 16:20:31.296078 17673 layer_factory.hpp:77] Creating layer data
I0803 16:20:31.297273 17673 db_lmdb.cpp:35] Opened lmdb train.LMDB
I0803 16:20:31.297513 17673 net.cpp:86] Creating Layer data
I0803 16:20:31.297531 17673 net.cpp:382] data -> data
I0803 16:20:31.297550 17673 net.cpp:382] data -> label
I0803 16:20:31.299019 17673 data_layer.cpp:45] output data size: 256,3,227,227
I0803 16:20:31.709961 17673 net.cpp:124] Setting up data
I0803 16:20:31.710016 17673 net.cpp:131] Top shape: 256 3 227 227 (39574272)
I0803 16:20:31.710024 17673 net.cpp:131] Top shape: 256 (256)
I0803 16:20:31.710029 17673 net.cpp:139] Memory required for data: 158298112
I0803 16:20:31.710038 17673 layer_factory.hpp:77] Creating layer conv1
I0803 16:20:31.710077 17673 net.cpp:86] Creating Layer conv1
I0803 16:20:31.710083 17673 net.cpp:408] conv1 <- data
I0803 16:20:31.710096 17673 net.cpp:382] conv1 -> conv1
I0803 16:20:32.038357 17673 net.cpp:124] Setting up conv1
I0803 16:20:32.038398 17673 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 16:20:32.038404 17673 net.cpp:139] Memory required for data: 455667712
I0803 16:20:32.038429 17673 layer_factory.hpp:77] Creating layer relu1
I0803 16:20:32.038461 17673 net.cpp:86] Creating Layer relu1
I0803 16:20:32.038473 17673 net.cpp:408] relu1 <- conv1
I0803 16:20:32.038491 17673 net.cpp:369] relu1 -> conv1 (in-place)
I0803 16:20:32.039387 17673 net.cpp:124] Setting up relu1
I0803 16:20:32.039405 17673 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 16:20:32.039424 17673 net.cpp:139] Memory required for data: 753037312
I0803 16:20:32.039429 17673 layer_factory.hpp:77] Creating layer pool1
I0803 16:20:32.039438 17673 net.cpp:86] Creating Layer pool1
I0803 16:20:32.039443 17673 net.cpp:408] pool1 <- conv1
I0803 16:20:32.039453 17673 net.cpp:382] pool1 -> pool1
I0803 16:20:32.039526 17673 net.cpp:124] Setting up pool1
I0803 16:20:32.039538 17673 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 16:20:32.039544 17673 net.cpp:139] Memory required for data: 824700928
I0803 16:20:32.039549 17673 layer_factory.hpp:77] Creating layer norm1
I0803 16:20:32.039563 17673 net.cpp:86] Creating Layer norm1
I0803 16:20:32.039571 17673 net.cpp:408] norm1 <- pool1
I0803 16:20:32.039577 17673 net.cpp:382] norm1 -> norm1
I0803 16:20:32.039834 17673 net.cpp:124] Setting up norm1
I0803 16:20:32.039849 17673 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 16:20:32.039855 17673 net.cpp:139] Memory required for data: 896364544
I0803 16:20:32.039860 17673 layer_factory.hpp:77] Creating layer conv2
I0803 16:20:32.039878 17673 net.cpp:86] Creating Layer conv2
I0803 16:20:32.039885 17673 net.cpp:408] conv2 <- norm1
I0803 16:20:32.039907 17673 net.cpp:382] conv2 -> conv2
I0803 16:20:32.055994 17673 net.cpp:124] Setting up conv2
I0803 16:20:32.056025 17673 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 16:20:32.056031 17673 net.cpp:139] Memory required for data: 1087467520
I0803 16:20:32.056043 17673 layer_factory.hpp:77] Creating layer relu2
I0803 16:20:32.056056 17673 net.cpp:86] Creating Layer relu2
I0803 16:20:32.056064 17673 net.cpp:408] relu2 <- conv2
I0803 16:20:32.056073 17673 net.cpp:369] relu2 -> conv2 (in-place)
I0803 16:20:32.056298 17673 net.cpp:124] Setting up relu2
I0803 16:20:32.056313 17673 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 16:20:32.056319 17673 net.cpp:139] Memory required for data: 1278570496
I0803 16:20:32.056324 17673 layer_factory.hpp:77] Creating layer pool2
I0803 16:20:32.056336 17673 net.cpp:86] Creating Layer pool2
I0803 16:20:32.056344 17673 net.cpp:408] pool2 <- conv2
I0803 16:20:32.056352 17673 net.cpp:382] pool2 -> pool2
I0803 16:20:32.056404 17673 net.cpp:124] Setting up pool2
I0803 16:20:32.056416 17673 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 16:20:32.056421 17673 net.cpp:139] Memory required for data: 1322872832
I0803 16:20:32.056427 17673 layer_factory.hpp:77] Creating layer norm2
I0803 16:20:32.056438 17673 net.cpp:86] Creating Layer norm2
I0803 16:20:32.056444 17673 net.cpp:408] norm2 <- pool2
I0803 16:20:32.056452 17673 net.cpp:382] norm2 -> norm2
I0803 16:20:32.057265 17673 net.cpp:124] Setting up norm2
I0803 16:20:32.057281 17673 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 16:20:32.057288 17673 net.cpp:139] Memory required for data: 1367175168
I0803 16:20:32.057293 17673 layer_factory.hpp:77] Creating layer conv3
I0803 16:20:32.057312 17673 net.cpp:86] Creating Layer conv3
I0803 16:20:32.057320 17673 net.cpp:408] conv3 <- norm2
I0803 16:20:32.057329 17673 net.cpp:382] conv3 -> conv3
I0803 16:20:32.093739 17673 net.cpp:124] Setting up conv3
I0803 16:20:32.093760 17673 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 16:20:32.093767 17673 net.cpp:139] Memory required for data: 1433628672
I0803 16:20:32.093780 17673 layer_factory.hpp:77] Creating layer relu3
I0803 16:20:32.093792 17673 net.cpp:86] Creating Layer relu3
I0803 16:20:32.093799 17673 net.cpp:408] relu3 <- conv3
I0803 16:20:32.093807 17673 net.cpp:369] relu3 -> conv3 (in-place)
I0803 16:20:32.094025 17673 net.cpp:124] Setting up relu3
I0803 16:20:32.094039 17673 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 16:20:32.094046 17673 net.cpp:139] Memory required for data: 1500082176
I0803 16:20:32.094051 17673 layer_factory.hpp:77] Creating layer conv4
I0803 16:20:32.094069 17673 net.cpp:86] Creating Layer conv4
I0803 16:20:32.094075 17673 net.cpp:408] conv4 <- conv3
I0803 16:20:32.094084 17673 net.cpp:382] conv4 -> conv4
I0803 16:20:32.123713 17673 net.cpp:124] Setting up conv4
I0803 16:20:32.123733 17673 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 16:20:32.123739 17673 net.cpp:139] Memory required for data: 1566535680
I0803 16:20:32.123749 17673 layer_factory.hpp:77] Creating layer relu4
I0803 16:20:32.123759 17673 net.cpp:86] Creating Layer relu4
I0803 16:20:32.123766 17673 net.cpp:408] relu4 <- conv4
I0803 16:20:32.123776 17673 net.cpp:369] relu4 -> conv4 (in-place)
I0803 16:20:32.123991 17673 net.cpp:124] Setting up relu4
I0803 16:20:32.124006 17673 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 16:20:32.124011 17673 net.cpp:139] Memory required for data: 1632989184
I0803 16:20:32.124017 17673 layer_factory.hpp:77] Creating layer conv5
I0803 16:20:32.124032 17673 net.cpp:86] Creating Layer conv5
I0803 16:20:32.124039 17673 net.cpp:408] conv5 <- conv4
I0803 16:20:32.124050 17673 net.cpp:382] conv5 -> conv5
I0803 16:20:32.145704 17673 net.cpp:124] Setting up conv5
I0803 16:20:32.145722 17673 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 16:20:32.145730 17673 net.cpp:139] Memory required for data: 1677291520
I0803 16:20:32.145742 17673 layer_factory.hpp:77] Creating layer relu5
I0803 16:20:32.145756 17673 net.cpp:86] Creating Layer relu5
I0803 16:20:32.145762 17673 net.cpp:408] relu5 <- conv5
I0803 16:20:32.145769 17673 net.cpp:369] relu5 -> conv5 (in-place)
I0803 16:20:32.145992 17673 net.cpp:124] Setting up relu5
I0803 16:20:32.146006 17673 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 16:20:32.146013 17673 net.cpp:139] Memory required for data: 1721593856
I0803 16:20:32.146018 17673 layer_factory.hpp:77] Creating layer pool5
I0803 16:20:32.146031 17673 net.cpp:86] Creating Layer pool5
I0803 16:20:32.146039 17673 net.cpp:408] pool5 <- conv5
I0803 16:20:32.146045 17673 net.cpp:382] pool5 -> pool5
I0803 16:20:32.146103 17673 net.cpp:124] Setting up pool5
I0803 16:20:32.146114 17673 net.cpp:131] Top shape: 256 256 6 6 (2359296)
I0803 16:20:32.146121 17673 net.cpp:139] Memory required for data: 1731031040
I0803 16:20:32.146126 17673 layer_factory.hpp:77] Creating layer fc6
I0803 16:20:32.146139 17673 net.cpp:86] Creating Layer fc6
I0803 16:20:32.146147 17673 net.cpp:408] fc6 <- pool5
I0803 16:20:32.146153 17673 net.cpp:382] fc6 -> fc6
I0803 16:20:33.523910 17673 net.cpp:124] Setting up fc6
I0803 16:20:33.523973 17673 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 16:20:33.523979 17673 net.cpp:139] Memory required for data: 1735225344
I0803 16:20:33.523998 17673 layer_factory.hpp:77] Creating layer relu6
I0803 16:20:33.524019 17673 net.cpp:86] Creating Layer relu6
I0803 16:20:33.524039 17673 net.cpp:408] relu6 <- fc6
I0803 16:20:33.524054 17673 net.cpp:369] relu6 -> fc6 (in-place)
I0803 16:20:33.525251 17673 net.cpp:124] Setting up relu6
I0803 16:20:33.525267 17673 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 16:20:33.525272 17673 net.cpp:139] Memory required for data: 1739419648
I0803 16:20:33.525290 17673 layer_factory.hpp:77] Creating layer drop6
I0803 16:20:33.525307 17673 net.cpp:86] Creating Layer drop6
I0803 16:20:33.525312 17673 net.cpp:408] drop6 <- fc6
I0803 16:20:33.525321 17673 net.cpp:369] drop6 -> fc6 (in-place)
I0803 16:20:33.525372 17673 net.cpp:124] Setting up drop6
I0803 16:20:33.525382 17673 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 16:20:33.525389 17673 net.cpp:139] Memory required for data: 1743613952
I0803 16:20:33.525394 17673 layer_factory.hpp:77] Creating layer fc7
I0803 16:20:33.525413 17673 net.cpp:86] Creating Layer fc7
I0803 16:20:33.525419 17673 net.cpp:408] fc7 <- fc6
I0803 16:20:33.525426 17673 net.cpp:382] fc7 -> fc7
I0803 16:20:34.161360 17673 net.cpp:124] Setting up fc7
I0803 16:20:34.161419 17673 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 16:20:34.161425 17673 net.cpp:139] Memory required for data: 1747808256
I0803 16:20:34.161439 17673 layer_factory.hpp:77] Creating layer relu7
I0803 16:20:34.161458 17673 net.cpp:86] Creating Layer relu7
I0803 16:20:34.161464 17673 net.cpp:408] relu7 <- fc7
I0803 16:20:34.161474 17673 net.cpp:369] relu7 -> fc7 (in-place)
I0803 16:20:34.162469 17673 net.cpp:124] Setting up relu7
I0803 16:20:34.162493 17673 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 16:20:34.162498 17673 net.cpp:139] Memory required for data: 1752002560
I0803 16:20:34.162504 17673 layer_factory.hpp:77] Creating layer drop7
I0803 16:20:34.162524 17673 net.cpp:86] Creating Layer drop7
I0803 16:20:34.162530 17673 net.cpp:408] drop7 <- fc7
I0803 16:20:34.162536 17673 net.cpp:369] drop7 -> fc7 (in-place)
I0803 16:20:34.162569 17673 net.cpp:124] Setting up drop7
I0803 16:20:34.162582 17673 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 16:20:34.162588 17673 net.cpp:139] Memory required for data: 1756196864
I0803 16:20:34.162593 17673 layer_factory.hpp:77] Creating layer fc8
I0803 16:20:34.162603 17673 net.cpp:86] Creating Layer fc8
I0803 16:20:34.162609 17673 net.cpp:408] fc8 <- fc7
I0803 16:20:34.162619 17673 net.cpp:382] fc8 -> fc8
I0803 16:20:34.163743 17673 net.cpp:124] Setting up fc8
I0803 16:20:34.163759 17673 net.cpp:131] Top shape: 256 2 (512)
I0803 16:20:34.163764 17673 net.cpp:139] Memory required for data: 1756198912
I0803 16:20:34.163774 17673 layer_factory.hpp:77] Creating layer loss
I0803 16:20:34.163790 17673 net.cpp:86] Creating Layer loss
I0803 16:20:34.163795 17673 net.cpp:408] loss <- fc8
I0803 16:20:34.163801 17673 net.cpp:408] loss <- label
I0803 16:20:34.163808 17673 net.cpp:382] loss -> loss
I0803 16:20:34.163823 17673 layer_factory.hpp:77] Creating layer loss
I0803 16:20:34.164150 17673 net.cpp:124] Setting up loss
I0803 16:20:34.164165 17673 net.cpp:131] Top shape: (1)
I0803 16:20:34.164170 17673 net.cpp:134]     with loss weight 1
I0803 16:20:34.164191 17673 net.cpp:139] Memory required for data: 1756198916
I0803 16:20:34.164197 17673 net.cpp:200] loss needs backward computation.
I0803 16:20:34.164202 17673 net.cpp:200] fc8 needs backward computation.
I0803 16:20:34.164206 17673 net.cpp:200] drop7 needs backward computation.
I0803 16:20:34.164214 17673 net.cpp:200] relu7 needs backward computation.
I0803 16:20:34.164218 17673 net.cpp:200] fc7 needs backward computation.
I0803 16:20:34.164222 17673 net.cpp:200] drop6 needs backward computation.
I0803 16:20:34.164227 17673 net.cpp:200] relu6 needs backward computation.
I0803 16:20:34.164230 17673 net.cpp:200] fc6 needs backward computation.
I0803 16:20:34.164235 17673 net.cpp:200] pool5 needs backward computation.
I0803 16:20:34.164242 17673 net.cpp:200] relu5 needs backward computation.
I0803 16:20:34.164247 17673 net.cpp:200] conv5 needs backward computation.
I0803 16:20:34.164254 17673 net.cpp:200] relu4 needs backward computation.
I0803 16:20:34.164259 17673 net.cpp:200] conv4 needs backward computation.
I0803 16:20:34.164265 17673 net.cpp:200] relu3 needs backward computation.
I0803 16:20:34.164270 17673 net.cpp:200] conv3 needs backward computation.
I0803 16:20:34.164278 17673 net.cpp:200] norm2 needs backward computation.
I0803 16:20:34.164288 17673 net.cpp:200] pool2 needs backward computation.
I0803 16:20:34.164294 17673 net.cpp:200] relu2 needs backward computation.
I0803 16:20:34.164299 17673 net.cpp:200] conv2 needs backward computation.
I0803 16:20:34.164305 17673 net.cpp:200] norm1 needs backward computation.
I0803 16:20:34.164311 17673 net.cpp:200] pool1 needs backward computation.
I0803 16:20:34.164316 17673 net.cpp:200] relu1 needs backward computation.
I0803 16:20:34.164321 17673 net.cpp:200] conv1 needs backward computation.
I0803 16:20:34.164325 17673 net.cpp:202] data does not need backward computation.
I0803 16:20:34.164331 17673 net.cpp:244] This network produces output loss
I0803 16:20:34.164350 17673 net.cpp:257] Network initialization done.
I0803 16:20:34.165709 17673 solver.cpp:173] Creating test net (#0) specified by net file: caffenet/train_val_exp6.prototxt
I0803 16:20:34.165767 17673 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0803 16:20:34.166012 17673 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "val.LMDB"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 16:20:34.166144 17673 layer_factory.hpp:77] Creating layer data
I0803 16:20:34.167348 17673 db_lmdb.cpp:35] Opened lmdb val.LMDB
I0803 16:20:34.167564 17673 net.cpp:86] Creating Layer data
I0803 16:20:34.167587 17673 net.cpp:382] data -> data
I0803 16:20:34.167603 17673 net.cpp:382] data -> label
I0803 16:20:34.168087 17673 data_layer.cpp:45] output data size: 50,3,227,227
I0803 16:20:34.294047 17673 net.cpp:124] Setting up data
I0803 16:20:34.294097 17673 net.cpp:131] Top shape: 50 3 227 227 (7729350)
I0803 16:20:34.294107 17673 net.cpp:131] Top shape: 50 (50)
I0803 16:20:34.294113 17673 net.cpp:139] Memory required for data: 30917600
I0803 16:20:34.294124 17673 layer_factory.hpp:77] Creating layer label_data_1_split
I0803 16:20:34.294143 17673 net.cpp:86] Creating Layer label_data_1_split
I0803 16:20:34.294152 17673 net.cpp:408] label_data_1_split <- label
I0803 16:20:34.294163 17673 net.cpp:382] label_data_1_split -> label_data_1_split_0
I0803 16:20:34.294181 17673 net.cpp:382] label_data_1_split -> label_data_1_split_1
I0803 16:20:34.294277 17673 net.cpp:124] Setting up label_data_1_split
I0803 16:20:34.294291 17673 net.cpp:131] Top shape: 50 (50)
I0803 16:20:34.294298 17673 net.cpp:131] Top shape: 50 (50)
I0803 16:20:34.294303 17673 net.cpp:139] Memory required for data: 30918000
I0803 16:20:34.294311 17673 layer_factory.hpp:77] Creating layer conv1
I0803 16:20:34.294330 17673 net.cpp:86] Creating Layer conv1
I0803 16:20:34.294337 17673 net.cpp:408] conv1 <- data
I0803 16:20:34.294347 17673 net.cpp:382] conv1 -> conv1
I0803 16:20:34.304268 17673 net.cpp:124] Setting up conv1
I0803 16:20:34.304339 17673 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 16:20:34.304352 17673 net.cpp:139] Memory required for data: 88998000
I0803 16:20:34.304390 17673 layer_factory.hpp:77] Creating layer relu1
I0803 16:20:34.304415 17673 net.cpp:86] Creating Layer relu1
I0803 16:20:34.304431 17673 net.cpp:408] relu1 <- conv1
I0803 16:20:34.304448 17673 net.cpp:369] relu1 -> conv1 (in-place)
I0803 16:20:34.304864 17673 net.cpp:124] Setting up relu1
I0803 16:20:34.304893 17673 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 16:20:34.304904 17673 net.cpp:139] Memory required for data: 147078000
I0803 16:20:34.304915 17673 layer_factory.hpp:77] Creating layer pool1
I0803 16:20:34.304939 17673 net.cpp:86] Creating Layer pool1
I0803 16:20:34.304949 17673 net.cpp:408] pool1 <- conv1
I0803 16:20:34.304965 17673 net.cpp:382] pool1 -> pool1
I0803 16:20:34.305095 17673 net.cpp:124] Setting up pool1
I0803 16:20:34.305119 17673 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 16:20:34.305130 17673 net.cpp:139] Memory required for data: 161074800
I0803 16:20:34.305140 17673 layer_factory.hpp:77] Creating layer norm1
I0803 16:20:34.305160 17673 net.cpp:86] Creating Layer norm1
I0803 16:20:34.305172 17673 net.cpp:408] norm1 <- pool1
I0803 16:20:34.305187 17673 net.cpp:382] norm1 -> norm1
I0803 16:20:34.306751 17673 net.cpp:124] Setting up norm1
I0803 16:20:34.306783 17673 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 16:20:34.306795 17673 net.cpp:139] Memory required for data: 175071600
I0803 16:20:34.306813 17673 layer_factory.hpp:77] Creating layer conv2
I0803 16:20:34.306843 17673 net.cpp:86] Creating Layer conv2
I0803 16:20:34.306856 17673 net.cpp:408] conv2 <- norm1
I0803 16:20:34.306874 17673 net.cpp:382] conv2 -> conv2
I0803 16:20:34.335857 17673 net.cpp:124] Setting up conv2
I0803 16:20:34.335917 17673 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 16:20:34.335927 17673 net.cpp:139] Memory required for data: 212396400
I0803 16:20:34.335959 17673 layer_factory.hpp:77] Creating layer relu2
I0803 16:20:34.335985 17673 net.cpp:86] Creating Layer relu2
I0803 16:20:34.335999 17673 net.cpp:408] relu2 <- conv2
I0803 16:20:34.336015 17673 net.cpp:369] relu2 -> conv2 (in-place)
I0803 16:20:34.336396 17673 net.cpp:124] Setting up relu2
I0803 16:20:34.336418 17673 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 16:20:34.336429 17673 net.cpp:139] Memory required for data: 249721200
I0803 16:20:34.336441 17673 layer_factory.hpp:77] Creating layer pool2
I0803 16:20:34.336467 17673 net.cpp:86] Creating Layer pool2
I0803 16:20:34.336477 17673 net.cpp:408] pool2 <- conv2
I0803 16:20:34.336494 17673 net.cpp:382] pool2 -> pool2
I0803 16:20:34.336592 17673 net.cpp:124] Setting up pool2
I0803 16:20:34.336616 17673 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 16:20:34.336625 17673 net.cpp:139] Memory required for data: 258374000
I0803 16:20:34.336633 17673 layer_factory.hpp:77] Creating layer norm2
I0803 16:20:34.336652 17673 net.cpp:86] Creating Layer norm2
I0803 16:20:34.336663 17673 net.cpp:408] norm2 <- pool2
I0803 16:20:34.336674 17673 net.cpp:382] norm2 -> norm2
I0803 16:20:34.337975 17673 net.cpp:124] Setting up norm2
I0803 16:20:34.338003 17673 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 16:20:34.338014 17673 net.cpp:139] Memory required for data: 267026800
I0803 16:20:34.338022 17673 layer_factory.hpp:77] Creating layer conv3
I0803 16:20:34.338057 17673 net.cpp:86] Creating Layer conv3
I0803 16:20:34.338068 17673 net.cpp:408] conv3 <- norm2
I0803 16:20:34.338083 17673 net.cpp:382] conv3 -> conv3
I0803 16:20:34.391119 17673 net.cpp:124] Setting up conv3
I0803 16:20:34.391176 17673 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 16:20:34.391183 17673 net.cpp:139] Memory required for data: 280006000
I0803 16:20:34.391211 17673 layer_factory.hpp:77] Creating layer relu3
I0803 16:20:34.391230 17673 net.cpp:86] Creating Layer relu3
I0803 16:20:34.391242 17673 net.cpp:408] relu3 <- conv3
I0803 16:20:34.391259 17673 net.cpp:369] relu3 -> conv3 (in-place)
I0803 16:20:34.392213 17673 net.cpp:124] Setting up relu3
I0803 16:20:34.392233 17673 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 16:20:34.392241 17673 net.cpp:139] Memory required for data: 292985200
I0803 16:20:34.392248 17673 layer_factory.hpp:77] Creating layer conv4
I0803 16:20:34.392273 17673 net.cpp:86] Creating Layer conv4
I0803 16:20:34.392282 17673 net.cpp:408] conv4 <- conv3
I0803 16:20:34.392292 17673 net.cpp:382] conv4 -> conv4
I0803 16:20:34.440440 17673 net.cpp:124] Setting up conv4
I0803 16:20:34.440475 17673 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 16:20:34.440485 17673 net.cpp:139] Memory required for data: 305964400
I0803 16:20:34.440502 17673 layer_factory.hpp:77] Creating layer relu4
I0803 16:20:34.440520 17673 net.cpp:86] Creating Layer relu4
I0803 16:20:34.440531 17673 net.cpp:408] relu4 <- conv4
I0803 16:20:34.440551 17673 net.cpp:369] relu4 -> conv4 (in-place)
I0803 16:20:34.440912 17673 net.cpp:124] Setting up relu4
I0803 16:20:34.440934 17673 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 16:20:34.440944 17673 net.cpp:139] Memory required for data: 318943600
I0803 16:20:34.440953 17673 layer_factory.hpp:77] Creating layer conv5
I0803 16:20:34.440979 17673 net.cpp:86] Creating Layer conv5
I0803 16:20:34.440990 17673 net.cpp:408] conv5 <- conv4
I0803 16:20:34.441005 17673 net.cpp:382] conv5 -> conv5
I0803 16:20:34.475548 17673 net.cpp:124] Setting up conv5
I0803 16:20:34.475581 17673 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 16:20:34.475589 17673 net.cpp:139] Memory required for data: 327596400
I0803 16:20:34.475608 17673 layer_factory.hpp:77] Creating layer relu5
I0803 16:20:34.475620 17673 net.cpp:86] Creating Layer relu5
I0803 16:20:34.475626 17673 net.cpp:408] relu5 <- conv5
I0803 16:20:34.475641 17673 net.cpp:369] relu5 -> conv5 (in-place)
I0803 16:20:34.475924 17673 net.cpp:124] Setting up relu5
I0803 16:20:34.475942 17673 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 16:20:34.475950 17673 net.cpp:139] Memory required for data: 336249200
I0803 16:20:34.475955 17673 layer_factory.hpp:77] Creating layer pool5
I0803 16:20:34.475970 17673 net.cpp:86] Creating Layer pool5
I0803 16:20:34.475976 17673 net.cpp:408] pool5 <- conv5
I0803 16:20:34.475989 17673 net.cpp:382] pool5 -> pool5
I0803 16:20:34.476070 17673 net.cpp:124] Setting up pool5
I0803 16:20:34.476085 17673 net.cpp:131] Top shape: 50 256 6 6 (460800)
I0803 16:20:34.476092 17673 net.cpp:139] Memory required for data: 338092400
I0803 16:20:34.476099 17673 layer_factory.hpp:77] Creating layer fc6
I0803 16:20:34.476115 17673 net.cpp:86] Creating Layer fc6
I0803 16:20:34.476125 17673 net.cpp:408] fc6 <- pool5
I0803 16:20:34.476136 17673 net.cpp:382] fc6 -> fc6
I0803 16:20:35.849942 17673 net.cpp:124] Setting up fc6
I0803 16:20:35.849999 17673 net.cpp:131] Top shape: 50 4096 (204800)
I0803 16:20:35.850005 17673 net.cpp:139] Memory required for data: 338911600
I0803 16:20:35.850020 17673 layer_factory.hpp:77] Creating layer relu6
I0803 16:20:35.850039 17673 net.cpp:86] Creating Layer relu6
I0803 16:20:35.850045 17673 net.cpp:408] relu6 <- fc6
I0803 16:20:35.850069 17673 net.cpp:369] relu6 -> fc6 (in-place)
I0803 16:20:35.851125 17673 net.cpp:124] Setting up relu6
I0803 16:20:35.851141 17673 net.cpp:131] Top shape: 50 4096 (204800)
I0803 16:20:35.851146 17673 net.cpp:139] Memory required for data: 339730800
I0803 16:20:35.851164 17673 layer_factory.hpp:77] Creating layer drop6
I0803 16:20:35.851176 17673 net.cpp:86] Creating Layer drop6
I0803 16:20:35.851182 17673 net.cpp:408] drop6 <- fc6
I0803 16:20:35.851188 17673 net.cpp:369] drop6 -> fc6 (in-place)
I0803 16:20:35.851246 17673 net.cpp:124] Setting up drop6
I0803 16:20:35.851258 17673 net.cpp:131] Top shape: 50 4096 (204800)
I0803 16:20:35.851264 17673 net.cpp:139] Memory required for data: 340550000
I0803 16:20:35.851269 17673 layer_factory.hpp:77] Creating layer fc7
I0803 16:20:35.851281 17673 net.cpp:86] Creating Layer fc7
I0803 16:20:35.851287 17673 net.cpp:408] fc7 <- fc6
I0803 16:20:35.851300 17673 net.cpp:382] fc7 -> fc7
I0803 16:20:36.478324 17673 net.cpp:124] Setting up fc7
I0803 16:20:36.478377 17673 net.cpp:131] Top shape: 50 4096 (204800)
I0803 16:20:36.478384 17673 net.cpp:139] Memory required for data: 341369200
I0803 16:20:36.478399 17673 layer_factory.hpp:77] Creating layer relu7
I0803 16:20:36.478412 17673 net.cpp:86] Creating Layer relu7
I0803 16:20:36.478420 17673 net.cpp:408] relu7 <- fc7
I0803 16:20:36.478446 17673 net.cpp:369] relu7 -> fc7 (in-place)
I0803 16:20:36.478763 17673 net.cpp:124] Setting up relu7
I0803 16:20:36.478778 17673 net.cpp:131] Top shape: 50 4096 (204800)
I0803 16:20:36.478783 17673 net.cpp:139] Memory required for data: 342188400
I0803 16:20:36.478788 17673 layer_factory.hpp:77] Creating layer drop7
I0803 16:20:36.478798 17673 net.cpp:86] Creating Layer drop7
I0803 16:20:36.478802 17673 net.cpp:408] drop7 <- fc7
I0803 16:20:36.478811 17673 net.cpp:369] drop7 -> fc7 (in-place)
I0803 16:20:36.478855 17673 net.cpp:124] Setting up drop7
I0803 16:20:36.478868 17673 net.cpp:131] Top shape: 50 4096 (204800)
I0803 16:20:36.478873 17673 net.cpp:139] Memory required for data: 343007600
I0803 16:20:36.478878 17673 layer_factory.hpp:77] Creating layer fc8
I0803 16:20:36.478890 17673 net.cpp:86] Creating Layer fc8
I0803 16:20:36.478896 17673 net.cpp:408] fc8 <- fc7
I0803 16:20:36.478906 17673 net.cpp:382] fc8 -> fc8
I0803 16:20:36.479379 17673 net.cpp:124] Setting up fc8
I0803 16:20:36.479393 17673 net.cpp:131] Top shape: 50 2 (100)
I0803 16:20:36.479398 17673 net.cpp:139] Memory required for data: 343008000
I0803 16:20:36.479418 17673 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0803 16:20:36.479444 17673 net.cpp:86] Creating Layer fc8_fc8_0_split
I0803 16:20:36.479449 17673 net.cpp:408] fc8_fc8_0_split <- fc8
I0803 16:20:36.479456 17673 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0803 16:20:36.479470 17673 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0803 16:20:36.479526 17673 net.cpp:124] Setting up fc8_fc8_0_split
I0803 16:20:36.479537 17673 net.cpp:131] Top shape: 50 2 (100)
I0803 16:20:36.479543 17673 net.cpp:131] Top shape: 50 2 (100)
I0803 16:20:36.479550 17673 net.cpp:139] Memory required for data: 343008800
I0803 16:20:36.479555 17673 layer_factory.hpp:77] Creating layer accuracy
I0803 16:20:36.479575 17673 net.cpp:86] Creating Layer accuracy
I0803 16:20:36.479581 17673 net.cpp:408] accuracy <- fc8_fc8_0_split_0
I0803 16:20:36.479588 17673 net.cpp:408] accuracy <- label_data_1_split_0
I0803 16:20:36.479596 17673 net.cpp:382] accuracy -> accuracy
I0803 16:20:36.479610 17673 net.cpp:124] Setting up accuracy
I0803 16:20:36.479619 17673 net.cpp:131] Top shape: (1)
I0803 16:20:36.479624 17673 net.cpp:139] Memory required for data: 343008804
I0803 16:20:36.479629 17673 layer_factory.hpp:77] Creating layer loss
I0803 16:20:36.479637 17673 net.cpp:86] Creating Layer loss
I0803 16:20:36.479641 17673 net.cpp:408] loss <- fc8_fc8_0_split_1
I0803 16:20:36.479647 17673 net.cpp:408] loss <- label_data_1_split_1
I0803 16:20:36.479658 17673 net.cpp:382] loss -> loss
I0803 16:20:36.479671 17673 layer_factory.hpp:77] Creating layer loss
I0803 16:20:36.480692 17673 net.cpp:124] Setting up loss
I0803 16:20:36.480710 17673 net.cpp:131] Top shape: (1)
I0803 16:20:36.480715 17673 net.cpp:134]     with loss weight 1
I0803 16:20:36.480736 17673 net.cpp:139] Memory required for data: 343008808
I0803 16:20:36.480741 17673 net.cpp:200] loss needs backward computation.
I0803 16:20:36.480747 17673 net.cpp:202] accuracy does not need backward computation.
I0803 16:20:36.480754 17673 net.cpp:200] fc8_fc8_0_split needs backward computation.
I0803 16:20:36.480759 17673 net.cpp:200] fc8 needs backward computation.
I0803 16:20:36.480765 17673 net.cpp:200] drop7 needs backward computation.
I0803 16:20:36.480769 17673 net.cpp:200] relu7 needs backward computation.
I0803 16:20:36.480773 17673 net.cpp:200] fc7 needs backward computation.
I0803 16:20:36.480778 17673 net.cpp:200] drop6 needs backward computation.
I0803 16:20:36.480783 17673 net.cpp:200] relu6 needs backward computation.
I0803 16:20:36.480788 17673 net.cpp:200] fc6 needs backward computation.
I0803 16:20:36.480792 17673 net.cpp:200] pool5 needs backward computation.
I0803 16:20:36.480798 17673 net.cpp:200] relu5 needs backward computation.
I0803 16:20:36.480803 17673 net.cpp:200] conv5 needs backward computation.
I0803 16:20:36.480808 17673 net.cpp:200] relu4 needs backward computation.
I0803 16:20:36.480813 17673 net.cpp:200] conv4 needs backward computation.
I0803 16:20:36.480818 17673 net.cpp:200] relu3 needs backward computation.
I0803 16:20:36.480824 17673 net.cpp:200] conv3 needs backward computation.
I0803 16:20:36.480829 17673 net.cpp:200] norm2 needs backward computation.
I0803 16:20:36.480835 17673 net.cpp:200] pool2 needs backward computation.
I0803 16:20:36.480839 17673 net.cpp:200] relu2 needs backward computation.
I0803 16:20:36.480845 17673 net.cpp:200] conv2 needs backward computation.
I0803 16:20:36.480854 17673 net.cpp:200] norm1 needs backward computation.
I0803 16:20:36.480857 17673 net.cpp:200] pool1 needs backward computation.
I0803 16:20:36.480864 17673 net.cpp:200] relu1 needs backward computation.
I0803 16:20:36.480868 17673 net.cpp:200] conv1 needs backward computation.
I0803 16:20:36.480875 17673 net.cpp:202] label_data_1_split does not need backward computation.
I0803 16:20:36.480882 17673 net.cpp:202] data does not need backward computation.
I0803 16:20:36.480888 17673 net.cpp:244] This network produces output accuracy
I0803 16:20:36.480893 17673 net.cpp:244] This network produces output loss
I0803 16:20:36.480921 17673 net.cpp:257] Network initialization done.
I0803 16:20:36.481035 17673 solver.cpp:56] Solver scaffolding done.
I0803 16:20:37.307374 17673 solver.cpp:331] Iteration 0, Testing net (#0)
I0803 16:20:37.700042 17673 blocking_queue.cpp:49] Waiting for data
I0803 16:20:38.696259 17673 solver.cpp:398]     Test net output #0: accuracy = 0.628
I0803 16:20:38.696336 17673 solver.cpp:398]     Test net output #1: loss = 0.83211 (* 1 = 0.83211 loss)
I0803 16:20:39.690454 17673 solver.cpp:219] Iteration 0 (0 iter/s, 2.3879s/20 iters), loss = 0.903415
I0803 16:20:39.690575 17673 solver.cpp:238]     Train net output #0: loss = 0.903415 (* 1 = 0.903415 loss)
I0803 16:20:39.690606 17673 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0803 16:20:58.004333 17673 solver.cpp:219] Iteration 20 (1.0921 iter/s, 18.3134s/20 iters), loss = 0.685172
I0803 16:20:58.004482 17673 solver.cpp:238]     Train net output #0: loss = 0.685172 (* 1 = 0.685172 loss)
I0803 16:20:58.004508 17673 sgd_solver.cpp:105] Iteration 20, lr = 0.001
I0803 16:21:16.302768 17673 solver.cpp:219] Iteration 40 (1.09302 iter/s, 18.298s/20 iters), loss = 0.729579
I0803 16:21:16.302886 17673 solver.cpp:238]     Train net output #0: loss = 0.729579 (* 1 = 0.729579 loss)
I0803 16:21:16.302903 17673 sgd_solver.cpp:105] Iteration 40, lr = 0.001
I0803 16:21:34.599020 17673 solver.cpp:219] Iteration 60 (1.09315 iter/s, 18.2958s/20 iters), loss = 0.813263
I0803 16:21:34.599167 17673 solver.cpp:238]     Train net output #0: loss = 0.813263 (* 1 = 0.813263 loss)
I0803 16:21:34.599194 17673 sgd_solver.cpp:105] Iteration 60, lr = 0.001
I0803 16:21:52.898347 17673 solver.cpp:219] Iteration 80 (1.09296 iter/s, 18.2989s/20 iters), loss = 0.700109
I0803 16:21:52.898448 17673 solver.cpp:238]     Train net output #0: loss = 0.700109 (* 1 = 0.700109 loss)
I0803 16:21:52.898468 17673 sgd_solver.cpp:105] Iteration 80, lr = 0.001
I0803 16:22:11.191620 17673 solver.cpp:219] Iteration 100 (1.09333 iter/s, 18.2928s/20 iters), loss = 0.721909
I0803 16:22:11.191745 17673 solver.cpp:238]     Train net output #0: loss = 0.721909 (* 1 = 0.721909 loss)
I0803 16:22:11.191766 17673 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0803 16:22:16.260576 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:22:29.498885 17673 solver.cpp:219] Iteration 120 (1.09249 iter/s, 18.3068s/20 iters), loss = 0.728084
I0803 16:22:29.498978 17673 solver.cpp:238]     Train net output #0: loss = 0.728084 (* 1 = 0.728084 loss)
I0803 16:22:29.498996 17673 sgd_solver.cpp:105] Iteration 120, lr = 0.001
I0803 16:22:47.763257 17673 solver.cpp:219] Iteration 140 (1.09505 iter/s, 18.264s/20 iters), loss = 0.66575
I0803 16:22:47.778569 17673 solver.cpp:238]     Train net output #0: loss = 0.66575 (* 1 = 0.66575 loss)
I0803 16:22:47.778594 17673 sgd_solver.cpp:105] Iteration 140, lr = 0.001
I0803 16:23:06.057107 17673 solver.cpp:219] Iteration 160 (1.0942 iter/s, 18.2782s/20 iters), loss = 0.78352
I0803 16:23:06.072412 17673 solver.cpp:238]     Train net output #0: loss = 0.78352 (* 1 = 0.78352 loss)
I0803 16:23:06.072439 17673 sgd_solver.cpp:105] Iteration 160, lr = 0.001
I0803 16:23:24.386806 17673 solver.cpp:219] Iteration 180 (1.09206 iter/s, 18.314s/20 iters), loss = 0.777815
I0803 16:23:24.386955 17673 solver.cpp:238]     Train net output #0: loss = 0.777815 (* 1 = 0.777815 loss)
I0803 16:23:24.386979 17673 sgd_solver.cpp:105] Iteration 180, lr = 0.001
I0803 16:23:42.688489 17673 solver.cpp:219] Iteration 200 (1.09283 iter/s, 18.3012s/20 iters), loss = 0.668678
I0803 16:23:42.688673 17673 solver.cpp:238]     Train net output #0: loss = 0.668678 (* 1 = 0.668678 loss)
I0803 16:23:42.688721 17673 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0803 16:23:55.962704 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:24:00.993381 17673 solver.cpp:219] Iteration 220 (1.09264 iter/s, 18.3043s/20 iters), loss = 1.06757
I0803 16:24:00.993507 17673 solver.cpp:238]     Train net output #0: loss = 1.06757 (* 1 = 1.06757 loss)
I0803 16:24:00.993538 17673 sgd_solver.cpp:105] Iteration 220, lr = 0.001
I0803 16:24:19.295797 17673 solver.cpp:219] Iteration 240 (1.09278 iter/s, 18.302s/20 iters), loss = 0.782906
I0803 16:24:19.295917 17673 solver.cpp:238]     Train net output #0: loss = 0.782906 (* 1 = 0.782906 loss)
I0803 16:24:19.295946 17673 sgd_solver.cpp:105] Iteration 240, lr = 0.001
I0803 16:24:26.989847 17673 solver.cpp:331] Iteration 250, Testing net (#0)
I0803 16:24:29.000126 17673 solver.cpp:398]     Test net output #0: accuracy = 0.623
I0803 16:24:29.000253 17673 solver.cpp:398]     Test net output #1: loss = 0.839715 (* 1 = 0.839715 loss)
I0803 16:24:38.966861 17673 solver.cpp:219] Iteration 260 (1.01674 iter/s, 19.6706s/20 iters), loss = 0.689351
I0803 16:24:38.981997 17673 solver.cpp:238]     Train net output #0: loss = 0.689351 (* 1 = 0.689351 loss)
I0803 16:24:38.982038 17673 sgd_solver.cpp:105] Iteration 260, lr = 0.001
I0803 16:24:57.297052 17673 solver.cpp:219] Iteration 280 (1.09202 iter/s, 18.3147s/20 iters), loss = 0.668059
I0803 16:24:57.312137 17673 solver.cpp:238]     Train net output #0: loss = 0.668059 (* 1 = 0.668059 loss)
I0803 16:24:57.312189 17673 sgd_solver.cpp:105] Iteration 280, lr = 0.001
I0803 16:25:15.631255 17673 solver.cpp:219] Iteration 300 (1.09177 iter/s, 18.3188s/20 iters), loss = 0.731304
I0803 16:25:15.631413 17673 solver.cpp:238]     Train net output #0: loss = 0.731304 (* 1 = 0.731304 loss)
I0803 16:25:15.631444 17673 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0803 16:25:33.940304 17673 solver.cpp:219] Iteration 320 (1.09238 iter/s, 18.3086s/20 iters), loss = 0.675329
I0803 16:25:33.940412 17673 solver.cpp:238]     Train net output #0: loss = 0.675329 (* 1 = 0.675329 loss)
I0803 16:25:33.940428 17673 sgd_solver.cpp:105] Iteration 320, lr = 0.001
I0803 16:25:37.212750 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:25:52.232647 17673 solver.cpp:219] Iteration 340 (1.09338 iter/s, 18.2919s/20 iters), loss = 0.678114
I0803 16:25:52.232751 17673 solver.cpp:238]     Train net output #0: loss = 0.678114 (* 1 = 0.678114 loss)
I0803 16:25:52.232780 17673 sgd_solver.cpp:105] Iteration 340, lr = 0.001
I0803 16:26:10.544276 17673 solver.cpp:219] Iteration 360 (1.09223 iter/s, 18.3112s/20 iters), loss = 0.69399
I0803 16:26:10.544363 17673 solver.cpp:238]     Train net output #0: loss = 0.69399 (* 1 = 0.69399 loss)
I0803 16:26:10.544374 17673 sgd_solver.cpp:105] Iteration 360, lr = 0.001
I0803 16:26:28.852082 17673 solver.cpp:219] Iteration 380 (1.09245 iter/s, 18.3074s/20 iters), loss = 0.736946
I0803 16:26:28.852218 17673 solver.cpp:238]     Train net output #0: loss = 0.736946 (* 1 = 0.736946 loss)
I0803 16:26:28.852242 17673 sgd_solver.cpp:105] Iteration 380, lr = 0.001
I0803 16:26:47.141074 17673 solver.cpp:219] Iteration 400 (1.09359 iter/s, 18.2884s/20 iters), loss = 0.702128
I0803 16:26:47.141257 17673 solver.cpp:238]     Train net output #0: loss = 0.702128 (* 1 = 0.702128 loss)
I0803 16:26:47.141283 17673 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0803 16:27:05.433329 17673 solver.cpp:219] Iteration 420 (1.09339 iter/s, 18.2918s/20 iters), loss = 0.747357
I0803 16:27:05.433430 17673 solver.cpp:238]     Train net output #0: loss = 0.747357 (* 1 = 0.747357 loss)
I0803 16:27:05.433445 17673 sgd_solver.cpp:105] Iteration 420, lr = 0.001
I0803 16:27:17.698323 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:27:23.746392 17673 solver.cpp:219] Iteration 440 (1.09214 iter/s, 18.3126s/20 iters), loss = 0.689108
I0803 16:27:23.746496 17673 solver.cpp:238]     Train net output #0: loss = 0.689108 (* 1 = 0.689108 loss)
I0803 16:27:23.746515 17673 sgd_solver.cpp:105] Iteration 440, lr = 0.001
I0803 16:27:42.012656 17673 solver.cpp:219] Iteration 460 (1.09494 iter/s, 18.2658s/20 iters), loss = 0.733273
I0803 16:27:42.027926 17673 solver.cpp:238]     Train net output #0: loss = 0.733273 (* 1 = 0.733273 loss)
I0803 16:27:42.027950 17673 sgd_solver.cpp:105] Iteration 460, lr = 0.001
I0803 16:28:00.341929 17673 solver.cpp:219] Iteration 480 (1.09208 iter/s, 18.3136s/20 iters), loss = 0.75437
I0803 16:28:00.342047 17673 solver.cpp:238]     Train net output #0: loss = 0.75437 (* 1 = 0.75437 loss)
I0803 16:28:00.342066 17673 sgd_solver.cpp:105] Iteration 480, lr = 0.001
I0803 16:28:17.158973 17673 solver.cpp:331] Iteration 500, Testing net (#0)
I0803 16:28:19.143064 17673 solver.cpp:398]     Test net output #0: accuracy = 0.621
I0803 16:28:19.143165 17673 solver.cpp:398]     Test net output #1: loss = 0.679064 (* 1 = 0.679064 loss)
I0803 16:28:20.039860 17673 solver.cpp:219] Iteration 500 (1.01536 iter/s, 19.6975s/20 iters), loss = 0.716839
I0803 16:28:20.042966 17673 solver.cpp:238]     Train net output #0: loss = 0.716839 (* 1 = 0.716839 loss)
I0803 16:28:20.043009 17673 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0803 16:28:38.345357 17673 solver.cpp:219] Iteration 520 (1.09277 iter/s, 18.3021s/20 iters), loss = 0.777335
I0803 16:28:38.345464 17673 solver.cpp:238]     Train net output #0: loss = 0.777335 (* 1 = 0.777335 loss)
I0803 16:28:38.345481 17673 sgd_solver.cpp:105] Iteration 520, lr = 0.001
I0803 16:28:56.660888 17673 solver.cpp:219] Iteration 540 (1.09199 iter/s, 18.3151s/20 iters), loss = 0.692902
I0803 16:28:56.661000 17673 solver.cpp:238]     Train net output #0: loss = 0.692902 (* 1 = 0.692902 loss)
I0803 16:28:56.661017 17673 sgd_solver.cpp:105] Iteration 540, lr = 0.001
I0803 16:28:58.922060 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:29:14.973846 17673 solver.cpp:219] Iteration 560 (1.09215 iter/s, 18.3124s/20 iters), loss = 0.707592
I0803 16:29:14.973984 17673 solver.cpp:238]     Train net output #0: loss = 0.707592 (* 1 = 0.707592 loss)
I0803 16:29:14.974019 17673 sgd_solver.cpp:105] Iteration 560, lr = 0.001
I0803 16:29:33.269938 17673 solver.cpp:219] Iteration 580 (1.09316 iter/s, 18.2957s/20 iters), loss = 0.766474
I0803 16:29:33.270056 17673 solver.cpp:238]     Train net output #0: loss = 0.766474 (* 1 = 0.766474 loss)
I0803 16:29:33.270087 17673 sgd_solver.cpp:105] Iteration 580, lr = 0.001
I0803 16:29:51.576712 17673 solver.cpp:219] Iteration 600 (1.09252 iter/s, 18.3064s/20 iters), loss = 0.688483
I0803 16:29:51.576822 17673 solver.cpp:238]     Train net output #0: loss = 0.688483 (* 1 = 0.688483 loss)
I0803 16:29:51.576851 17673 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0803 16:30:09.883092 17673 solver.cpp:219] Iteration 620 (1.09254 iter/s, 18.306s/20 iters), loss = 0.668138
I0803 16:30:09.883198 17673 solver.cpp:238]     Train net output #0: loss = 0.668138 (* 1 = 0.668138 loss)
I0803 16:30:09.883215 17673 sgd_solver.cpp:105] Iteration 620, lr = 0.001
I0803 16:30:28.197525 17673 solver.cpp:219] Iteration 640 (1.09206 iter/s, 18.314s/20 iters), loss = 0.713655
I0803 16:30:28.197633 17673 solver.cpp:238]     Train net output #0: loss = 0.713655 (* 1 = 0.713655 loss)
I0803 16:30:28.197649 17673 sgd_solver.cpp:105] Iteration 640, lr = 0.001
I0803 16:30:39.644968 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:30:46.460852 17673 solver.cpp:219] Iteration 660 (1.09512 iter/s, 18.2629s/20 iters), loss = 0.786592
I0803 16:30:46.476222 17673 solver.cpp:238]     Train net output #0: loss = 0.786592 (* 1 = 0.786592 loss)
I0803 16:30:46.476256 17673 sgd_solver.cpp:105] Iteration 660, lr = 0.001
I0803 16:31:04.792364 17673 solver.cpp:219] Iteration 680 (1.09196 iter/s, 18.3158s/20 iters), loss = 0.757191
I0803 16:31:04.792526 17673 solver.cpp:238]     Train net output #0: loss = 0.757191 (* 1 = 0.757191 loss)
I0803 16:31:04.792557 17673 sgd_solver.cpp:105] Iteration 680, lr = 0.001
I0803 16:31:23.092036 17673 solver.cpp:219] Iteration 700 (1.09294 iter/s, 18.2992s/20 iters), loss = 0.737377
I0803 16:31:23.092129 17673 solver.cpp:238]     Train net output #0: loss = 0.737377 (* 1 = 0.737377 loss)
I0803 16:31:23.092144 17673 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0803 16:31:41.399556 17673 solver.cpp:219] Iteration 720 (1.09248 iter/s, 18.307s/20 iters), loss = 0.730567
I0803 16:31:41.399709 17673 solver.cpp:238]     Train net output #0: loss = 0.730567 (* 1 = 0.730567 loss)
I0803 16:31:41.399739 17673 sgd_solver.cpp:105] Iteration 720, lr = 0.001
I0803 16:31:59.706254 17673 solver.cpp:219] Iteration 740 (1.09252 iter/s, 18.3063s/20 iters), loss = 0.742171
I0803 16:31:59.706341 17673 solver.cpp:238]     Train net output #0: loss = 0.742171 (* 1 = 0.742171 loss)
I0803 16:31:59.706357 17673 sgd_solver.cpp:105] Iteration 740, lr = 0.001
I0803 16:32:07.416224 17673 solver.cpp:331] Iteration 750, Testing net (#0)
I0803 16:32:09.386351 17673 solver.cpp:398]     Test net output #0: accuracy = 0.652
I0803 16:32:09.386432 17673 solver.cpp:398]     Test net output #1: loss = 0.637421 (* 1 = 0.637421 loss)
I0803 16:32:19.449430 17673 solver.cpp:219] Iteration 760 (1.01303 iter/s, 19.7428s/20 iters), loss = 0.703466
I0803 16:32:19.449529 17673 solver.cpp:238]     Train net output #0: loss = 0.703466 (* 1 = 0.703466 loss)
I0803 16:32:19.449548 17673 sgd_solver.cpp:105] Iteration 760, lr = 0.001
I0803 16:32:19.838766 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:32:37.751226 17673 solver.cpp:219] Iteration 780 (1.09281 iter/s, 18.3014s/20 iters), loss = 0.693518
I0803 16:32:37.751340 17673 solver.cpp:238]     Train net output #0: loss = 0.693518 (* 1 = 0.693518 loss)
I0803 16:32:37.751359 17673 sgd_solver.cpp:105] Iteration 780, lr = 0.001
I0803 16:32:56.058414 17673 solver.cpp:219] Iteration 800 (1.09249 iter/s, 18.3068s/20 iters), loss = 0.71741
I0803 16:32:56.058526 17673 solver.cpp:238]     Train net output #0: loss = 0.71741 (* 1 = 0.71741 loss)
I0803 16:32:56.058542 17673 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0803 16:33:14.358417 17673 solver.cpp:219] Iteration 820 (1.09292 iter/s, 18.2996s/20 iters), loss = 0.718616
I0803 16:33:14.358533 17673 solver.cpp:238]     Train net output #0: loss = 0.718616 (* 1 = 0.718616 loss)
I0803 16:33:14.358548 17673 sgd_solver.cpp:105] Iteration 820, lr = 0.001
I0803 16:33:32.660305 17673 solver.cpp:219] Iteration 840 (1.09281 iter/s, 18.3014s/20 iters), loss = 0.807545
I0803 16:33:32.660455 17673 solver.cpp:238]     Train net output #0: loss = 0.807545 (* 1 = 0.807545 loss)
I0803 16:33:32.660491 17673 sgd_solver.cpp:105] Iteration 840, lr = 0.001
I0803 16:33:50.916703 17673 solver.cpp:219] Iteration 860 (1.09554 iter/s, 18.2559s/20 iters), loss = 0.675257
I0803 16:33:50.932063 17673 solver.cpp:238]     Train net output #0: loss = 0.675257 (* 1 = 0.675257 loss)
I0803 16:33:50.932086 17673 sgd_solver.cpp:105] Iteration 860, lr = 0.001
I0803 16:34:01.334691 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:34:09.204329 17673 solver.cpp:219] Iteration 880 (1.09457 iter/s, 18.272s/20 iters), loss = 0.676992
I0803 16:34:09.219447 17673 solver.cpp:238]     Train net output #0: loss = 0.676992 (* 1 = 0.676992 loss)
I0803 16:34:09.219475 17673 sgd_solver.cpp:105] Iteration 880, lr = 0.001
I0803 16:34:27.539257 17673 solver.cpp:219] Iteration 900 (1.09173 iter/s, 18.3195s/20 iters), loss = 0.648908
I0803 16:34:27.539363 17673 solver.cpp:238]     Train net output #0: loss = 0.648908 (* 1 = 0.648908 loss)
I0803 16:34:27.539377 17673 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0803 16:34:45.838362 17673 solver.cpp:219] Iteration 920 (1.09298 iter/s, 18.2986s/20 iters), loss = 0.669763
I0803 16:34:45.838555 17673 solver.cpp:238]     Train net output #0: loss = 0.669763 (* 1 = 0.669763 loss)
I0803 16:34:45.838590 17673 sgd_solver.cpp:105] Iteration 920, lr = 0.001
I0803 16:35:04.138954 17673 solver.cpp:219] Iteration 940 (1.0929 iter/s, 18.3s/20 iters), loss = 0.726611
I0803 16:35:04.139140 17673 solver.cpp:238]     Train net output #0: loss = 0.726611 (* 1 = 0.726611 loss)
I0803 16:35:04.139170 17673 sgd_solver.cpp:105] Iteration 940, lr = 0.001
I0803 16:35:22.450700 17673 solver.cpp:219] Iteration 960 (1.09223 iter/s, 18.3112s/20 iters), loss = 0.675645
I0803 16:35:22.450800 17673 solver.cpp:238]     Train net output #0: loss = 0.675645 (* 1 = 0.675645 loss)
I0803 16:35:22.450830 17673 sgd_solver.cpp:105] Iteration 960, lr = 0.001
I0803 16:35:40.351847 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:35:40.761670 17673 solver.cpp:219] Iteration 980 (1.09227 iter/s, 18.3106s/20 iters), loss = 0.716899
I0803 16:35:40.761800 17673 solver.cpp:238]     Train net output #0: loss = 0.716899 (* 1 = 0.716899 loss)
I0803 16:35:40.761834 17673 sgd_solver.cpp:105] Iteration 980, lr = 0.001
I0803 16:35:57.611657 17673 solver.cpp:331] Iteration 1000, Testing net (#0)
I0803 16:35:59.483721 17673 solver.cpp:398]     Test net output #0: accuracy = 0.606
I0803 16:35:59.483865 17673 solver.cpp:398]     Test net output #1: loss = 0.647793 (* 1 = 0.647793 loss)
I0803 16:36:00.374867 17673 solver.cpp:219] Iteration 1000 (1.01974 iter/s, 19.6128s/20 iters), loss = 0.65605
I0803 16:36:00.377985 17673 solver.cpp:238]     Train net output #0: loss = 0.65605 (* 1 = 0.65605 loss)
I0803 16:36:00.378026 17673 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0803 16:36:18.680277 17673 solver.cpp:219] Iteration 1020 (1.09278 iter/s, 18.302s/20 iters), loss = 0.718647
I0803 16:36:18.680377 17673 solver.cpp:238]     Train net output #0: loss = 0.718647 (* 1 = 0.718647 loss)
I0803 16:36:18.680394 17673 sgd_solver.cpp:105] Iteration 1020, lr = 0.001
I0803 16:36:36.993608 17673 solver.cpp:219] Iteration 1040 (1.09213 iter/s, 18.3129s/20 iters), loss = 0.685553
I0803 16:36:36.993744 17673 solver.cpp:238]     Train net output #0: loss = 0.685553 (* 1 = 0.685553 loss)
I0803 16:36:36.993777 17673 sgd_solver.cpp:105] Iteration 1040, lr = 0.001
I0803 16:36:55.259606 17673 solver.cpp:219] Iteration 1060 (1.09496 iter/s, 18.2656s/20 iters), loss = 0.691534
I0803 16:36:55.274868 17673 solver.cpp:238]     Train net output #0: loss = 0.691534 (* 1 = 0.691534 loss)
I0803 16:36:55.274900 17673 sgd_solver.cpp:105] Iteration 1060, lr = 0.001
I0803 16:37:13.586861 17673 solver.cpp:219] Iteration 1080 (1.0922 iter/s, 18.3117s/20 iters), loss = 0.636657
I0803 16:37:13.586951 17673 solver.cpp:238]     Train net output #0: loss = 0.636657 (* 1 = 0.636657 loss)
I0803 16:37:13.586967 17673 sgd_solver.cpp:105] Iteration 1080, lr = 0.001
I0803 16:37:21.436028 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:37:31.884054 17673 solver.cpp:219] Iteration 1100 (1.09309 iter/s, 18.2967s/20 iters), loss = 0.656881
I0803 16:37:31.884145 17673 solver.cpp:238]     Train net output #0: loss = 0.656881 (* 1 = 0.656881 loss)
I0803 16:37:31.884161 17673 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0803 16:37:50.188603 17673 solver.cpp:219] Iteration 1120 (1.09265 iter/s, 18.3041s/20 iters), loss = 0.695376
I0803 16:37:50.188711 17673 solver.cpp:238]     Train net output #0: loss = 0.695376 (* 1 = 0.695376 loss)
I0803 16:37:50.188735 17673 sgd_solver.cpp:105] Iteration 1120, lr = 0.001
I0803 16:38:08.493937 17673 solver.cpp:219] Iteration 1140 (1.0926 iter/s, 18.3049s/20 iters), loss = 0.724829
I0803 16:38:08.494050 17673 solver.cpp:238]     Train net output #0: loss = 0.724829 (* 1 = 0.724829 loss)
I0803 16:38:08.494068 17673 sgd_solver.cpp:105] Iteration 1140, lr = 0.001
I0803 16:38:26.796290 17673 solver.cpp:219] Iteration 1160 (1.09278 iter/s, 18.3019s/20 iters), loss = 0.704494
I0803 16:38:26.796378 17673 solver.cpp:238]     Train net output #0: loss = 0.704494 (* 1 = 0.704494 loss)
I0803 16:38:26.796391 17673 sgd_solver.cpp:105] Iteration 1160, lr = 0.001
I0803 16:38:45.105008 17673 solver.cpp:219] Iteration 1180 (1.09241 iter/s, 18.3082s/20 iters), loss = 0.709833
I0803 16:38:45.105144 17673 solver.cpp:238]     Train net output #0: loss = 0.709833 (* 1 = 0.709833 loss)
I0803 16:38:45.105170 17673 sgd_solver.cpp:105] Iteration 1180, lr = 0.001
I0803 16:39:01.979111 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:39:03.406442 17673 solver.cpp:219] Iteration 1200 (1.09284 iter/s, 18.301s/20 iters), loss = 0.598631
I0803 16:39:03.406580 17673 solver.cpp:238]     Train net output #0: loss = 0.598631 (* 1 = 0.598631 loss)
I0803 16:39:03.406612 17673 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0803 16:39:21.703430 17673 solver.cpp:219] Iteration 1220 (1.0931 iter/s, 18.2965s/20 iters), loss = 0.679077
I0803 16:39:21.703547 17673 solver.cpp:238]     Train net output #0: loss = 0.679077 (* 1 = 0.679077 loss)
I0803 16:39:21.703565 17673 sgd_solver.cpp:105] Iteration 1220, lr = 0.001
I0803 16:39:40.013957 17673 solver.cpp:219] Iteration 1240 (1.0923 iter/s, 18.31s/20 iters), loss = 0.722593
I0803 16:39:40.014096 17673 solver.cpp:238]     Train net output #0: loss = 0.722593 (* 1 = 0.722593 loss)
I0803 16:39:40.014130 17673 sgd_solver.cpp:105] Iteration 1240, lr = 0.001
I0803 16:39:47.628751 17673 solver.cpp:331] Iteration 1250, Testing net (#0)
I0803 16:39:49.006078 17729 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:39:49.652709 17673 solver.cpp:398]     Test net output #0: accuracy = 0.627
I0803 16:39:49.652814 17673 solver.cpp:398]     Test net output #1: loss = 0.631351 (* 1 = 0.631351 loss)
I0803 16:39:59.708413 17673 solver.cpp:219] Iteration 1260 (1.01554 iter/s, 19.6939s/20 iters), loss = 0.651848
I0803 16:39:59.708565 17673 solver.cpp:238]     Train net output #0: loss = 0.651848 (* 1 = 0.651848 loss)
I0803 16:39:59.708596 17673 sgd_solver.cpp:105] Iteration 1260, lr = 0.001
I0803 16:40:18.014448 17673 solver.cpp:219] Iteration 1280 (1.09257 iter/s, 18.3055s/20 iters), loss = 0.646434
I0803 16:40:18.014667 17673 solver.cpp:238]     Train net output #0: loss = 0.646434 (* 1 = 0.646434 loss)
I0803 16:40:18.014695 17673 sgd_solver.cpp:105] Iteration 1280, lr = 0.001
I0803 16:40:36.328721 17673 solver.cpp:219] Iteration 1300 (1.09208 iter/s, 18.3137s/20 iters), loss = 0.669501
I0803 16:40:36.328856 17673 solver.cpp:238]     Train net output #0: loss = 0.669501 (* 1 = 0.669501 loss)
I0803 16:40:36.328891 17673 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0803 16:40:43.104804 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:40:54.589316 17673 solver.cpp:219] Iteration 1320 (1.09528 iter/s, 18.2602s/20 iters), loss = 0.634381
I0803 16:40:54.604454 17673 solver.cpp:238]     Train net output #0: loss = 0.634381 (* 1 = 0.634381 loss)
I0803 16:40:54.604485 17673 sgd_solver.cpp:105] Iteration 1320, lr = 0.001
I0803 16:41:12.919100 17673 solver.cpp:219] Iteration 1340 (1.09204 iter/s, 18.3144s/20 iters), loss = 0.677772
I0803 16:41:12.919186 17673 solver.cpp:238]     Train net output #0: loss = 0.677772 (* 1 = 0.677772 loss)
I0803 16:41:12.919203 17673 sgd_solver.cpp:105] Iteration 1340, lr = 0.001
I0803 16:41:31.224751 17673 solver.cpp:219] Iteration 1360 (1.09258 iter/s, 18.3052s/20 iters), loss = 0.714162
I0803 16:41:31.224861 17673 solver.cpp:238]     Train net output #0: loss = 0.714162 (* 1 = 0.714162 loss)
I0803 16:41:31.224892 17673 sgd_solver.cpp:105] Iteration 1360, lr = 0.001
I0803 16:41:49.545336 17673 solver.cpp:219] Iteration 1380 (1.09169 iter/s, 18.3202s/20 iters), loss = 0.68075
I0803 16:41:49.545428 17673 solver.cpp:238]     Train net output #0: loss = 0.68075 (* 1 = 0.68075 loss)
I0803 16:41:49.545444 17673 sgd_solver.cpp:105] Iteration 1380, lr = 0.001
I0803 16:42:07.855167 17673 solver.cpp:219] Iteration 1400 (1.09234 iter/s, 18.3093s/20 iters), loss = 0.666538
I0803 16:42:07.855365 17673 solver.cpp:238]     Train net output #0: loss = 0.666538 (* 1 = 0.666538 loss)
I0803 16:42:07.855402 17673 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0803 16:42:23.020332 17728 data_layer.cpp:73] Restarting data prefetching from start.
I0803 16:42:26.165232 17673 solver.cpp:219] Iteration 1420 (1.09233 iter/s, 18.3095s/20 iters), loss = 0.662237
I0803 16:42:26.165354 17673 solver.cpp:238]     Train net output #0: loss = 0.662237 (* 1 = 0.662237 loss)
I0803 16:42:26.165376 17673 sgd_solver.cpp:105] Iteration 1420, lr = 0.001
I0803 16:42:44.462004 17673 solver.cpp:219] Iteration 1440 (1.09312 iter/s, 18.2962s/20 iters), loss = 0.623652
I0803 16:42:44.462191 17673 solver.cpp:238]     Train net output #0: loss = 0.623652 (* 1 = 0.623652 loss)
I0803 16:42:44.462229 17673 sgd_solver.cpp:105] Iteration 1440, lr = 0.001
I0803 16:43:02.780748 17673 solver.cpp:219] Iteration 1460 (1.09181 iter/s, 18.3182s/20 iters), loss = 0.709504
I0803 16:43:02.780915 17673 solver.cpp:238]     Train net output #0: loss = 0.709504 (* 1 = 0.709504 loss)
I0803 16:43:02.780959 17673 sgd_solver.cpp:105] Iteration 1460, lr = 0.001
I0803 16:43:21.092289 17673 solver.cpp:219] Iteration 1480 (1.09224 iter/s, 18.311s/20 iters), loss = 0.621444
I0803 16:43:21.092454 17673 solver.cpp:238]     Train net output #0: loss = 0.621444 (* 1 = 0.621444 loss)
I0803 16:43:21.092494 17673 sgd_solver.cpp:105] Iteration 1480, lr = 0.001
I0803 16:43:38.006081 17673 solver.cpp:331] Iteration 1500, Testing net (#0)
I0803 16:43:39.934561 17673 solver.cpp:398]     Test net output #0: accuracy = 0.632
I0803 16:43:39.934654 17673 solver.cpp:398]     Test net output #1: loss = 0.617068 (* 1 = 0.617068 loss)
I0803 16:43:40.828603 17673 solver.cpp:219] Iteration 1500 (1.01339 iter/s, 19.7358s/20 iters), loss = 0.63303
I0803 16:43:40.831738 17673 solver.cpp:238]     Train net output #0: loss = 0.63303 (* 1 = 0.63303 loss)
I0803 16:43:40.831779 17673 sgd_solver.cpp:105] Iteration 1500, lr = 0.0001
data	(256, 3, 227, 227)
label	(256,)
conv1	(256, 96, 55, 55)
pool1	(256, 96, 27, 27)
norm1	(256, 96, 27, 27)
conv2	(256, 256, 27, 27)
pool2	(256, 256, 13, 13)
norm2	(256, 256, 13, 13)
conv3	(256, 384, 13, 13)
conv4	(256, 384, 13, 13)
conv5	(256, 256, 13, 13)
pool5	(256, 256, 6, 6)
fc6	(256, 4096)
fc7	(256, 4096)
fc8	(256, 2)
loss	()
conv1	(96, 3, 11, 11) (96,)
conv2	(256, 48, 5, 5) (256,)
conv3	(384, 256, 3, 3) (384,)
conv4	(384, 192, 3, 3) (384,)
conv5	(256, 192, 3, 3) (256,)
fc6	(4096, 9216) (4096,)
fc7	(4096, 4096) (4096,)
fc8	(2, 4096) (2,)
