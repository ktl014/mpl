WARNING: Logging before InitGoogleLogging() is written to STDERR
I0803 15:42:59.166016 16203 solver.cpp:44] Initializing solver from parameters: 
test_iter: 20
test_interval: 250
base_lr: 0.001
display: 20
max_iter: 5000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1500
snapshot: 5000
snapshot_prefix: "caffenet_train"
solver_mode: GPU
net: "caffenet/train_val_exp5.prototxt"
I0803 15:42:59.166937 16203 solver.cpp:87] Creating training net from net file: caffenet/train_val_exp5.prototxt
I0803 15:42:59.168213 16203 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0803 15:42:59.168241 16203 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0803 15:42:59.168436 16203 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "train.LMDB"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 15:42:59.168532 16203 layer_factory.hpp:77] Creating layer data
I0803 15:42:59.169553 16203 db_lmdb.cpp:35] Opened lmdb train.LMDB
I0803 15:42:59.169737 16203 net.cpp:86] Creating Layer data
I0803 15:42:59.169754 16203 net.cpp:382] data -> data
I0803 15:42:59.169775 16203 net.cpp:382] data -> label
I0803 15:42:59.171900 16203 data_layer.cpp:45] output data size: 256,3,227,227
I0803 15:42:59.611618 16203 net.cpp:124] Setting up data
I0803 15:42:59.611690 16203 net.cpp:131] Top shape: 256 3 227 227 (39574272)
I0803 15:42:59.611701 16203 net.cpp:131] Top shape: 256 (256)
I0803 15:42:59.611707 16203 net.cpp:139] Memory required for data: 158298112
I0803 15:42:59.611721 16203 layer_factory.hpp:77] Creating layer conv1
I0803 15:42:59.611764 16203 net.cpp:86] Creating Layer conv1
I0803 15:42:59.611774 16203 net.cpp:408] conv1 <- data
I0803 15:42:59.611790 16203 net.cpp:382] conv1 -> conv1
I0803 15:42:59.893918 16203 net.cpp:124] Setting up conv1
I0803 15:42:59.893959 16203 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 15:42:59.893965 16203 net.cpp:139] Memory required for data: 455667712
I0803 15:42:59.893986 16203 layer_factory.hpp:77] Creating layer relu1
I0803 15:42:59.894004 16203 net.cpp:86] Creating Layer relu1
I0803 15:42:59.894011 16203 net.cpp:408] relu1 <- conv1
I0803 15:42:59.894019 16203 net.cpp:369] relu1 -> conv1 (in-place)
I0803 15:42:59.894820 16203 net.cpp:124] Setting up relu1
I0803 15:42:59.894837 16203 net.cpp:131] Top shape: 256 96 55 55 (74342400)
I0803 15:42:59.894842 16203 net.cpp:139] Memory required for data: 753037312
I0803 15:42:59.894847 16203 layer_factory.hpp:77] Creating layer pool1
I0803 15:42:59.894857 16203 net.cpp:86] Creating Layer pool1
I0803 15:42:59.894861 16203 net.cpp:408] pool1 <- conv1
I0803 15:42:59.894871 16203 net.cpp:382] pool1 -> pool1
I0803 15:42:59.894932 16203 net.cpp:124] Setting up pool1
I0803 15:42:59.894946 16203 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 15:42:59.894951 16203 net.cpp:139] Memory required for data: 824700928
I0803 15:42:59.894956 16203 layer_factory.hpp:77] Creating layer norm1
I0803 15:42:59.894968 16203 net.cpp:86] Creating Layer norm1
I0803 15:42:59.894973 16203 net.cpp:408] norm1 <- pool1
I0803 15:42:59.894981 16203 net.cpp:382] norm1 -> norm1
I0803 15:42:59.895241 16203 net.cpp:124] Setting up norm1
I0803 15:42:59.895256 16203 net.cpp:131] Top shape: 256 96 27 27 (17915904)
I0803 15:42:59.895262 16203 net.cpp:139] Memory required for data: 896364544
I0803 15:42:59.895267 16203 layer_factory.hpp:77] Creating layer conv2
I0803 15:42:59.895285 16203 net.cpp:86] Creating Layer conv2
I0803 15:42:59.895290 16203 net.cpp:408] conv2 <- norm1
I0803 15:42:59.895300 16203 net.cpp:382] conv2 -> conv2
I0803 15:42:59.911650 16203 net.cpp:124] Setting up conv2
I0803 15:42:59.911687 16203 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 15:42:59.911694 16203 net.cpp:139] Memory required for data: 1087467520
I0803 15:42:59.911710 16203 layer_factory.hpp:77] Creating layer relu2
I0803 15:42:59.911734 16203 net.cpp:86] Creating Layer relu2
I0803 15:42:59.911741 16203 net.cpp:408] relu2 <- conv2
I0803 15:42:59.911749 16203 net.cpp:369] relu2 -> conv2 (in-place)
I0803 15:42:59.911993 16203 net.cpp:124] Setting up relu2
I0803 15:42:59.912009 16203 net.cpp:131] Top shape: 256 256 27 27 (47775744)
I0803 15:42:59.912014 16203 net.cpp:139] Memory required for data: 1278570496
I0803 15:42:59.912019 16203 layer_factory.hpp:77] Creating layer pool2
I0803 15:42:59.912029 16203 net.cpp:86] Creating Layer pool2
I0803 15:42:59.912034 16203 net.cpp:408] pool2 <- conv2
I0803 15:42:59.912040 16203 net.cpp:382] pool2 -> pool2
I0803 15:42:59.912098 16203 net.cpp:124] Setting up pool2
I0803 15:42:59.912111 16203 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 15:42:59.912118 16203 net.cpp:139] Memory required for data: 1322872832
I0803 15:42:59.912122 16203 layer_factory.hpp:77] Creating layer norm2
I0803 15:42:59.912137 16203 net.cpp:86] Creating Layer norm2
I0803 15:42:59.912143 16203 net.cpp:408] norm2 <- pool2
I0803 15:42:59.912150 16203 net.cpp:382] norm2 -> norm2
I0803 15:42:59.912984 16203 net.cpp:124] Setting up norm2
I0803 15:42:59.913007 16203 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 15:42:59.913012 16203 net.cpp:139] Memory required for data: 1367175168
I0803 15:42:59.913017 16203 layer_factory.hpp:77] Creating layer conv3
I0803 15:42:59.913035 16203 net.cpp:86] Creating Layer conv3
I0803 15:42:59.913040 16203 net.cpp:408] conv3 <- norm2
I0803 15:42:59.913051 16203 net.cpp:382] conv3 -> conv3
I0803 15:42:59.949065 16203 net.cpp:124] Setting up conv3
I0803 15:42:59.949122 16203 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 15:42:59.949129 16203 net.cpp:139] Memory required for data: 1433628672
I0803 15:42:59.949147 16203 layer_factory.hpp:77] Creating layer relu3
I0803 15:42:59.949162 16203 net.cpp:86] Creating Layer relu3
I0803 15:42:59.949172 16203 net.cpp:408] relu3 <- conv3
I0803 15:42:59.949184 16203 net.cpp:369] relu3 -> conv3 (in-place)
I0803 15:42:59.949404 16203 net.cpp:124] Setting up relu3
I0803 15:42:59.949419 16203 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 15:42:59.949425 16203 net.cpp:139] Memory required for data: 1500082176
I0803 15:42:59.949430 16203 layer_factory.hpp:77] Creating layer conv4
I0803 15:42:59.949448 16203 net.cpp:86] Creating Layer conv4
I0803 15:42:59.949455 16203 net.cpp:408] conv4 <- conv3
I0803 15:42:59.949466 16203 net.cpp:382] conv4 -> conv4
I0803 15:42:59.997233 16203 net.cpp:124] Setting up conv4
I0803 15:42:59.997304 16203 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 15:42:59.997314 16203 net.cpp:139] Memory required for data: 1566535680
I0803 15:42:59.997339 16203 layer_factory.hpp:77] Creating layer relu4
I0803 15:42:59.997371 16203 net.cpp:86] Creating Layer relu4
I0803 15:42:59.997385 16203 net.cpp:408] relu4 <- conv4
I0803 15:42:59.997403 16203 net.cpp:369] relu4 -> conv4 (in-place)
I0803 15:42:59.997786 16203 net.cpp:124] Setting up relu4
I0803 15:42:59.997817 16203 net.cpp:131] Top shape: 256 384 13 13 (16613376)
I0803 15:42:59.997826 16203 net.cpp:139] Memory required for data: 1632989184
I0803 15:42:59.997834 16203 layer_factory.hpp:77] Creating layer conv5
I0803 15:42:59.997869 16203 net.cpp:86] Creating Layer conv5
I0803 15:42:59.997881 16203 net.cpp:408] conv5 <- conv4
I0803 15:42:59.997900 16203 net.cpp:382] conv5 -> conv5
I0803 15:43:00.039080 16203 net.cpp:124] Setting up conv5
I0803 15:43:00.039151 16203 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 15:43:00.039160 16203 net.cpp:139] Memory required for data: 1677291520
I0803 15:43:00.039239 16203 layer_factory.hpp:77] Creating layer relu5
I0803 15:43:00.039315 16203 net.cpp:86] Creating Layer relu5
I0803 15:43:00.039335 16203 net.cpp:408] relu5 <- conv5
I0803 15:43:00.039352 16203 net.cpp:369] relu5 -> conv5 (in-place)
I0803 15:43:00.039650 16203 net.cpp:124] Setting up relu5
I0803 15:43:00.039669 16203 net.cpp:131] Top shape: 256 256 13 13 (11075584)
I0803 15:43:00.039677 16203 net.cpp:139] Memory required for data: 1721593856
I0803 15:43:00.039685 16203 layer_factory.hpp:77] Creating layer pool5
I0803 15:43:00.039724 16203 net.cpp:86] Creating Layer pool5
I0803 15:43:00.039734 16203 net.cpp:408] pool5 <- conv5
I0803 15:43:00.039757 16203 net.cpp:382] pool5 -> pool5
I0803 15:43:00.039891 16203 net.cpp:124] Setting up pool5
I0803 15:43:00.039908 16203 net.cpp:131] Top shape: 256 256 6 6 (2359296)
I0803 15:43:00.039916 16203 net.cpp:139] Memory required for data: 1731031040
I0803 15:43:00.039922 16203 layer_factory.hpp:77] Creating layer fc6
I0803 15:43:00.040015 16203 net.cpp:86] Creating Layer fc6
I0803 15:43:00.040026 16203 net.cpp:408] fc6 <- pool5
I0803 15:43:00.040036 16203 net.cpp:382] fc6 -> fc6
I0803 15:43:01.468258 16203 net.cpp:124] Setting up fc6
I0803 15:43:01.468330 16203 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 15:43:01.468338 16203 net.cpp:139] Memory required for data: 1735225344
I0803 15:43:01.468369 16203 layer_factory.hpp:77] Creating layer relu6
I0803 15:43:01.468395 16203 net.cpp:86] Creating Layer relu6
I0803 15:43:01.468403 16203 net.cpp:408] relu6 <- fc6
I0803 15:43:01.468418 16203 net.cpp:369] relu6 -> fc6 (in-place)
I0803 15:43:01.469492 16203 net.cpp:124] Setting up relu6
I0803 15:43:01.469508 16203 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 15:43:01.469513 16203 net.cpp:139] Memory required for data: 1739419648
I0803 15:43:01.469518 16203 layer_factory.hpp:77] Creating layer drop6
I0803 15:43:01.469547 16203 net.cpp:86] Creating Layer drop6
I0803 15:43:01.469566 16203 net.cpp:408] drop6 <- fc6
I0803 15:43:01.469573 16203 net.cpp:369] drop6 -> fc6 (in-place)
I0803 15:43:01.469605 16203 net.cpp:124] Setting up drop6
I0803 15:43:01.469624 16203 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 15:43:01.469630 16203 net.cpp:139] Memory required for data: 1743613952
I0803 15:43:01.469633 16203 layer_factory.hpp:77] Creating layer fc7
I0803 15:43:01.469647 16203 net.cpp:86] Creating Layer fc7
I0803 15:43:01.469652 16203 net.cpp:408] fc7 <- fc6
I0803 15:43:01.469660 16203 net.cpp:382] fc7 -> fc7
I0803 15:43:02.097815 16203 net.cpp:124] Setting up fc7
I0803 15:43:02.097867 16203 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 15:43:02.097873 16203 net.cpp:139] Memory required for data: 1747808256
I0803 15:43:02.097890 16203 layer_factory.hpp:77] Creating layer relu7
I0803 15:43:02.097924 16203 net.cpp:86] Creating Layer relu7
I0803 15:43:02.097931 16203 net.cpp:408] relu7 <- fc7
I0803 15:43:02.097944 16203 net.cpp:369] relu7 -> fc7 (in-place)
I0803 15:43:02.098948 16203 net.cpp:124] Setting up relu7
I0803 15:43:02.098965 16203 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 15:43:02.098970 16203 net.cpp:139] Memory required for data: 1752002560
I0803 15:43:02.098989 16203 layer_factory.hpp:77] Creating layer drop7
I0803 15:43:02.099000 16203 net.cpp:86] Creating Layer drop7
I0803 15:43:02.099007 16203 net.cpp:408] drop7 <- fc7
I0803 15:43:02.099016 16203 net.cpp:369] drop7 -> fc7 (in-place)
I0803 15:43:02.099061 16203 net.cpp:124] Setting up drop7
I0803 15:43:02.099072 16203 net.cpp:131] Top shape: 256 4096 (1048576)
I0803 15:43:02.099079 16203 net.cpp:139] Memory required for data: 1756196864
I0803 15:43:02.099084 16203 layer_factory.hpp:77] Creating layer fc8
I0803 15:43:02.099102 16203 net.cpp:86] Creating Layer fc8
I0803 15:43:02.099107 16203 net.cpp:408] fc8 <- fc7
I0803 15:43:02.099118 16203 net.cpp:382] fc8 -> fc8
I0803 15:43:02.100265 16203 net.cpp:124] Setting up fc8
I0803 15:43:02.100281 16203 net.cpp:131] Top shape: 256 2 (512)
I0803 15:43:02.100298 16203 net.cpp:139] Memory required for data: 1756198912
I0803 15:43:02.100308 16203 layer_factory.hpp:77] Creating layer loss
I0803 15:43:02.100327 16203 net.cpp:86] Creating Layer loss
I0803 15:43:02.100332 16203 net.cpp:408] loss <- fc8
I0803 15:43:02.100338 16203 net.cpp:408] loss <- label
I0803 15:43:02.100347 16203 net.cpp:382] loss -> loss
I0803 15:43:02.100365 16203 layer_factory.hpp:77] Creating layer loss
I0803 15:43:02.100713 16203 net.cpp:124] Setting up loss
I0803 15:43:02.100726 16203 net.cpp:131] Top shape: (1)
I0803 15:43:02.100730 16203 net.cpp:134]     with loss weight 1
I0803 15:43:02.100787 16203 net.cpp:139] Memory required for data: 1756198916
I0803 15:43:02.100793 16203 net.cpp:200] loss needs backward computation.
I0803 15:43:02.100801 16203 net.cpp:200] fc8 needs backward computation.
I0803 15:43:02.100807 16203 net.cpp:200] drop7 needs backward computation.
I0803 15:43:02.100812 16203 net.cpp:200] relu7 needs backward computation.
I0803 15:43:02.100816 16203 net.cpp:200] fc7 needs backward computation.
I0803 15:43:02.100821 16203 net.cpp:200] drop6 needs backward computation.
I0803 15:43:02.100824 16203 net.cpp:200] relu6 needs backward computation.
I0803 15:43:02.100828 16203 net.cpp:200] fc6 needs backward computation.
I0803 15:43:02.100833 16203 net.cpp:202] pool5 does not need backward computation.
I0803 15:43:02.100838 16203 net.cpp:202] relu5 does not need backward computation.
I0803 15:43:02.100842 16203 net.cpp:202] conv5 does not need backward computation.
I0803 15:43:02.100847 16203 net.cpp:202] relu4 does not need backward computation.
I0803 15:43:02.100857 16203 net.cpp:202] conv4 does not need backward computation.
I0803 15:43:02.100863 16203 net.cpp:202] relu3 does not need backward computation.
I0803 15:43:02.100872 16203 net.cpp:202] conv3 does not need backward computation.
I0803 15:43:02.100878 16203 net.cpp:202] norm2 does not need backward computation.
I0803 15:43:02.100884 16203 net.cpp:202] pool2 does not need backward computation.
I0803 15:43:02.100889 16203 net.cpp:202] relu2 does not need backward computation.
I0803 15:43:02.100894 16203 net.cpp:202] conv2 does not need backward computation.
I0803 15:43:02.100900 16203 net.cpp:202] norm1 does not need backward computation.
I0803 15:43:02.100905 16203 net.cpp:202] pool1 does not need backward computation.
I0803 15:43:02.100911 16203 net.cpp:202] relu1 does not need backward computation.
I0803 15:43:02.100916 16203 net.cpp:202] conv1 does not need backward computation.
I0803 15:43:02.100922 16203 net.cpp:202] data does not need backward computation.
I0803 15:43:02.100926 16203 net.cpp:244] This network produces output loss
I0803 15:43:02.100946 16203 net.cpp:257] Network initialization done.
I0803 15:43:02.102972 16203 solver.cpp:173] Creating test net (#0) specified by net file: caffenet/train_val_exp5.prototxt
I0803 15:43:02.103049 16203 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0803 15:43:02.103301 16203 net.cpp:53] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "val.LMDB"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0803 15:43:02.103440 16203 layer_factory.hpp:77] Creating layer data
I0803 15:43:02.104913 16203 db_lmdb.cpp:35] Opened lmdb val.LMDB
I0803 15:43:02.105202 16203 net.cpp:86] Creating Layer data
I0803 15:43:02.105244 16203 net.cpp:382] data -> data
I0803 15:43:02.105283 16203 net.cpp:382] data -> label
I0803 15:43:02.106007 16203 data_layer.cpp:45] output data size: 50,3,227,227
I0803 15:43:02.220017 16203 net.cpp:124] Setting up data
I0803 15:43:02.220074 16203 net.cpp:131] Top shape: 50 3 227 227 (7729350)
I0803 15:43:02.220084 16203 net.cpp:131] Top shape: 50 (50)
I0803 15:43:02.220090 16203 net.cpp:139] Memory required for data: 30917600
I0803 15:43:02.220100 16203 layer_factory.hpp:77] Creating layer label_data_1_split
I0803 15:43:02.220122 16203 net.cpp:86] Creating Layer label_data_1_split
I0803 15:43:02.220130 16203 net.cpp:408] label_data_1_split <- label
I0803 15:43:02.220147 16203 net.cpp:382] label_data_1_split -> label_data_1_split_0
I0803 15:43:02.220170 16203 net.cpp:382] label_data_1_split -> label_data_1_split_1
I0803 15:43:02.220260 16203 net.cpp:124] Setting up label_data_1_split
I0803 15:43:02.220274 16203 net.cpp:131] Top shape: 50 (50)
I0803 15:43:02.220291 16203 net.cpp:131] Top shape: 50 (50)
I0803 15:43:02.220299 16203 net.cpp:139] Memory required for data: 30918000
I0803 15:43:02.220305 16203 layer_factory.hpp:77] Creating layer conv1
I0803 15:43:02.220329 16203 net.cpp:86] Creating Layer conv1
I0803 15:43:02.220336 16203 net.cpp:408] conv1 <- data
I0803 15:43:02.220348 16203 net.cpp:382] conv1 -> conv1
I0803 15:43:02.230392 16203 net.cpp:124] Setting up conv1
I0803 15:43:02.230435 16203 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 15:43:02.230450 16203 net.cpp:139] Memory required for data: 88998000
I0803 15:43:02.230505 16203 layer_factory.hpp:77] Creating layer relu1
I0803 15:43:02.230528 16203 net.cpp:86] Creating Layer relu1
I0803 15:43:02.230542 16203 net.cpp:408] relu1 <- conv1
I0803 15:43:02.230561 16203 net.cpp:369] relu1 -> conv1 (in-place)
I0803 15:43:02.230979 16203 net.cpp:124] Setting up relu1
I0803 15:43:02.231007 16203 net.cpp:131] Top shape: 50 96 55 55 (14520000)
I0803 15:43:02.231019 16203 net.cpp:139] Memory required for data: 147078000
I0803 15:43:02.231029 16203 layer_factory.hpp:77] Creating layer pool1
I0803 15:43:02.231048 16203 net.cpp:86] Creating Layer pool1
I0803 15:43:02.231060 16203 net.cpp:408] pool1 <- conv1
I0803 15:43:02.231075 16203 net.cpp:382] pool1 -> pool1
I0803 15:43:02.231204 16203 net.cpp:124] Setting up pool1
I0803 15:43:02.231227 16203 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 15:43:02.231238 16203 net.cpp:139] Memory required for data: 161074800
I0803 15:43:02.231248 16203 layer_factory.hpp:77] Creating layer norm1
I0803 15:43:02.231266 16203 net.cpp:86] Creating Layer norm1
I0803 15:43:02.231278 16203 net.cpp:408] norm1 <- pool1
I0803 15:43:02.231292 16203 net.cpp:382] norm1 -> norm1
I0803 15:43:02.232852 16203 net.cpp:124] Setting up norm1
I0803 15:43:02.232885 16203 net.cpp:131] Top shape: 50 96 27 27 (3499200)
I0803 15:43:02.232898 16203 net.cpp:139] Memory required for data: 175071600
I0803 15:43:02.232908 16203 layer_factory.hpp:77] Creating layer conv2
I0803 15:43:02.232946 16203 net.cpp:86] Creating Layer conv2
I0803 15:43:02.232959 16203 net.cpp:408] conv2 <- norm1
I0803 15:43:02.232980 16203 net.cpp:382] conv2 -> conv2
I0803 15:43:02.261212 16203 net.cpp:124] Setting up conv2
I0803 15:43:02.261265 16203 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 15:43:02.261278 16203 net.cpp:139] Memory required for data: 212396400
I0803 15:43:02.261307 16203 layer_factory.hpp:77] Creating layer relu2
I0803 15:43:02.261340 16203 net.cpp:86] Creating Layer relu2
I0803 15:43:02.261353 16203 net.cpp:408] relu2 <- conv2
I0803 15:43:02.261368 16203 net.cpp:369] relu2 -> conv2 (in-place)
I0803 15:43:02.261741 16203 net.cpp:124] Setting up relu2
I0803 15:43:02.261768 16203 net.cpp:131] Top shape: 50 256 27 27 (9331200)
I0803 15:43:02.261780 16203 net.cpp:139] Memory required for data: 249721200
I0803 15:43:02.261788 16203 layer_factory.hpp:77] Creating layer pool2
I0803 15:43:02.261808 16203 net.cpp:86] Creating Layer pool2
I0803 15:43:02.261826 16203 net.cpp:408] pool2 <- conv2
I0803 15:43:02.261837 16203 net.cpp:382] pool2 -> pool2
I0803 15:43:02.261936 16203 net.cpp:124] Setting up pool2
I0803 15:43:02.261956 16203 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 15:43:02.261966 16203 net.cpp:139] Memory required for data: 258374000
I0803 15:43:02.261973 16203 layer_factory.hpp:77] Creating layer norm2
I0803 15:43:02.261992 16203 net.cpp:86] Creating Layer norm2
I0803 15:43:02.262002 16203 net.cpp:408] norm2 <- pool2
I0803 15:43:02.262013 16203 net.cpp:382] norm2 -> norm2
I0803 15:43:02.263327 16203 net.cpp:124] Setting up norm2
I0803 15:43:02.263358 16203 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 15:43:02.263370 16203 net.cpp:139] Memory required for data: 267026800
I0803 15:43:02.263378 16203 layer_factory.hpp:77] Creating layer conv3
I0803 15:43:02.263406 16203 net.cpp:86] Creating Layer conv3
I0803 15:43:02.263417 16203 net.cpp:408] conv3 <- norm2
I0803 15:43:02.263442 16203 net.cpp:382] conv3 -> conv3
I0803 15:43:02.314442 16203 net.cpp:124] Setting up conv3
I0803 15:43:02.314496 16203 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 15:43:02.314504 16203 net.cpp:139] Memory required for data: 280006000
I0803 15:43:02.314541 16203 layer_factory.hpp:77] Creating layer relu3
I0803 15:43:02.314560 16203 net.cpp:86] Creating Layer relu3
I0803 15:43:02.314570 16203 net.cpp:408] relu3 <- conv3
I0803 15:43:02.314585 16203 net.cpp:369] relu3 -> conv3 (in-place)
I0803 15:43:02.315563 16203 net.cpp:124] Setting up relu3
I0803 15:43:02.315587 16203 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 15:43:02.315593 16203 net.cpp:139] Memory required for data: 292985200
I0803 15:43:02.315599 16203 layer_factory.hpp:77] Creating layer conv4
I0803 15:43:02.315621 16203 net.cpp:86] Creating Layer conv4
I0803 15:43:02.315629 16203 net.cpp:408] conv4 <- conv3
I0803 15:43:02.315639 16203 net.cpp:382] conv4 -> conv4
I0803 15:43:02.355855 16203 net.cpp:124] Setting up conv4
I0803 15:43:02.355908 16203 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 15:43:02.355916 16203 net.cpp:139] Memory required for data: 305964400
I0803 15:43:02.355933 16203 layer_factory.hpp:77] Creating layer relu4
I0803 15:43:02.355948 16203 net.cpp:86] Creating Layer relu4
I0803 15:43:02.355955 16203 net.cpp:408] relu4 <- conv4
I0803 15:43:02.355969 16203 net.cpp:369] relu4 -> conv4 (in-place)
I0803 15:43:02.356235 16203 net.cpp:124] Setting up relu4
I0803 15:43:02.356251 16203 net.cpp:131] Top shape: 50 384 13 13 (3244800)
I0803 15:43:02.356256 16203 net.cpp:139] Memory required for data: 318943600
I0803 15:43:02.356261 16203 layer_factory.hpp:77] Creating layer conv5
I0803 15:43:02.356277 16203 net.cpp:86] Creating Layer conv5
I0803 15:43:02.356282 16203 net.cpp:408] conv5 <- conv4
I0803 15:43:02.356294 16203 net.cpp:382] conv5 -> conv5
I0803 15:43:02.378469 16203 net.cpp:124] Setting up conv5
I0803 15:43:02.378523 16203 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 15:43:02.378530 16203 net.cpp:139] Memory required for data: 327596400
I0803 15:43:02.378559 16203 layer_factory.hpp:77] Creating layer relu5
I0803 15:43:02.378576 16203 net.cpp:86] Creating Layer relu5
I0803 15:43:02.378582 16203 net.cpp:408] relu5 <- conv5
I0803 15:43:02.378592 16203 net.cpp:369] relu5 -> conv5 (in-place)
I0803 15:43:02.378821 16203 net.cpp:124] Setting up relu5
I0803 15:43:02.378836 16203 net.cpp:131] Top shape: 50 256 13 13 (2163200)
I0803 15:43:02.378842 16203 net.cpp:139] Memory required for data: 336249200
I0803 15:43:02.378849 16203 layer_factory.hpp:77] Creating layer pool5
I0803 15:43:02.378866 16203 net.cpp:86] Creating Layer pool5
I0803 15:43:02.378873 16203 net.cpp:408] pool5 <- conv5
I0803 15:43:02.378895 16203 net.cpp:382] pool5 -> pool5
I0803 15:43:02.378967 16203 net.cpp:124] Setting up pool5
I0803 15:43:02.378978 16203 net.cpp:131] Top shape: 50 256 6 6 (460800)
I0803 15:43:02.378985 16203 net.cpp:139] Memory required for data: 338092400
I0803 15:43:02.378989 16203 layer_factory.hpp:77] Creating layer fc6
I0803 15:43:02.379004 16203 net.cpp:86] Creating Layer fc6
I0803 15:43:02.379010 16203 net.cpp:408] fc6 <- pool5
I0803 15:43:02.379019 16203 net.cpp:382] fc6 -> fc6
I0803 15:43:03.806905 16203 net.cpp:124] Setting up fc6
I0803 15:43:03.806968 16203 net.cpp:131] Top shape: 50 4096 (204800)
I0803 15:43:03.806974 16203 net.cpp:139] Memory required for data: 338911600
I0803 15:43:03.806993 16203 layer_factory.hpp:77] Creating layer relu6
I0803 15:43:03.807024 16203 net.cpp:86] Creating Layer relu6
I0803 15:43:03.807032 16203 net.cpp:408] relu6 <- fc6
I0803 15:43:03.807044 16203 net.cpp:369] relu6 -> fc6 (in-place)
I0803 15:43:03.808079 16203 net.cpp:124] Setting up relu6
I0803 15:43:03.808094 16203 net.cpp:131] Top shape: 50 4096 (204800)
I0803 15:43:03.808099 16203 net.cpp:139] Memory required for data: 339730800
I0803 15:43:03.808116 16203 layer_factory.hpp:77] Creating layer drop6
I0803 15:43:03.808127 16203 net.cpp:86] Creating Layer drop6
I0803 15:43:03.808132 16203 net.cpp:408] drop6 <- fc6
I0803 15:43:03.808141 16203 net.cpp:369] drop6 -> fc6 (in-place)
I0803 15:43:03.808200 16203 net.cpp:124] Setting up drop6
I0803 15:43:03.808212 16203 net.cpp:131] Top shape: 50 4096 (204800)
I0803 15:43:03.808218 16203 net.cpp:139] Memory required for data: 340550000
I0803 15:43:03.808223 16203 layer_factory.hpp:77] Creating layer fc7
I0803 15:43:03.808235 16203 net.cpp:86] Creating Layer fc7
I0803 15:43:03.808243 16203 net.cpp:408] fc7 <- fc6
I0803 15:43:03.808255 16203 net.cpp:382] fc7 -> fc7
I0803 15:43:04.464221 16203 net.cpp:124] Setting up fc7
I0803 15:43:04.464282 16203 net.cpp:131] Top shape: 50 4096 (204800)
I0803 15:43:04.464288 16203 net.cpp:139] Memory required for data: 341369200
I0803 15:43:04.464305 16203 layer_factory.hpp:77] Creating layer relu7
I0803 15:43:04.464321 16203 net.cpp:86] Creating Layer relu7
I0803 15:43:04.464328 16203 net.cpp:408] relu7 <- fc7
I0803 15:43:04.464341 16203 net.cpp:369] relu7 -> fc7 (in-place)
I0803 15:43:04.464663 16203 net.cpp:124] Setting up relu7
I0803 15:43:04.464679 16203 net.cpp:131] Top shape: 50 4096 (204800)
I0803 15:43:04.464684 16203 net.cpp:139] Memory required for data: 342188400
I0803 15:43:04.464689 16203 layer_factory.hpp:77] Creating layer drop7
I0803 15:43:04.464699 16203 net.cpp:86] Creating Layer drop7
I0803 15:43:04.464704 16203 net.cpp:408] drop7 <- fc7
I0803 15:43:04.464715 16203 net.cpp:369] drop7 -> fc7 (in-place)
I0803 15:43:04.464757 16203 net.cpp:124] Setting up drop7
I0803 15:43:04.464768 16203 net.cpp:131] Top shape: 50 4096 (204800)
I0803 15:43:04.464776 16203 net.cpp:139] Memory required for data: 343007600
I0803 15:43:04.464779 16203 layer_factory.hpp:77] Creating layer fc8
I0803 15:43:04.464793 16203 net.cpp:86] Creating Layer fc8
I0803 15:43:04.464800 16203 net.cpp:408] fc8 <- fc7
I0803 15:43:04.464810 16203 net.cpp:382] fc8 -> fc8
I0803 15:43:04.465279 16203 net.cpp:124] Setting up fc8
I0803 15:43:04.465296 16203 net.cpp:131] Top shape: 50 2 (100)
I0803 15:43:04.465302 16203 net.cpp:139] Memory required for data: 343008000
I0803 15:43:04.465312 16203 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0803 15:43:04.465322 16203 net.cpp:86] Creating Layer fc8_fc8_0_split
I0803 15:43:04.465327 16203 net.cpp:408] fc8_fc8_0_split <- fc8
I0803 15:43:04.465334 16203 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0803 15:43:04.465344 16203 net.cpp:382] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0803 15:43:04.465401 16203 net.cpp:124] Setting up fc8_fc8_0_split
I0803 15:43:04.465412 16203 net.cpp:131] Top shape: 50 2 (100)
I0803 15:43:04.465420 16203 net.cpp:131] Top shape: 50 2 (100)
I0803 15:43:04.465425 16203 net.cpp:139] Memory required for data: 343008800
I0803 15:43:04.465430 16203 layer_factory.hpp:77] Creating layer accuracy
I0803 15:43:04.465453 16203 net.cpp:86] Creating Layer accuracy
I0803 15:43:04.465461 16203 net.cpp:408] accuracy <- fc8_fc8_0_split_0
I0803 15:43:04.465466 16203 net.cpp:408] accuracy <- label_data_1_split_0
I0803 15:43:04.465477 16203 net.cpp:382] accuracy -> accuracy
I0803 15:43:04.465493 16203 net.cpp:124] Setting up accuracy
I0803 15:43:04.465502 16203 net.cpp:131] Top shape: (1)
I0803 15:43:04.465507 16203 net.cpp:139] Memory required for data: 343008804
I0803 15:43:04.465512 16203 layer_factory.hpp:77] Creating layer loss
I0803 15:43:04.465520 16203 net.cpp:86] Creating Layer loss
I0803 15:43:04.465525 16203 net.cpp:408] loss <- fc8_fc8_0_split_1
I0803 15:43:04.465531 16203 net.cpp:408] loss <- label_data_1_split_1
I0803 15:43:04.465539 16203 net.cpp:382] loss -> loss
I0803 15:43:04.465553 16203 layer_factory.hpp:77] Creating layer loss
I0803 15:43:04.466608 16203 net.cpp:124] Setting up loss
I0803 15:43:04.466624 16203 net.cpp:131] Top shape: (1)
I0803 15:43:04.466630 16203 net.cpp:134]     with loss weight 1
I0803 15:43:04.466652 16203 net.cpp:139] Memory required for data: 343008808
I0803 15:43:04.466658 16203 net.cpp:200] loss needs backward computation.
I0803 15:43:04.466665 16203 net.cpp:202] accuracy does not need backward computation.
I0803 15:43:04.466672 16203 net.cpp:200] fc8_fc8_0_split needs backward computation.
I0803 15:43:04.466676 16203 net.cpp:200] fc8 needs backward computation.
I0803 15:43:04.466681 16203 net.cpp:200] drop7 needs backward computation.
I0803 15:43:04.466686 16203 net.cpp:200] relu7 needs backward computation.
I0803 15:43:04.466691 16203 net.cpp:200] fc7 needs backward computation.
I0803 15:43:04.466696 16203 net.cpp:200] drop6 needs backward computation.
I0803 15:43:04.466701 16203 net.cpp:200] relu6 needs backward computation.
I0803 15:43:04.466706 16203 net.cpp:200] fc6 needs backward computation.
I0803 15:43:04.466711 16203 net.cpp:202] pool5 does not need backward computation.
I0803 15:43:04.466717 16203 net.cpp:202] relu5 does not need backward computation.
I0803 15:43:04.466722 16203 net.cpp:202] conv5 does not need backward computation.
I0803 15:43:04.466727 16203 net.cpp:202] relu4 does not need backward computation.
I0803 15:43:04.466732 16203 net.cpp:202] conv4 does not need backward computation.
I0803 15:43:04.466737 16203 net.cpp:202] relu3 does not need backward computation.
I0803 15:43:04.466742 16203 net.cpp:202] conv3 does not need backward computation.
I0803 15:43:04.466747 16203 net.cpp:202] norm2 does not need backward computation.
I0803 15:43:04.466753 16203 net.cpp:202] pool2 does not need backward computation.
I0803 15:43:04.466758 16203 net.cpp:202] relu2 does not need backward computation.
I0803 15:43:04.466763 16203 net.cpp:202] conv2 does not need backward computation.
I0803 15:43:04.466769 16203 net.cpp:202] norm1 does not need backward computation.
I0803 15:43:04.466774 16203 net.cpp:202] pool1 does not need backward computation.
I0803 15:43:04.466781 16203 net.cpp:202] relu1 does not need backward computation.
I0803 15:43:04.466785 16203 net.cpp:202] conv1 does not need backward computation.
I0803 15:43:04.466792 16203 net.cpp:202] label_data_1_split does not need backward computation.
I0803 15:43:04.466799 16203 net.cpp:202] data does not need backward computation.
I0803 15:43:04.466804 16203 net.cpp:244] This network produces output accuracy
I0803 15:43:04.466810 16203 net.cpp:244] This network produces output loss
I0803 15:43:04.466835 16203 net.cpp:257] Network initialization done.
I0803 15:43:04.466961 16203 solver.cpp:56] Solver scaffolding done.
I0803 15:43:05.096204 16203 solver.cpp:331] Iteration 0, Testing net (#0)
I0803 15:43:05.424625 16203 blocking_queue.cpp:49] Waiting for data
I0803 15:43:06.472470 16203 solver.cpp:398]     Test net output #0: accuracy = 0.628
I0803 15:43:06.472544 16203 solver.cpp:398]     Test net output #1: loss = 0.716107 (* 1 = 0.716107 loss)
I0803 15:43:06.836431 16203 solver.cpp:219] Iteration 0 (0 iter/s, 1.74561s/20 iters), loss = 0.780811
I0803 15:43:06.836475 16203 solver.cpp:238]     Train net output #0: loss = 0.780811 (* 1 = 0.780811 loss)
I0803 15:43:06.836508 16203 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0803 15:43:13.623286 16203 solver.cpp:219] Iteration 20 (2.94695 iter/s, 6.78668s/20 iters), loss = 0.731238
I0803 15:43:13.623404 16203 solver.cpp:238]     Train net output #0: loss = 0.731238 (* 1 = 0.731238 loss)
I0803 15:43:13.623422 16203 sgd_solver.cpp:105] Iteration 20, lr = 0.001
I0803 15:43:21.118366 16203 solver.cpp:219] Iteration 40 (2.6685 iter/s, 7.49485s/20 iters), loss = 0.763459
I0803 15:43:21.118458 16203 solver.cpp:238]     Train net output #0: loss = 0.763459 (* 1 = 0.763459 loss)
I0803 15:43:21.118474 16203 sgd_solver.cpp:105] Iteration 40, lr = 0.001
I0803 15:43:28.410874 16203 solver.cpp:219] Iteration 60 (2.74263 iter/s, 7.29226s/20 iters), loss = 0.739899
I0803 15:43:28.410995 16203 solver.cpp:238]     Train net output #0: loss = 0.739899 (* 1 = 0.739899 loss)
I0803 15:43:28.411010 16203 sgd_solver.cpp:105] Iteration 60, lr = 0.001
I0803 15:43:35.899119 16203 solver.cpp:219] Iteration 80 (2.67093 iter/s, 7.48803s/20 iters), loss = 0.896069
I0803 15:43:35.899209 16203 solver.cpp:238]     Train net output #0: loss = 0.896069 (* 1 = 0.896069 loss)
I0803 15:43:35.899221 16203 sgd_solver.cpp:105] Iteration 80, lr = 0.001
I0803 15:43:43.189406 16203 solver.cpp:219] Iteration 100 (2.74347 iter/s, 7.29005s/20 iters), loss = 0.691772
I0803 15:43:43.189538 16203 solver.cpp:238]     Train net output #0: loss = 0.691772 (* 1 = 0.691772 loss)
I0803 15:43:43.189589 16203 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0803 15:43:45.978507 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:43:50.577159 16203 solver.cpp:219] Iteration 120 (2.70728 iter/s, 7.3875s/20 iters), loss = 0.711703
I0803 15:43:50.577281 16203 solver.cpp:238]     Train net output #0: loss = 0.711703 (* 1 = 0.711703 loss)
I0803 15:43:50.577303 16203 sgd_solver.cpp:105] Iteration 120, lr = 0.001
I0803 15:43:57.723515 16203 solver.cpp:219] Iteration 140 (2.79872 iter/s, 7.14613s/20 iters), loss = 0.666175
I0803 15:43:57.723594 16203 solver.cpp:238]     Train net output #0: loss = 0.666175 (* 1 = 0.666175 loss)
I0803 15:43:57.723608 16203 sgd_solver.cpp:105] Iteration 140, lr = 0.001
I0803 15:44:04.965826 16203 solver.cpp:219] Iteration 160 (2.76162 iter/s, 7.24212s/20 iters), loss = 0.734989
I0803 15:44:04.965919 16203 solver.cpp:238]     Train net output #0: loss = 0.734989 (* 1 = 0.734989 loss)
I0803 15:44:04.965931 16203 sgd_solver.cpp:105] Iteration 160, lr = 0.001
I0803 15:44:12.100792 16203 solver.cpp:219] Iteration 180 (2.80318 iter/s, 7.13476s/20 iters), loss = 0.712014
I0803 15:44:12.100889 16203 solver.cpp:238]     Train net output #0: loss = 0.712014 (* 1 = 0.712014 loss)
I0803 15:44:12.100903 16203 sgd_solver.cpp:105] Iteration 180, lr = 0.001
I0803 15:44:19.221293 16203 solver.cpp:219] Iteration 200 (2.80887 iter/s, 7.12029s/20 iters), loss = 0.657378
I0803 15:44:19.221412 16203 solver.cpp:238]     Train net output #0: loss = 0.657378 (* 1 = 0.657378 loss)
I0803 15:44:19.221464 16203 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0803 15:44:25.554451 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:44:26.587060 16203 solver.cpp:219] Iteration 220 (2.71535 iter/s, 7.36552s/20 iters), loss = 0.716493
I0803 15:44:26.587151 16203 solver.cpp:238]     Train net output #0: loss = 0.716493 (* 1 = 0.716493 loss)
I0803 15:44:26.587167 16203 sgd_solver.cpp:105] Iteration 220, lr = 0.001
I0803 15:44:33.750457 16203 solver.cpp:219] Iteration 240 (2.79206 iter/s, 7.16316s/20 iters), loss = 0.629842
I0803 15:44:33.750584 16203 solver.cpp:238]     Train net output #0: loss = 0.629842 (* 1 = 0.629842 loss)
I0803 15:44:33.750599 16203 sgd_solver.cpp:105] Iteration 240, lr = 0.001
I0803 15:44:37.018638 16203 solver.cpp:331] Iteration 250, Testing net (#0)
I0803 15:44:38.481879 16203 solver.cpp:398]     Test net output #0: accuracy = 0.623
I0803 15:44:38.481989 16203 solver.cpp:398]     Test net output #1: loss = 0.678589 (* 1 = 0.678589 loss)
I0803 15:44:41.888797 16203 solver.cpp:219] Iteration 260 (2.4576 iter/s, 8.13801s/20 iters), loss = 0.680952
I0803 15:44:41.888975 16203 solver.cpp:238]     Train net output #0: loss = 0.680952 (* 1 = 0.680952 loss)
I0803 15:44:41.889011 16203 sgd_solver.cpp:105] Iteration 260, lr = 0.001
I0803 15:44:49.020728 16203 solver.cpp:219] Iteration 280 (2.80441 iter/s, 7.13163s/20 iters), loss = 0.692394
I0803 15:44:49.020826 16203 solver.cpp:238]     Train net output #0: loss = 0.692394 (* 1 = 0.692394 loss)
I0803 15:44:49.020839 16203 sgd_solver.cpp:105] Iteration 280, lr = 0.001
I0803 15:44:56.218456 16203 solver.cpp:219] Iteration 300 (2.77873 iter/s, 7.19752s/20 iters), loss = 0.661652
I0803 15:44:56.218546 16203 solver.cpp:238]     Train net output #0: loss = 0.661652 (* 1 = 0.661652 loss)
I0803 15:44:56.218559 16203 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0803 15:45:03.566898 16203 solver.cpp:219] Iteration 320 (2.72177 iter/s, 7.34816s/20 iters), loss = 0.65023
I0803 15:45:03.567030 16203 solver.cpp:238]     Train net output #0: loss = 0.65023 (* 1 = 0.65023 loss)
I0803 15:45:03.567045 16203 sgd_solver.cpp:105] Iteration 320, lr = 0.001
I0803 15:45:05.791311 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:45:10.644801 16203 solver.cpp:219] Iteration 340 (2.82579 iter/s, 7.07766s/20 iters), loss = 0.663608
I0803 15:45:10.644896 16203 solver.cpp:238]     Train net output #0: loss = 0.663608 (* 1 = 0.663608 loss)
I0803 15:45:10.644912 16203 sgd_solver.cpp:105] Iteration 340, lr = 0.001
I0803 15:45:18.003103 16203 solver.cpp:219] Iteration 360 (2.7181 iter/s, 7.35807s/20 iters), loss = 0.692057
I0803 15:45:18.003204 16203 solver.cpp:238]     Train net output #0: loss = 0.692057 (* 1 = 0.692057 loss)
I0803 15:45:18.003257 16203 sgd_solver.cpp:105] Iteration 360, lr = 0.001
I0803 15:45:24.968236 16203 solver.cpp:219] Iteration 380 (2.87155 iter/s, 6.96489s/20 iters), loss = 0.664437
I0803 15:45:24.968379 16203 solver.cpp:238]     Train net output #0: loss = 0.664437 (* 1 = 0.664437 loss)
I0803 15:45:24.968405 16203 sgd_solver.cpp:105] Iteration 380, lr = 0.001
I0803 15:45:32.232100 16203 solver.cpp:219] Iteration 400 (2.75345 iter/s, 7.26362s/20 iters), loss = 0.712002
I0803 15:45:32.232180 16203 solver.cpp:238]     Train net output #0: loss = 0.712002 (* 1 = 0.712002 loss)
I0803 15:45:32.232195 16203 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0803 15:45:39.586472 16203 solver.cpp:219] Iteration 420 (2.71955 iter/s, 7.35416s/20 iters), loss = 0.682865
I0803 15:45:39.586576 16203 solver.cpp:238]     Train net output #0: loss = 0.682865 (* 1 = 0.682865 loss)
I0803 15:45:39.586592 16203 sgd_solver.cpp:105] Iteration 420, lr = 0.001
I0803 15:45:45.152525 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:45:46.691385 16203 solver.cpp:219] Iteration 440 (2.81505 iter/s, 7.10467s/20 iters), loss = 0.708061
I0803 15:45:46.691476 16203 solver.cpp:238]     Train net output #0: loss = 0.708061 (* 1 = 0.708061 loss)
I0803 15:45:46.691490 16203 sgd_solver.cpp:105] Iteration 440, lr = 0.001
I0803 15:45:54.208565 16203 solver.cpp:219] Iteration 460 (2.66066 iter/s, 7.51693s/20 iters), loss = 0.70176
I0803 15:45:54.208694 16203 solver.cpp:238]     Train net output #0: loss = 0.70176 (* 1 = 0.70176 loss)
I0803 15:45:54.208712 16203 sgd_solver.cpp:105] Iteration 460, lr = 0.001
I0803 15:46:01.547771 16203 solver.cpp:219] Iteration 480 (2.72519 iter/s, 7.33895s/20 iters), loss = 0.644039
I0803 15:46:01.547857 16203 solver.cpp:238]     Train net output #0: loss = 0.644039 (* 1 = 0.644039 loss)
I0803 15:46:01.547873 16203 sgd_solver.cpp:105] Iteration 480, lr = 0.001
I0803 15:46:08.337996 16203 solver.cpp:331] Iteration 500, Testing net (#0)
I0803 15:46:09.817311 16203 solver.cpp:398]     Test net output #0: accuracy = 0.621
I0803 15:46:09.817430 16203 solver.cpp:398]     Test net output #1: loss = 0.659734 (* 1 = 0.659734 loss)
I0803 15:46:10.087604 16203 solver.cpp:219] Iteration 500 (2.34204 iter/s, 8.53958s/20 iters), loss = 0.696062
I0803 15:46:10.087713 16203 solver.cpp:238]     Train net output #0: loss = 0.696062 (* 1 = 0.696062 loss)
I0803 15:46:10.087740 16203 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0803 15:46:16.934962 16203 solver.cpp:219] Iteration 520 (2.92095 iter/s, 6.84709s/20 iters), loss = 0.656722
I0803 15:46:16.935086 16203 solver.cpp:238]     Train net output #0: loss = 0.656722 (* 1 = 0.656722 loss)
I0803 15:46:16.935112 16203 sgd_solver.cpp:105] Iteration 520, lr = 0.001
I0803 15:46:24.038322 16203 solver.cpp:219] Iteration 540 (2.81567 iter/s, 7.1031s/20 iters), loss = 0.652802
I0803 15:46:24.038403 16203 solver.cpp:238]     Train net output #0: loss = 0.652802 (* 1 = 0.652802 loss)
I0803 15:46:24.038417 16203 sgd_solver.cpp:105] Iteration 540, lr = 0.001
I0803 15:46:25.825886 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:46:31.233651 16203 solver.cpp:219] Iteration 560 (2.77967 iter/s, 7.1951s/20 iters), loss = 0.675848
I0803 15:46:31.233744 16203 solver.cpp:238]     Train net output #0: loss = 0.675848 (* 1 = 0.675848 loss)
I0803 15:46:31.233767 16203 sgd_solver.cpp:105] Iteration 560, lr = 0.001
I0803 15:46:38.359652 16203 solver.cpp:219] Iteration 580 (2.80671 iter/s, 7.12578s/20 iters), loss = 0.665654
I0803 15:46:38.359736 16203 solver.cpp:238]     Train net output #0: loss = 0.665654 (* 1 = 0.665654 loss)
I0803 15:46:38.359747 16203 sgd_solver.cpp:105] Iteration 580, lr = 0.001
I0803 15:46:44.602707 16203 solver.cpp:219] Iteration 600 (3.20369 iter/s, 6.24279s/20 iters), loss = 0.659869
I0803 15:46:44.602869 16203 solver.cpp:238]     Train net output #0: loss = 0.659869 (* 1 = 0.659869 loss)
I0803 15:46:44.602888 16203 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0803 15:46:52.205551 16203 solver.cpp:219] Iteration 620 (2.63071 iter/s, 7.6025s/20 iters), loss = 0.649863
I0803 15:46:52.205677 16203 solver.cpp:238]     Train net output #0: loss = 0.649863 (* 1 = 0.649863 loss)
I0803 15:46:52.205693 16203 sgd_solver.cpp:105] Iteration 620, lr = 0.001
I0803 15:46:59.543315 16203 solver.cpp:219] Iteration 640 (2.72572 iter/s, 7.3375s/20 iters), loss = 0.677772
I0803 15:46:59.543401 16203 solver.cpp:238]     Train net output #0: loss = 0.677772 (* 1 = 0.677772 loss)
I0803 15:46:59.543414 16203 sgd_solver.cpp:105] Iteration 640, lr = 0.001
I0803 15:47:04.654954 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:47:06.816593 16203 solver.cpp:219] Iteration 660 (2.74988 iter/s, 7.27304s/20 iters), loss = 0.683526
I0803 15:47:06.816684 16203 solver.cpp:238]     Train net output #0: loss = 0.683526 (* 1 = 0.683526 loss)
I0803 15:47:06.816696 16203 sgd_solver.cpp:105] Iteration 660, lr = 0.001
I0803 15:47:14.199805 16203 solver.cpp:219] Iteration 680 (2.70893 iter/s, 7.38298s/20 iters), loss = 0.68236
I0803 15:47:14.199905 16203 solver.cpp:238]     Train net output #0: loss = 0.68236 (* 1 = 0.68236 loss)
I0803 15:47:14.199920 16203 sgd_solver.cpp:105] Iteration 680, lr = 0.001
I0803 15:47:21.791249 16203 solver.cpp:219] Iteration 700 (2.63465 iter/s, 7.59115s/20 iters), loss = 0.624297
I0803 15:47:21.791414 16203 solver.cpp:238]     Train net output #0: loss = 0.624297 (* 1 = 0.624297 loss)
I0803 15:47:21.791432 16203 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0803 15:47:29.146060 16203 solver.cpp:219] Iteration 720 (2.71942 iter/s, 7.3545s/20 iters), loss = 0.658223
I0803 15:47:29.146169 16203 solver.cpp:238]     Train net output #0: loss = 0.658223 (* 1 = 0.658223 loss)
I0803 15:47:29.146185 16203 sgd_solver.cpp:105] Iteration 720, lr = 0.001
I0803 15:47:36.303777 16203 solver.cpp:219] Iteration 740 (2.79428 iter/s, 7.15748s/20 iters), loss = 0.661835
I0803 15:47:36.303859 16203 solver.cpp:238]     Train net output #0: loss = 0.661835 (* 1 = 0.661835 loss)
I0803 15:47:36.303874 16203 sgd_solver.cpp:105] Iteration 740, lr = 0.001
I0803 15:47:39.621781 16203 solver.cpp:331] Iteration 750, Testing net (#0)
I0803 15:47:41.038902 16203 solver.cpp:398]     Test net output #0: accuracy = 0.652
I0803 15:47:41.039017 16203 solver.cpp:398]     Test net output #1: loss = 0.656993 (* 1 = 0.656993 loss)
I0803 15:47:44.198940 16203 solver.cpp:219] Iteration 760 (2.53327 iter/s, 7.89492s/20 iters), loss = 0.672369
I0803 15:47:44.199008 16203 solver.cpp:238]     Train net output #0: loss = 0.672369 (* 1 = 0.672369 loss)
I0803 15:47:44.199019 16203 sgd_solver.cpp:105] Iteration 760, lr = 0.001
I0803 15:47:44.763382 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:47:50.165248 16203 solver.cpp:219] Iteration 780 (3.35226 iter/s, 5.96613s/20 iters), loss = 0.676373
I0803 15:47:50.165340 16203 solver.cpp:238]     Train net output #0: loss = 0.676373 (* 1 = 0.676373 loss)
I0803 15:47:50.165356 16203 sgd_solver.cpp:105] Iteration 780, lr = 0.001
I0803 15:47:57.471947 16203 solver.cpp:219] Iteration 800 (2.73731 iter/s, 7.30644s/20 iters), loss = 0.663645
I0803 15:47:57.472059 16203 solver.cpp:238]     Train net output #0: loss = 0.663645 (* 1 = 0.663645 loss)
I0803 15:47:57.472071 16203 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0803 15:48:04.761344 16203 solver.cpp:219] Iteration 820 (2.7438 iter/s, 7.28916s/20 iters), loss = 0.698131
I0803 15:48:04.761432 16203 solver.cpp:238]     Train net output #0: loss = 0.698131 (* 1 = 0.698131 loss)
I0803 15:48:04.761447 16203 sgd_solver.cpp:105] Iteration 820, lr = 0.001
I0803 15:48:12.046924 16203 solver.cpp:219] Iteration 840 (2.74523 iter/s, 7.28536s/20 iters), loss = 0.683701
I0803 15:48:12.046996 16203 solver.cpp:238]     Train net output #0: loss = 0.683701 (* 1 = 0.683701 loss)
I0803 15:48:12.047009 16203 sgd_solver.cpp:105] Iteration 840, lr = 0.001
I0803 15:48:19.207336 16203 solver.cpp:219] Iteration 860 (2.79322 iter/s, 7.1602s/20 iters), loss = 0.662231
I0803 15:48:19.207434 16203 solver.cpp:238]     Train net output #0: loss = 0.662231 (* 1 = 0.662231 loss)
I0803 15:48:19.207448 16203 sgd_solver.cpp:105] Iteration 860, lr = 0.001
I0803 15:48:23.669795 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:48:26.162600 16203 solver.cpp:219] Iteration 880 (2.87563 iter/s, 6.95501s/20 iters), loss = 0.694299
I0803 15:48:26.162706 16203 solver.cpp:238]     Train net output #0: loss = 0.694299 (* 1 = 0.694299 loss)
I0803 15:48:26.162721 16203 sgd_solver.cpp:105] Iteration 880, lr = 0.001
I0803 15:48:33.315914 16203 solver.cpp:219] Iteration 900 (2.796 iter/s, 7.15306s/20 iters), loss = 0.67579
I0803 15:48:33.315987 16203 solver.cpp:238]     Train net output #0: loss = 0.67579 (* 1 = 0.67579 loss)
I0803 15:48:33.316001 16203 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0803 15:48:40.880420 16203 solver.cpp:219] Iteration 920 (2.644 iter/s, 7.56428s/20 iters), loss = 0.669842
I0803 15:48:40.880511 16203 solver.cpp:238]     Train net output #0: loss = 0.669842 (* 1 = 0.669842 loss)
I0803 15:48:40.880527 16203 sgd_solver.cpp:105] Iteration 920, lr = 0.001
I0803 15:48:48.060405 16203 solver.cpp:219] Iteration 940 (2.78564 iter/s, 7.17968s/20 iters), loss = 0.666291
I0803 15:48:48.060533 16203 solver.cpp:238]     Train net output #0: loss = 0.666291 (* 1 = 0.666291 loss)
I0803 15:48:48.060549 16203 sgd_solver.cpp:105] Iteration 940, lr = 0.001
I0803 15:48:55.518390 16203 solver.cpp:219] Iteration 960 (2.6818 iter/s, 7.45767s/20 iters), loss = 0.64227
I0803 15:48:55.518581 16203 solver.cpp:238]     Train net output #0: loss = 0.64227 (* 1 = 0.64227 loss)
I0803 15:48:55.518620 16203 sgd_solver.cpp:105] Iteration 960, lr = 0.001
I0803 15:49:02.431108 16203 solver.cpp:219] Iteration 980 (2.89335 iter/s, 6.91241s/20 iters), loss = 0.684449
I0803 15:49:02.431196 16203 solver.cpp:238]     Train net output #0: loss = 0.684449 (* 1 = 0.684449 loss)
I0803 15:49:02.431211 16203 sgd_solver.cpp:105] Iteration 980, lr = 0.001
I0803 15:49:03.161638 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:49:09.325884 16203 solver.cpp:331] Iteration 1000, Testing net (#0)
I0803 15:49:10.774925 16203 solver.cpp:398]     Test net output #0: accuracy = 0.606
I0803 15:49:10.775037 16203 solver.cpp:398]     Test net output #1: loss = 0.666158 (* 1 = 0.666158 loss)
I0803 15:49:11.046532 16203 solver.cpp:219] Iteration 1000 (2.32149 iter/s, 8.61515s/20 iters), loss = 0.667054
I0803 15:49:11.046646 16203 solver.cpp:238]     Train net output #0: loss = 0.667054 (* 1 = 0.667054 loss)
I0803 15:49:11.046672 16203 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0803 15:49:13.144883 16203 blocking_queue.cpp:49] Waiting for data
I0803 15:49:17.849967 16203 solver.cpp:219] Iteration 1020 (2.9398 iter/s, 6.80318s/20 iters), loss = 0.663396
I0803 15:49:17.850062 16203 solver.cpp:238]     Train net output #0: loss = 0.663396 (* 1 = 0.663396 loss)
I0803 15:49:17.850077 16203 sgd_solver.cpp:105] Iteration 1020, lr = 0.001
I0803 15:49:25.238387 16203 solver.cpp:219] Iteration 1040 (2.70703 iter/s, 7.38816s/20 iters), loss = 0.666636
I0803 15:49:25.238461 16203 solver.cpp:238]     Train net output #0: loss = 0.666636 (* 1 = 0.666636 loss)
I0803 15:49:25.238476 16203 sgd_solver.cpp:105] Iteration 1040, lr = 0.001
I0803 15:49:32.650414 16203 solver.cpp:219] Iteration 1060 (2.69839 iter/s, 7.41182s/20 iters), loss = 0.666093
I0803 15:49:32.650537 16203 solver.cpp:238]     Train net output #0: loss = 0.666093 (* 1 = 0.666093 loss)
I0803 15:49:32.650552 16203 sgd_solver.cpp:105] Iteration 1060, lr = 0.001
I0803 15:49:39.955600 16203 solver.cpp:219] Iteration 1080 (2.73789 iter/s, 7.3049s/20 iters), loss = 0.653913
I0803 15:49:39.955736 16203 solver.cpp:238]     Train net output #0: loss = 0.653913 (* 1 = 0.653913 loss)
I0803 15:49:39.955790 16203 sgd_solver.cpp:105] Iteration 1080, lr = 0.001
I0803 15:49:43.935029 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:49:47.323200 16203 solver.cpp:219] Iteration 1100 (2.7147 iter/s, 7.36731s/20 iters), loss = 0.68257
I0803 15:49:47.323329 16203 solver.cpp:238]     Train net output #0: loss = 0.68257 (* 1 = 0.68257 loss)
I0803 15:49:47.323348 16203 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0803 15:49:54.551491 16203 solver.cpp:219] Iteration 1120 (2.767 iter/s, 7.22803s/20 iters), loss = 0.662445
I0803 15:49:54.551576 16203 solver.cpp:238]     Train net output #0: loss = 0.662445 (* 1 = 0.662445 loss)
I0803 15:49:54.551591 16203 sgd_solver.cpp:105] Iteration 1120, lr = 0.001
I0803 15:50:01.876775 16203 solver.cpp:219] Iteration 1140 (2.73035 iter/s, 7.32506s/20 iters), loss = 0.690209
I0803 15:50:01.876869 16203 solver.cpp:238]     Train net output #0: loss = 0.690209 (* 1 = 0.690209 loss)
I0803 15:50:01.876885 16203 sgd_solver.cpp:105] Iteration 1140, lr = 0.001
I0803 15:50:09.179294 16203 solver.cpp:219] Iteration 1160 (2.73887 iter/s, 7.30229s/20 iters), loss = 0.684253
I0803 15:50:09.179383 16203 solver.cpp:238]     Train net output #0: loss = 0.684253 (* 1 = 0.684253 loss)
I0803 15:50:09.179396 16203 sgd_solver.cpp:105] Iteration 1160, lr = 0.001
I0803 15:50:16.295892 16203 solver.cpp:219] Iteration 1180 (2.81041 iter/s, 7.11639s/20 iters), loss = 0.660262
I0803 15:50:16.295964 16203 solver.cpp:238]     Train net output #0: loss = 0.660262 (* 1 = 0.660262 loss)
I0803 15:50:16.295976 16203 sgd_solver.cpp:105] Iteration 1180, lr = 0.001
I0803 15:50:23.390025 16203 solver.cpp:219] Iteration 1200 (2.81931 iter/s, 7.09394s/20 iters), loss = 0.636781
I0803 15:50:23.390100 16203 solver.cpp:238]     Train net output #0: loss = 0.636781 (* 1 = 0.636781 loss)
I0803 15:50:23.390113 16203 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0803 15:50:23.665729 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:50:30.726541 16203 solver.cpp:219] Iteration 1220 (2.72617 iter/s, 7.33631s/20 iters), loss = 0.673089
I0803 15:50:30.726619 16203 solver.cpp:238]     Train net output #0: loss = 0.673089 (* 1 = 0.673089 loss)
I0803 15:50:30.726632 16203 sgd_solver.cpp:105] Iteration 1220, lr = 0.001
I0803 15:50:38.002601 16203 solver.cpp:219] Iteration 1240 (2.74884 iter/s, 7.27581s/20 iters), loss = 0.667413
I0803 15:50:38.002699 16203 solver.cpp:238]     Train net output #0: loss = 0.667413 (* 1 = 0.667413 loss)
I0803 15:50:38.002712 16203 sgd_solver.cpp:105] Iteration 1240, lr = 0.001
I0803 15:50:41.259485 16203 solver.cpp:331] Iteration 1250, Testing net (#0)
I0803 15:50:42.041393 16260 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:50:42.738332 16203 solver.cpp:398]     Test net output #0: accuracy = 0.627
I0803 15:50:42.738423 16203 solver.cpp:398]     Test net output #1: loss = 0.651498 (* 1 = 0.651498 loss)
I0803 15:50:45.894613 16203 solver.cpp:219] Iteration 1260 (2.53429 iter/s, 7.89176s/20 iters), loss = 0.657661
I0803 15:50:45.894727 16203 solver.cpp:238]     Train net output #0: loss = 0.657661 (* 1 = 0.657661 loss)
I0803 15:50:45.894748 16203 sgd_solver.cpp:105] Iteration 1260, lr = 0.001
I0803 15:50:52.865185 16203 solver.cpp:219] Iteration 1280 (2.8693 iter/s, 6.97033s/20 iters), loss = 0.622716
I0803 15:50:52.865290 16203 solver.cpp:238]     Train net output #0: loss = 0.622716 (* 1 = 0.622716 loss)
I0803 15:50:52.865305 16203 sgd_solver.cpp:105] Iteration 1280, lr = 0.001
I0803 15:51:00.080549 16203 solver.cpp:219] Iteration 1300 (2.77197 iter/s, 7.21508s/20 iters), loss = 0.679748
I0803 15:51:00.080653 16203 solver.cpp:238]     Train net output #0: loss = 0.679748 (* 1 = 0.679748 loss)
I0803 15:51:00.080668 16203 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0803 15:51:03.635426 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:51:07.253885 16203 solver.cpp:219] Iteration 1320 (2.78819 iter/s, 7.17311s/20 iters), loss = 0.645732
I0803 15:51:07.253952 16203 solver.cpp:238]     Train net output #0: loss = 0.645732 (* 1 = 0.645732 loss)
I0803 15:51:07.253967 16203 sgd_solver.cpp:105] Iteration 1320, lr = 0.001
I0803 15:51:14.657966 16203 solver.cpp:219] Iteration 1340 (2.70129 iter/s, 7.40387s/20 iters), loss = 0.655686
I0803 15:51:14.658059 16203 solver.cpp:238]     Train net output #0: loss = 0.655686 (* 1 = 0.655686 loss)
I0803 15:51:14.658074 16203 sgd_solver.cpp:105] Iteration 1340, lr = 0.001
I0803 15:51:21.873095 16203 solver.cpp:219] Iteration 1360 (2.77206 iter/s, 7.21486s/20 iters), loss = 0.668615
I0803 15:51:21.873214 16203 solver.cpp:238]     Train net output #0: loss = 0.668615 (* 1 = 0.668615 loss)
I0803 15:51:21.873257 16203 sgd_solver.cpp:105] Iteration 1360, lr = 0.001
I0803 15:51:29.049224 16203 solver.cpp:219] Iteration 1380 (2.78711 iter/s, 7.17589s/20 iters), loss = 0.626492
I0803 15:51:29.049286 16203 solver.cpp:238]     Train net output #0: loss = 0.626492 (* 1 = 0.626492 loss)
I0803 15:51:29.049299 16203 sgd_solver.cpp:105] Iteration 1380, lr = 0.001
I0803 15:51:36.373136 16203 solver.cpp:219] Iteration 1400 (2.73086 iter/s, 7.32369s/20 iters), loss = 0.652063
I0803 15:51:36.373239 16203 solver.cpp:238]     Train net output #0: loss = 0.652063 (* 1 = 0.652063 loss)
I0803 15:51:36.373255 16203 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0803 15:51:43.387161 16258 data_layer.cpp:73] Restarting data prefetching from start.
I0803 15:51:43.722754 16203 solver.cpp:219] Iteration 1420 (2.72131 iter/s, 7.34939s/20 iters), loss = 0.641474
I0803 15:51:43.722857 16203 solver.cpp:238]     Train net output #0: loss = 0.641474 (* 1 = 0.641474 loss)
I0803 15:51:43.722872 16203 sgd_solver.cpp:105] Iteration 1420, lr = 0.001
I0803 15:51:51.024745 16203 solver.cpp:219] Iteration 1440 (2.73908 iter/s, 7.30171s/20 iters), loss = 0.641773
I0803 15:51:51.024878 16203 solver.cpp:238]     Train net output #0: loss = 0.641773 (* 1 = 0.641773 loss)
I0803 15:51:51.024894 16203 sgd_solver.cpp:105] Iteration 1440, lr = 0.001
I0803 15:51:58.528664 16203 solver.cpp:219] Iteration 1460 (2.66537 iter/s, 7.50366s/20 iters), loss = 0.667613
I0803 15:51:58.528749 16203 solver.cpp:238]     Train net output #0: loss = 0.667613 (* 1 = 0.667613 loss)
I0803 15:51:58.528762 16203 sgd_solver.cpp:105] Iteration 1460, lr = 0.001
I0803 15:52:05.806630 16203 solver.cpp:219] Iteration 1480 (2.74811 iter/s, 7.27774s/20 iters), loss = 0.641614
I0803 15:52:05.806754 16203 solver.cpp:238]     Train net output #0: loss = 0.641614 (* 1 = 0.641614 loss)
I0803 15:52:05.806788 16203 sgd_solver.cpp:105] Iteration 1480, lr = 0.001
I0803 15:52:12.795509 16203 solver.cpp:331] Iteration 1500, Testing net (#0)
I0803 15:52:14.161698 16203 solver.cpp:398]     Test net output #0: accuracy = 0.621
I0803 15:52:14.161834 16203 solver.cpp:398]     Test net output #1: loss = 0.654926 (* 1 = 0.654926 loss)
I0803 15:52:14.431046 16203 solver.cpp:219] Iteration 1500 (2.31907 iter/s, 8.62414s/20 iters), loss = 0.626282
I0803 15:52:14.431147 16203 solver.cpp:238]     Train net output #0: loss = 0.626282 (* 1 = 0.626282 loss)
I0803 15:52:14.431175 16203 sgd_solver.cpp:105] Iteration 1500, lr = 0.0001
data	(256, 3, 227, 227)
label	(256,)
conv1	(256, 96, 55, 55)
pool1	(256, 96, 27, 27)
norm1	(256, 96, 27, 27)
conv2	(256, 256, 27, 27)
pool2	(256, 256, 13, 13)
norm2	(256, 256, 13, 13)
conv3	(256, 384, 13, 13)
conv4	(256, 384, 13, 13)
conv5	(256, 256, 13, 13)
pool5	(256, 256, 6, 6)
fc6	(256, 4096)
fc7	(256, 4096)
fc8	(256, 2)
loss	()
conv1	(96, 3, 11, 11) (96,)
conv2	(256, 48, 5, 5) (256,)
conv3	(384, 256, 3, 3) (384,)
conv4	(384, 192, 3, 3) (384,)
conv5	(256, 192, 3, 3) (256,)
fc6	(4096, 9216) (4096,)
fc7	(4096, 4096) (4096,)
fc8	(2, 4096) (2,)
